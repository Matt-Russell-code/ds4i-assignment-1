[
  {
    "objectID": "llm_use.html",
    "href": "llm_use.html",
    "title": "LLM use",
    "section": "",
    "text": "LLM Use Notes\nPlaceholder. Add content later."
  },
  {
    "objectID": "DS4i_writeup.html",
    "href": "DS4i_writeup.html",
    "title": "DS4I - Project Writeup",
    "section": "",
    "text": "Photo by Nicolas Cool on Unsplash"
  },
  {
    "objectID": "DS4i_writeup.html#abstract",
    "href": "DS4i_writeup.html#abstract",
    "title": "DS4I - Project Writeup",
    "section": "Abstract",
    "text": "Abstract\nThis study evaluates the potential of neural networks for avalanche forecasting in Scotland, a region where research has been sparse and limited in scope. Using a 15-year archive of avalanche forecasts from the Scottish Avalanche Information Service (SAIS) across six regions (Creah Meagaidh, Glencoe, Lochaber, Northern Cairngorms, Southern Cairngorms, and Torridon), we compared three architectures: a feedforward multilayer perceptron (FFNN-MLP), a convolutional neural network (CNN), and a recurrent neural network (RNN). Each was selected for its theoretical strengths. Given the sequential nature of avalanche risk, we hypothesised that the CNN and RNN would outperform the FFNN-MLP.\nContrary to this expectation, the FFNN-MLP achieved the best overall accuracy (75%) and the most balanced detection across hazard levels. The CNN (65% accuracy) and RNN (59% accuracy) both exhibited strong bias toward the majority “Low” and “Moderate” classes, and performed poorly on higher-risk cases. Even the FFNN-MLP struggled to identify rare but critical events, with a “High” hazard sensitivity of 0.56.\nThese findings highlight both the promise and limitations of applying neural networks directly to avalanche hazard forecasting. While the FFNN-MLP proved most effective in this context, improving detection of minority, high-risk categories remains essential. Future research should prioritise methods for addressing class imbalance, region-specific modelling, and ensemble strategies to better capture Scotland’s diverse avalanche dynamics under changing climate conditions."
  },
  {
    "objectID": "DS4i_writeup.html#introduction-and-literature-review",
    "href": "DS4i_writeup.html#introduction-and-literature-review",
    "title": "DS4I - Project Writeup",
    "section": "Introduction and Literature Review",
    "text": "Introduction and Literature Review\nSnow avalanches are a hazard in snow-prone regions. Unlike more alpine regions, avalanches in Scotland rarely impact settlements and infrastructure, but pose risks to recreational users (Bain, 2024; Diggins, 2009; Ward, 1980; WSL Institute for Snow and Avalanche Research SLF [WSL], 2016). Rising popularity of winter activities such as climbing, walking, and skiing has increased exposure to avalanche-prone terrain, resulting in injuries and occasional fatalities each year (Scottish Avalanche Information Service [SAIS], 2024; Webster, 2020, WSL, 2016). These risks prompted studies of Scottish snowpack and forecasting strategies beginning in the 1970s (Langmuir, 1970; Spink, 1970; Beattie, 1976; Ward, 1980; Ward et al., 1985). However, early work was largely descriptive, relying on snow-pit measurements and expert judgement. Building on this, Ward (1984a and 1984b) applied one of the first predictive avalanche forecasting models for Scotland.\nAvalanche modelling is inherently complex, where predictions depend on multiple interacting factors (Choubin et al., 2019; Helbig et al., 2015; Hendrick et al., 2023; Pozdnoukhov et al., 2008; Singh & Ganju, 2008). This challenge is heightened in regions with variable weather (Sharma & Ganju, 1999). Early statistical methods, like nearest neighbours, supported forecasters by relating current weather to past avalanche events, however, were prone to overfitting and struggled with high-dimensional data (Blagovechshenskiy et al., 2023; Kala et al., 2025; Pozdnoukhov et al., 2018; Singh & Ganju, 2008). Using a dataset from Lochaber, Scotland, Pozdnoukhov et al. (2008) reported that support vector machines (SVMs) performed similarly to nearest neighbours while handling high-dimensional data more effectively and producing richer forecasts. SVMs have also proven successful in Iran, India, Switzerland, and Tibet (Choubin et al., 2019; Rahmati et al., 2019; Schirmer et al., 2009; Tiwari et al., 2021; Wen et al., 2022). \nRecently, more complex machine learning (ML) algorithms (regression trees, random forests, neural networks) have demonstrated strong predictive performance in avalanche forecasting (Blagovechshenskiy et al., 2023; Choubin et al., 2019; Gauthier et al., 2025; Hendrick et al., 2023; Rahmati et al., 2019; Singh & Ganju, 2018; Tiwari et al., 2021; Wen et al., 2022). Avalanche datasets are often high-dimensional, and using forecaster-led feature selection can be highly subjective (Helbig et al., 2015; Pozdnoukhov et al., 2008). Hybrid approaches combining ML tools with expert-led feature selection have shown practical value (Gauthier et al., 2025). However, fully data-driven forecasting methods, particularly neural networks (NNs), offer important advantages.\nNNs simulate brain processes through interconnected nodes, capturing complex, nonlinear relationships (Blagovechshenskiy et al., 2023; Sharma et al., 2023; Tu, 1996). By processing inputs simultaneously, patterns are learned without prior knowledge of which variables matter most (Blagovechshenskiy et al., 2023; Fromm & Schonberger, 2022; Sharma et al., 2023). Applications in Switzerland, Kazakhstan, and India show that NNs not only perform strongly but can assess the relative importance of variables (Fromm & Schonberger; Sharma et al., 2023; Singh & Ganju, 2008). This makes them particularly well suited for high-dimensional avalanche datasets and situations with limited domain knowledge or uncertain predictors.\nDespite advances in data-driven avalanche forecasting worldwide, research specific to Scotland remains limited in three ways:\n\nFew published studies exist (only 9 relevant peer-reviewed papers exist on Scopus, with the most recent study conducted in 2011 (keywords: ( “avalanche forecasting” OR “avalanche prediction” ) AND Scotland)).\nThe restricted range of statistical approaches explored (only KNN and SVM) (Heierli et al., 2004; Pozdnoukhov et al., 2008; Pozdnoukhov et al., 2011; Purves et al., 2003).\nThe focus on only a few regions in Scotland (primarily the Cairngorms and Lochaber regions).\n\nThis limited interest likely stems from the highly localised nature of avalanche risk in Scotland, which generally occurs in remote areas, affecting relatively few people (Diggins, 2009; SAIS, 2024). Nevertheless, improved avalanche forecasting is increasingly important under changing climate conditions, which may alter snowpack properties and avalanche risk (Gauthier et al., 2025; Werritty & Sugden, 2013). A detailed discussion of climate change, however, is beyond this paper’s scope.\nThe present study aims to extend forecasting research in Scotland both spatially and methodologically by applying three NNs (convolutional (CNN), recurrent (RNN), and a feedforward multilater perceptron (FFNN-MLP)) trained on combined data from the following six regions of Scotland: Creah Meagaidh, Glencoe, Lochaber, Northern Cairngorms, Southern Cairngorms, and Torridon. Each model was chosen for its particular strengths: the FFNN-MLP works well with categorical predictors, the CNN is good at identifying spatial and short-term temporal patterns, and the RNN is suited to capturing longer-term sequential trends over time.\nAvalanche risk is sequential in nature because current conditions are influenced by how weather and snowpack have developed over previous days, weeks, or months. For this reason, we hypothesise that the CNN and RNN will perform better than the FFNN-MLP, as they can account for these temporal patterns."
  },
  {
    "objectID": "DS4i_writeup.html#context-and-dataset",
    "href": "DS4i_writeup.html#context-and-dataset",
    "title": "DS4I - Project Writeup",
    "section": "Context and Dataset",
    "text": "Context and Dataset\nScotland has a mild, wet, temperate maritime climate influenced by the North Atlantic Ocean and persistent south-westerly winds (Pozdnoukhov et al., 2008; Scottish Government, 2011). These conditions produce rapid temperature changes, frequent heavy precipitation (snow or rain), and strong winds. With its highest peak, Ben Nevis (1345 m), Scotland’s mountains are low compared to alpine ranges (~4000 m), resulting in a snowpack that is typically shallower, wetter, and more variable (Britannica, 2025; Pozdnoukhov et al., 2008; WSL, 2016). Rapid freeze-thaw cycles, rain-on-snow, and wind redistribution further destabilise snow, creating avalanche risks that can appear and disappear within a short time (Diggins, 2009; Pozdnoukhov et al., 2008; Purves et al., 2003; Scottish Government, 2011). Snowpack conditions also differ regionally; the West is strongly maritime, with mild and wet winters, while the North and East are colder and drier (MetOffice, 2010). Scotland’s climate is thus highly variable spatially and temporally, adding to the complexity of avalanche risk prediction.\nThis paper uses a 15-year archive of avalanche forecasts from Scotland across the six areas of Creah Meagaidh, Glencoe, Lochaber, Northern Cairngorms, Southern Cairngorms, and Torridon, produced by the SAIS (2025). Figure 1 illustrates the spatial distribution of observations, with points marking individual events and labels indicating area locations. Total observations by area and hazard category (Table 1) reflect differences in observation frequency rather than absolute avalanche activity. Cairngorms (Northern and Southern) and Lochaber, located in the colder and drier North and East, have the most recorded observations, whereas Torridon in the maritime West has substantially fewer. “Low” and “Moderate” hazard levels dominate across all areas, while “High” hazard events are rare and concentrated in Northern Cairngorms and Lochaber.\n\n\n\n\n\n\n\n\nFigure 1: Observed avalanche hazard (OAH): Scotland.\n\n\n\n\n\n\n\n\n\n\nTable 1: Overall total observations by area and hazard category.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArea\nLow\nModerate\nConsiderable -\nConsiderable +\nHigh\nTotal\n\n\n\n\nCreag Meagaidh\n636\n545\n413\n155\n63\n1812\n\n\nGlencoe\n716\n524\n405\n109\n58\n1812\n\n\nLochaber\n597\n498\n506\n116\n56\n1773\n\n\nNorthern Cairngorms\n655\n567\n501\n122\n46\n1891\n\n\nSouthern Cairngorms\n751\n546\n383\n72\n35\n1787\n\n\nTorridon\n678\n373\n85\n7\n0\n1143\n\n\n\n\n\n\n\n\n\nData span winters (November to April) from the December 2009 to March 2025. Predictors are categorised into: (1) position and topography, (2) weather, and (3) snowpack test, all collected at the forecast location. Each observation includes the date and the forecast area (metadata). For an overview of the predictors available, see Table 2 .\n\n\n\n\nTable 2: Overview of predictor variables available in the avalanche forecast dataset.\n\n\n\n\n\n\n\n\n\n\nPredictor_Group\nVariables\n\n\n\n\nMetadata\ndate, area\n\n\nPosition and Topography\nlongitude, latitude, altitude, aspect of slope, incline of slope\n\n\nWeather\nair temperature, wind direction, wind speed, cloud cover, precipitation code, snowdrift, total snow depth, foot penetration, ski penetration, rain observed at 900m elevation, summit air temperature, summit wind direction, summit wind speed\n\n\nSnowpack Test\nmax. temperature gradient, max. hardness gradient, no.settle, snow.index, insolation, crystals, wetness, AV.Cat, and snow temperature"
  },
  {
    "objectID": "DS4i_writeup.html#exploratory-data-analysis",
    "href": "DS4i_writeup.html#exploratory-data-analysis",
    "title": "DS4I - Project Writeup",
    "section": "Exploratory Data Analysis",
    "text": "Exploratory Data Analysis\n\na. Feature Map Correlations\nThe Scottish avalanche dataset contains several features that are moderately to strongly correlated, while others appear largely independent (Figure 2). The presence of correlated variables is not inherently problematic, since NNs can learn non-linear interactions. However, strong correlations can increase the risk of overfitting if the network memorises patterns rather than generalising to new data.\nThe highest correlation found was between summit.air.temperature and air.temperature (r = 0.85), indicating strong redundancy. Nevertheless, both variables were retained as each likely capture a different aspect relevant to forecasting (i.e., summit conditions versus broader conditions). Air.temperature is an identified key driver of avalanche conditions and is expected to be an important predictor in the NNs (Fromm & Schonberger, 2022; Gauthier et al., 2025; Pozdnoukhouv et al., 2018; Souckova et al., 2022; Ward,1980). Exploration of air.temperature across observed.avalanche.hazard (OAH) levels further supports its importance in forecasting (Figure 3). Higher hazard categories are generally associated with lower observed air.temperatures, with “Considerable-”, “Considerable+”, and “High” hazards centred below 0 °C. “Low” hazard days show medians above freezing and wider variability. The transition at “Moderate” hazard, where temperatures cluster around 0 °C, reflects conditions that can either stabilise or destabilise the snowpack through freeze-thaw cycles. These patterns highlight Scotland’s sensitivity to rapid temperature shifts and their impact on avalanche activity (Diggins, 2009; Pozdnoukhov et al., 2008).\nOther moderate correlations (0.5&lt;|r|&lt; 0.80) remain informative and manageable for NNs.\n\n\n\n\n\n\n\n\nFigure 2: Map of feature correlations.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 3: Distribution of air temperature across observed avalanche hazard (OAH).\n\n\n\n\n\n\n\nb. Precipitation Type\nThe distribution of OAH levels varies across different precipitation types (Figure 4). “Low” hazard predominates under conditions of none (0) or trace (2) precipitation, whereas higher hazard categories become increasingly prevalent as snowfall intensity increases. Heavy snow (level 10) is associated with a substantially larger share of “Considerable” and “High” hazard ratings (81.9%). This aligns with existing knowledge of the Scottish climate, where rapid weather shifts and frequent rain or snow-on-snow events contribute to unstable snowpack conditions (Diggins, 2009; Pozdnoukhov et al., 2008; Purves et al., 2003; Ward, 1980).\n\n\n\n\n\n\n\n\nFigure 4: Precipitation type vs observed avalanche hazard (OAH).\n\n\n\n\n\n\n\nc. Seasonality\nMonthly OAH patterns reveal clear seasonal trends (Figure 5). Early (November-December) and late (April-May) months are dominated by “Low” and “Moderate” hazards, while mid-season (Jan-Mar) sees more “Considerable” and “High” hazards. Most forecasts occur between December-March, highlighting the period of greatest avalanche risk (SAIS, 2024). These patterns reflect cyclical snowpack development, with mid-winter weather instability driving higher-risk avalanches (Diggins, 2009; Podzdnoukhov et al., 2008; Purves et al., 2003).\n\n\n\n\n\n\n\n\nFigure 5: Seasonality of observed avalanche hazard (OAH): Nov–May.\n\n\n\n\n\n\n\nd. FAH vs OAH\nLastly, Figure 6 shows the relationship between forecast.avalanche.hazard (FAH) (day t) and OAH (day t+1). Each column sums to 100%, showing how outcomes were distributed for each forecast level. “Low” forecasts were highly accurate (86.51%), while intermediate levels (“Moderate”, “Considerable-”) had weaker agreement, often spilling into neighbouring categories. “High” forecasts were least reliable, frequently over-predicting extreme conditions.\nThe agreement between OAH and FAH are highest in stable “Low”-risk conditions and lowest at the extremes, with middle categories showing uncertainty.\n\n\n\n\n\n\n\n\nFigure 6: FAH (day t) → OAH (day t+1) proportions.\n\n\n\n\n\nClass imbalances will need to be addressed during model training to ensure the network does not simply default to predicting the most common outcomes."
  },
  {
    "objectID": "DS4i_writeup.html#data-cleaning-and-feature-engineering",
    "href": "DS4i_writeup.html#data-cleaning-and-feature-engineering",
    "title": "DS4I - Project Writeup",
    "section": "Data Cleaning and Feature Engineering",
    "text": "Data Cleaning and Feature Engineering\nFeature engineering focused on creating temporal variables to capture trends not natively handled by a feedforward neural network (FFNN). Given that EDA highlighted that OAH varies according to the season and time of year, several variables were derived.\nBefore removing observations with extensive missing values, we extracted the time of day, day of the week, month, and year from each date. Next, a categorical variable for each avalanche season (December-April, 2009-2025) was created to account for the observed year-on-year increase in risk.\nTable 3 presents the percentage of missing values for features with incomplete data. Av.Cat was removed because its meaning was unclear and it contained implausible values (e.g., NA, 3, -2, 2, 1, 4, 8800, -1, 0, 1021, 4400, -9999, 99, 88, 44, 5031, 121). Figure 7 shows no clear relationship between ski.penetration and OAH, except for “High” risk avalanches; we therefore excluded it since snowfall effects are likely better represented by other variables. Summit.wind.direction (12.35% missing) was also dropped for lack of a discernible trend.\n\n\n\n\nTable 3: Missing value exploration.\n\n\n\n\n\n\nVariable\nMissing (%)\n\n\n\n\nAV.Cat\n23.3718\n\n\nSki.Pen\n22.5283\n\n\nSummit.Wind.Dir\n12.3512\n\n\nCrystals\n9.2587\n\n\nSummit.Wind.Speed\n8.5184\n\n\nSummit.Air.Temp\n7.0659\n\n\nSnow.Index\n6.9815\n\n\nMax.Temp.Grad\n6.6535\n\n\nMax.Hardness.Grad\n5.8945\n\n\nWetness\n5.3978\n\n\nInsolation\n4.7512\n\n\nOAH\n4.2452\n\n\nSnow.Temp\n3.8422\n\n\nAspect\n3.3268\n\n\nNo.Settle\n2.7083\n\n\nTotal.Snow.Depth\n1.5275\n\n\nWind.Dir\n1.4806\n\n\nWind.Speed\n0.4967\n\n\nFoot.Pen\n0.3936\n\n\nIncline\n0.3374\n\n\nAir.Temp\n0.3186\n\n\nCloud\n0.2718\n\n\nAlt\n0.0562\n\n\n\nNote: \n\n\n\n All values rounded to 4 decimal places.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 7: Boxplots of ski penetration and summit wind direction by avalanche hazard category.\n\n\n\n\n\n\nLastly, we dropped high-cardinality categorical variables, including osgrid and location. Osgrid appeared to be an ID variable with &gt;3000 unique values, which would make one-hot encoding overly complex and was unlikely to indicate risk. Location was also removed, since latitude, longitude, and area already captured spatial information more effectively.\nAfter removing variables, 36% of samples remained incomplete and were subsequently dropped. While retaining more observations would be ideal, the remaining ~7000 were sufficient for representative results."
  },
  {
    "objectID": "DS4i_writeup.html#modeling-methodology",
    "href": "DS4i_writeup.html#modeling-methodology",
    "title": "DS4I - Project Writeup",
    "section": "Modeling Methodology",
    "text": "Modeling Methodology\nThree main models were used in this project to exploit different structural properties of the data: a convolutional neural network (CNN), a recurrent neural network (RNN), and a feedforward multilayer perceptron neural network network (FFNN-MLP). The FFNN-MLP was expected to handle categorical predictors (e.g., forecasted avalanche risk) effectively, while the CNNs and RNNs were targeted at uncovering temporal or sequential patterns.\nCNNs process data by applying convolutional filters (small weight matrices) that slide across the input space. Each filter computes dot products with local patches of the data, producing feature maps that emphasise spatial or temporal patterns based on the type of filters used. Non-linear activations are then applied, followed by pooling layers to down-sample the maps and reduce dimensionality while preserving key information. Multiple convolutional layers can be stacked, though two layers were found to balance complexity and generalisation. The below interactive animation aims to illustrate how the filtering process works.\n\n\n\nCredit: OpenAI (2025a and 2025b)\n\nRNNs are designed to capture sequential dependencies by incorporating feedback connections. At each time step, the hidden state depends not only on the current input but also on the state from the previous step. This architecture makes RNNs well suited for modeling temporal dynamics in sequential data.\nA FFNN-MLP consists of an input layer, hidden layers of perceptron nodes, and an output layer. Each hidden node computes a weighted sum of its inputs, applies a non-linear activation, and forwards the result to the next layer. The weights are initialised randomly and updated through backpropogation using an optimiser such as gradient descent. Activation functions (e.g., ReLu, sigmoid) introduce non-linearity enabling the model to capture complex relationships between predictors and the target.\n\nData Preprocessing of Neural Networks models\nTo prepare the data for CNN and RNN, we constructed a 3-dimensional array (number of samples × the time window length × the number of features). Each training sample was created by sliding a fixed window across the data. Within the array, columns represent time steps, and depth represent data features. Since we predict one day ahead, each sample represents the observations within the window, for the next day prediction. Data were standardised to avoid distortion from differing feature scales, and one-hot encoded so each categorical variable is split into multiple binary variable demarcating each category.\nTo preserve temporal trends for the CNN and RNN, the train and test split was not randomly shuffled. Instead, the first 80% of records were used for training, while the remainder was used for testing, similar to the approach of Fromm and Schonberger (2022).\n\n\nConvolutional Neural Network\nCNN and RNN hyperparameter selection involved two stages. Firstly, several candidate datasets were created with time windows of 3-days, 7-days, 30-days and 120-days. A narrow search was used, training many models to identify the most appropriate window based on validation accuracy, estimated using cross-validation with a 20% holdout set. All models were trained for 60 epochs, except the FFNN-MLP.\nIn the second stage, a broader grid-search was conducted on the chosen window, selecting the model with the highest validation accuracy. In all our tests presented for the CNN, we assessed architectures with two convolutional layers using the ReLU function in those latent layers, and the softmax function in our input layer to create the class predictions. More complex architectures did not perform meaningfully better. Measures to control for class imbalance were taken for both the CNN and RNN by calculating weights, inversely proportional to the frequency of observations of that class in the training set.\n\n\n\n\n\n\n\n\nFigure 8: CNN prediction window search.\n\n\n\n\n\nConsidering Figure 8 above, we chose a 7-day prediction window. Overall, most models performed best with 3-7 days, while 30- and 120-day windows were meaningfully worse. Validation accuracy was competitive between 3 and-7-days. However, 7-days appeared more robust, as some 3-day models showed much lower accuracy at the extremes.\n\n\n\n\nTable 4: Expanded grid-search hyperparameters and values.\n\n\n\n\n\n\nhyperparameter\nvalues\n\n\n\n\nfilters1\n16, 32, 64\n\n\nfilters2\n16, 32, 64\n\n\nkernel_sz\n5, 15\n\n\ndrop1\n0.5, 0.7\n\n\ndrop2\n0.5, 0.7\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 9: Top 30 configurations by validation accuracy for CNN.\n\n\n\n\n\nAfter completing the initial window search, an expanded grid-search using the hyperparameters presented in Table 4 was conducted. The validation accuracy for the top 30 models is presented in Figure 9 .The best performing model during the initial prediction window search (the first stage of the search) achieved a validation accuracy of 67, while the top model using the chosen window in the second stage of the search reached 69.1%. We stuck with this model, as validation accuracy had not improved meaningfully over the course of the expanded search. The final selected model involved 32 filters in the first layer, followed by a dropout of 70%, then 16 layers in the second layers, followed by dropout of 70% and global average pooling into the final output layer, and 5 Kernels (Filters).\n\n\nRecurrent Neural Network\nThe approach for the RNN was the same as the CNN discussed above. Firstly, a search over numerous candidate window sizes were completed, and a subsequent expanded search was done as well. In Figure 10 below, the validation accuracy of the models test is presented. We used a combination of two Long Short-Term memory layers (LSTM), with dropout and varied the LSTM units and dropout values in the grid-search.\n\n\n\n\n\n\n\n\nFigure 10: RNN prediction window search.\n\n\n\n\n\nThe RNN window search showed more competitive performance. In this case, a window of 30-days appears more robust overall, containing fewer worse performing models towards the extreme end. We therefore conducted an expanded grid-search using this window, the results of which are presented in Figure 11 below. The grid-search hyperparameters are listed in Table 5.\n\n\n\n\nTable 5: Expanded grid-search hyperparameters and values.\n\n\n\n\n\n\nHyperparameter\nValues\n\n\n\n\nunits1\n16, 32, 64\n\n\nunits2\n16, 32, 64\n\n\ndrop1\n0.3, 0.5, 0.7\n\n\ndrop2\n0.3, 0.5, 0.7\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 11: Top 30 configurations by validation accuracy for RNN.\n\n\n\n\n\nFrom the expanded grid-search, the best performing model achieved a validation accuracy of 64.8%. We fitted this model to the full training data and used that to predict onto the test set. Overall, we were satisfied with this search as the validation accuracy had not improved drastically compared to the initial window search; for example, the best model in the first stage reached roughly 60%.\n\n\nFeedforward Multilayer Perceptron Neural Network\nThe FFNN-MLP architecture was optimised through a randomised grid-search to identify the most effective combination of hyperparameters. Candidate models has consistent input and output layers, while hidden layers varied. The input layer comprised 61 nodes, corresponding to the full set of hot-one encoded engineered predictor variables, plus bias.\nThe grid-search explored 2-4 hidden layers with neuron configurations such as [256, 128, 64], [512, 256, 128], and [128, 64]. The RectifierWithDropout activation promoted non-linearity and mitigated overfitting. The output layer had five neurons, with softmax to generate class probabilities.\nTo address class imbalance, each training observation was weighted inversely to its class frequency, increasing the influence of minority classes during optimisation.\nThe randomised grid-search evaluated a maximum of 216 candidate models, exploring a predefined hyperparameter space. Key hyperparameters and their search ranges are detailed in Table 6 .\n\n\n\n\nTable 6: FFNN-MLP Hyperparameter Grid.\n\n\n\n\n\n\nHyperparameter\nValues\n\n\n\n\nhidden\n(128,128); (128,64); (64,32,16); (64,32)\n\n\nactivation\nRectifierWithDropout\n\n\nl1\n0, 1e-4, 1e-5\n\n\nl2\n0, 1e-4, 1e-5\n\n\nrate\n0.01, 0.001\n\n\ninput dropout ratio\n0.1, 0.3, 0.5\n\n\n\n\n\n\n\n\n\n\nModels were trained for up to 200 epochs with early stopping, terminating if validation logloss did not improve by at least 1e-4 over 10 consecutive intervals."
  },
  {
    "objectID": "DS4i_writeup.html#results-and-discussion",
    "href": "DS4i_writeup.html#results-and-discussion",
    "title": "DS4I - Project Writeup",
    "section": "Results and Discussion",
    "text": "Results and Discussion\n\nConfusion Matrices\nFigure 12 displays the confusion matrices for our 3 models. Our FFNN-MLP has a clearer diagonal, which is indicative of its stronger performance. The confusion matrix for the RNN is very diffused with widespread misclassifications. Table 8 confirms this observation is indeed the case, as the RNN had the lowest accuracy (59%) and the FFNN-MLP had the highest (75%). The FFN-MLP accuracy just narrowly beats the accuracy of the forecasted values (FAH) which had an accuracy of 74. One limitation of the approach for evaluating the CNN and RNN is that, since the training and tests sets are not shuffled, and “High” risk avalanches are rare, there are no “High” risk avalanches in the test set. Therefore, the models ability to distinguish “High” risk cases is not assessed here.\nThe diagonal of the RNN is the weakest of them all. Its correct predictions are concentrated in “Low” (752) with spillover to “Moderate” (165), and its misclassifications are concentrated in “Low” (197) and “Moderate” (299) predictions which were actually “Moderate” instances. This tells us the model often confused “Moderate” cases as either “Low” (underestimated hazard) or “Moderate” (overestimated hazard).\nIn the case of the CNN, almost all predictions go to “Low”. There is a noticeable concentration on the diagonal for “Low” classifications (735), and a considerable concentration of “Moderate” classifications (270). On the off-diagonal, the model misclassified 205 “Low” instances as “Moderate”, and 148 “Moderate” instances as “Low”. Clearly, the CNN collapsed into predicting these two classes in many cases and often confused the two classes. The rows for these respective classes are all zeros. This shows that this model fell for imbalance bias, favouring the majority class “Low” and second most abundance “Moderate”.\nThe FFNN-MLP had some noticeable misclassifications between “Considerable-” and “Moderate”. We see that for 103 cases, the model predicted “Considerable-” instead of “Moderate”, which shows that it confuses those two classes a lot. Similarly, 112 Low cases were predicted as “Moderate”, another pair of classes the model struggles with.\n\n\n\n\n\n\n\n\nFigure 12: Confusion Matrices Comparing the Final CNN, RNN, and Feed-Forward Models\n\n\n\n\n\n\n\n\nTable 7\n\n\n\n\n\n\n\n\n\n\nOverall Model Performance\nIn Table 8 below, an overview of each model’s performance is presented. The FFNN-MLP performs best, achieving the highest accuracy, recall, specificity, and precision scores. The CNN serves as a middle ground, with acceptable accuracy but low recall. The RNN model performs the worst overall. In particular its recall and precision are fairly low which cast its practical reliability into doubt.\n\n\n\n\nTable 8: Overall Model performance Statistics\n\n\n\n\n\n\n\nAccuracy\nKappa\nRecall\nSpecificity\nPrecision\n\n\n\n\nFFNN\n0.754\n0.645\n0.618\n0.934\n0.633\n\n\nCNN\n0.650\n0.353\n0.407\n0.880\n0.439\n\n\nRNN\n0.594\n0.222\n0.368\n0.849\n0.311\n\n\n\n\n\n\n\n\n\n\nThe FFNN-MLP’s specificity was 0.934, the highest value observed. This tells us that this model is very good at ruling out cases that do not belong to a class. Its recall was 0.618, which means the model correctly identifies ~62% of true cases or in other words, it catches more actual danger levels compared to CNN and RNN. The CNN and RNN specificity scores were 0.880 and 0.849 respectively, which indicates they have fair ability to avoid false alarms of avalanche hazards, but not up to par with the FFNN-MLP’s ability. Notably, the RNN’s precision was 0.311, which means when it predicts, its only correct about 31% of the time. This means the RNN is wrong more often than its not. Contrast that with the CNN’s 0.439 precision (correct a little under half the time) and the FFNN-MLP’s 0.633 (correct around 63% of the time).\n\n\nModel Statistics by Class\nLooking at performance statistics by class can help us gain useful insight into each models strengths and weaknesses, providing a more detailed overview of model behaviour. Table 11, Table 9 and Table 10 summarise our model statistics by class.\n\n\n\n\nTable 9: CNN Statistics by Class\n\n\n\n\n\n\n\nSensitivity\nSpecificity\nPos Pred Value\nNeg Pred Value\nPrecision\nRecall\nF1\nPrevalence\nDetection Rate\nDetection Prevalence\nBalanced Accuracy\n\n\n\n\nClass: low\n0.0350877\n0.9951824\n0.3636364\n0.9293059\n0.3636364\n0.0350877\n0.0640000\n0.0727505\n0.0025526\n0.0070198\n0.5151351\n\n\nClass: moderate\n0.2368421\n0.9444081\n0.0957447\n0.9803123\n0.0957447\n0.2368421\n0.1363636\n0.0242502\n0.0057435\n0.0599872\n0.5906251\n\n\nClass: considerable -\nNA\n1.0000000\nNA\nNA\nNA\nNA\nNA\n0.0000000\n0.0000000\n0.0000000\nNA\n\n\nClass: considerable +\n0.7736842\n0.7179903\n0.8085809\n0.6732523\n0.8085809\n0.7736842\n0.7907477\n0.6062540\n0.4690491\n0.5800893\n0.7458372\n\n\nClass: high\n0.5806452\n0.7431942\n0.4882459\n0.8076923\n0.4882459\n0.5806452\n0.5304519\n0.2967454\n0.1723038\n0.3529036\n0.6619197\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTable 10: RNN Statistics by Class\n\n\n\n\n\n\n\nSensitivity\nSpecificity\nPos Pred Value\nNeg Pred Value\nPrecision\nRecall\nF1\nPrevalence\nDetection Rate\nDetection Prevalence\nBalanced Accuracy\n\n\n\n\nClass: low\n0.0000000\n0.9986207\n0.0000000\n0.9276105\n0.0000000\n0.0000000\nNaN\n0.0722969\n0.0000000\n0.0012796\n0.4993103\n\n\nClass: moderate\n0.3243243\n0.9141547\n0.0839161\n0.9823944\n0.0839161\n0.3243243\n0.1333333\n0.0236724\n0.0076775\n0.0914907\n0.6192395\n\n\nClass: considerable -\nNA\n1.0000000\nNA\nNA\nNA\nNA\nNA\n0.0000000\n0.0000000\n0.0000000\nNA\n\n\nClass: considerable +\n0.7924131\n0.5244300\n0.7203065\n0.6204239\n0.7203065\n0.7924131\n0.7546412\n0.6071657\n0.4811260\n0.6679463\n0.6584215\n\n\nClass: high\n0.3556034\n0.8098271\n0.4411765\n0.7485282\n0.4411765\n0.3556034\n0.3937947\n0.2968650\n0.1055662\n0.2392834\n0.5827153\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTable 11: FFNN Statistics by Class\n\n\n\n\n\n\n\nSensitivity\nSpecificity\nPos Pred Value\nNeg Pred Value\nPrecision\nRecall\nF1\nPrevalence\nDetection Rate\nDetection Prevalence\nBalanced Accuracy\n\n\n\n\nClass: low\n0.8208270\n0.9792350\n0.9657658\n0.8845015\n0.9657658\n0.8208270\n0.8874172\n0.4164541\n0.3418367\n0.3539541\n0.9000310\n\n\nClass: moderate\n0.7515275\n0.8523677\n0.6988636\n0.8826923\n0.6988636\n0.7515275\n0.7242395\n0.3131378\n0.2353316\n0.3367347\n0.8019476\n\n\nClass: considerable -\n0.7812500\n0.8613782\n0.5910165\n0.9388646\n0.5910165\n0.7812500\n0.6729475\n0.2040816\n0.1594388\n0.2697704\n0.8213141\n\n\nClass: considerable +\n0.1772152\n0.9865682\n0.4117647\n0.9576271\n0.4117647\n0.1772152\n0.2477876\n0.0503827\n0.0089286\n0.0216837\n0.5818917\n\n\nClass: high\n0.5600000\n0.9909268\n0.5000000\n0.9928571\n0.5000000\n0.5600000\n0.5283019\n0.0159439\n0.0089286\n0.0178571\n0.7754634\n\n\n\n\n\n\n\n\n\n\nThe CNN statistics detailed in Table 9 show that this model had mixed results. It was able to capture 82% of “Low” danger cases, which is the majority class, but its performance dropped sharply for the minority cases. Additionally, it only detected 3.5% of “Considerable -” cases.\nOn a class-by-class basis, the RNN performed poorly overall. It was able to catch about 79% of “Low” hazard cases. Beyond that, its performance collapsed. It missed all “Considerable-” cases (0 sensitivity overall). It only managed to detect 35% of “Moderate” cases. Moreover, its specificity for the “Low” class was just 52%, meaning it frequently mislabelled other classes as “Low”.\nHowever, the FFNN-MLP (Table 11) was especially good at recognising the common classes such as “Low” and “Moderate” danger, correctly identifying around 82% of “Low” cases and 75% of “Moderate” cases. This means that it both caught most of the true cases and rarely mislabelled other classes as these. However, the FFNN-MLP struggled with the rarer categories. For example, it only detected 18% of “Considerable +” cases. This suggests some model bias towards classes that were better represented.\n\n\nFeature Importance\nIn Figure 13, the feature importance plot from the FFNN-MLP is shown, for the first 30 most important variables.\nThe avalanche forecast varibles were highly influential. FAH dominates, with FAH.Moderate, FAH.Low, FAH.Considerable-, and FAH.Considerable+ as the top predictors. Altitude (Alt) ranks just below FAH, while snow.index and the seasonal variables we constructed (season12, season5, season3) have moderate influence. This suggests seasonal trends do matter, but no single season dominates the prediction. If avalanches were restricted to one specific time/season window, we would have seen that season stand out much more.\nGeographical area (Area.Torridon, Area.Glencoe) contributes minimally, indicating spatial differences are less important than FAH. Precipitation ranks lowest, adding little predictive power beyond snow index, which is clearly more influential.\nOverall, the FAH drives predictions, followed by altitude and snow.index, with seasonal, spatial, and precipitation factors also adding influence to the model.\n\n\n\n\n\n\n\n\nFigure 13: Variable Importance for FFN-MLP.\n\n\n\n\n\n\n\nSummary of Findings\nIn this study, we compared the ability of CNN, RNN, and FFNN-MLP models in predicting avalanches hazard levels. This was a multi-factorial investigation, as many different variables contribute to avalanche outcomes. The nature of this problem is sequential and we were able to preprocess that data to fit a time series context. We expected the CNN and RNN to outperform the FFNN-MLP since they have the added time series advantage.\nThe FFNN-MLP turned out to be the best-performing model overall. It showed the most balanced detection and achieved the best performance across hazard levels. The CNN struggled with the minority classes and mainly acted as a majority-class (“Low” hazard) detector despite the controls put in place to re-weight the classes. This is unsatisfactory because in a real forecasting setting, it’s of great importance that we can detect the higher-risk cases. The RNN performed worst overall, missing nearly all the minority class instances (“Considerable+”, “Considerable-”), and doing fairly well with the majority cases but still not as well as the other two models. This strongly disagrees with our initial hypothesis that sequential models would benefit from time series dependencies.\nA clear strength across all models is that they handled the majority class fairly well. On the other hand, all three models showed clear bias toward the most common categories. None of them did especially well with the higher-risk, minority classes. The highlights a weakness in applying these models directly without addressing the class imbalance and customising the class weights more in favour of the minority classes could help counteract this measure.\n\n\nImprovements & Future Contributions\nWhile the present study used only complete cases for modelling, avalanche forecasters in practice must often make predictions despite missing data. Imputation was considered, but fell outside the scope of evaluating NN performance. Developing an imputation model for this dataset could be a way of meaningfully improving its practical use, robustness, and accuracy overall. Secondly, Incorporating climate change effects by accounting for evolving weather and snowpack conditions alongside historical data could be a helpful tactic for researchers. Our models show that seasonal patterns are influential in predicting the risk. One reason this may be, is due to the evolving impact of climate change on the snow pack and ice condition. Furthermore, one may consider exploring region-specific models to reflect Scotland’s highly localized weather patterns and improve forecast reliability. Lastly, We restricted our hypothesis set to neural network models only, but these may not be best suited to this data. Therefore, exploring alternative model types, such as gradient boosting or hybrid deep learning ensembles, to better handle categorical data and strengthen predictions could allow for more accurate prediction."
  },
  {
    "objectID": "DS4i_writeup.html#conclusion",
    "href": "DS4i_writeup.html#conclusion",
    "title": "DS4I - Project Writeup",
    "section": "Conclusion",
    "text": "Conclusion\nOur objective requires reliable detection across all avalanche danger levels, not just the majority ones. In this study, the FFNN-MLP came closest to meeting this requirement, but even it struggled with the minority, high-risk cases. Future work should focus on reducing class imbalance, exploring region-specific models, and experimenting with ensemble approaches to ensure that high-risk avalanche conditions are consistently and accurately detected."
  },
  {
    "objectID": "DS4i_writeup.html#references",
    "href": "DS4i_writeup.html#references",
    "title": "DS4I - Project Writeup",
    "section": "References",
    "text": "References\n\nBain, D. 2024. Avalanche awareness in Scotland. Sports Scotland Glenmore Lodge. Available: https://www.glenmorelodge.org.uk/avalanche-awareness-in-scotland/ [2025, September 11].\nBeattie, B. 1976. The densification of a seasonal snowpack in the Cairngorms with relation to avalanches. B.Sc. (professional) dissertation. University of Lancaster.\nBlagovechshenskiy, V., Medeu, A., Gulyayeva, T., Zhdanov, V., Ranova, S., Kamalbekova, A. & Aldabergen, U. 2023. Application of artificial intelligence in the assessment and forecast of avalanche danger in the Ile Alatau Ridge. Water. 15(7). DOI: 10.3390/w15071438\nBritannica. 2026. Ben Nevis. Available: https://www.britannica.com/place/Ben-Nevis [2025, September 10].\nChoubin, B., Borji, M., Mosavi, A., Sajedi-Hosseini, F., Singh, V.P. & Shamshirband, S. 2019. Snow avalanche hazard prediction using machine learning methods. Journal of Hydrology. 577. DOI: 10.1016/j.jhydrol.2019.123929\nDiggins, M. 2009. The challenges for Scottish avalanche forecasters observing a maritime snowpack. Proceedings of the International Snow Science Workshop. 22-24. Available: https://arc.lib.montana.edu/snow-science/objects/issw-2009-0024-0026.pdf [2025, September 10].\nFromm, R. & Schonberger, C. 2022. Estimating the danger of snow avalanches with machine learning approach using a comprehensive snow cover model. Machine Learning with Applications. 10. DOI: 10.1016/j.mlwa.2022.100405\nGauthier, F., Laliberte, J. & Meloche, F. 2025. Assessing the predictive capability of several machine learning algorithms to forecast snow avalanches using numerical weather prediction model in eastern Canada. EGUsphere. DOI: 10.5194/egusphere-2025-1572\nHeierli, J., Purves, R.S., Felber, A. & Kowalski, J. 2004. Verification of nearest-neighbours interpretations in avalanche forecasting. Annals of Glaciology. 38:84-88. DOI: 10.3189/172756404781815095\nHelbig, N., van Herwijnen, A. & Jonas, T. 2015. Forecasting wet-snow avalanche probability in mountainous terrain. Cold Regions Science and Technology. 120:219-226. DOI: 10.1016/j.coldregions.2015.07.001\nHendrick, M., Techel, F., Volpi, M., Olvski, T., Perez-Guillen, van Herwijnen, A. & Schweizer, J. 2023. Automated prediction of wet-snow avalanche activity in the Swiss Alps. Journal of Glaciology. 69(277):1365-1378. DOI: 10.1017/jog.2023.24\nKala, M., Jain, S., Singh, A. & Krishnan, N.C. 2025. Addressing class imbalance in avalanche forecasting. Cold Regions Science and Technology. 231. DOI: 10.1016/j.coldregions.2024.104411\nLangmuir, E. 1970. Snow profiles in Scotland. Weather. 25(5):205-209. DOI: 10.1002/j.1477-8696.1970.tb03262.x\nMetOffice. 2010. UK regional climates. Available: https://www.metoffice.gov.uk/research/climate/maps-and-data/regional-climates/index [2025, September 11].\nOpenAI. (2025a). ChatGPT (Sept 23 version) [Large language model]. https://chatgpt.com/share/68d59a8c-efa4-8004-a295-616e83f37afb\nOpenAI. (2025b). ChatGPT (Sept 23 version) [Large language model]. https://chatgpt.com/share/68d59ad3-d50c-8004-b862-18966277b186\nPozdnoukhov, A., Purves, R.S. & Kanevski, M. 2008. Applying machine learning methods to avalanche forecasting. Annals of Glaciology. 49:107-113. DOI: 10.3189/172756408787814870\nPozdnoukhov, A., Matasci, G., Kanevski, M. & Purves, R.S. 2011. Spatio-temporal avalanche forecasting with Support Vector Machines. Nat. Hazards Earth Syst. Sci. 11:367-382. DOI: 10.5194/nhess-11-367-2011\nPurves, R.S., Morrison, K.W., Moss, G. & Wright, D.S.B. Nearest neighbours for avalanche forecasting in Scotland - Development, verification and optimisation of a model. Cold Regions Science and Technology. 37(3):343-355. DOI: 10.1016/S0165-232X(03)00075-2\nRahmati, O., Ghorbanzadeh, O., Teimurian, T., Mohammadi, F., Tiefenbacher, J.P., Falah, F., Pirasteh, S., Ngo, P.T.T. & Bui, D.T. 2019. Spatial modeling of snow avalanche using machine learning models and geo-environmental factors: Comparison of effectiveness in two mountain regions. Remote Sensing. 11(24). DOI: 10.3390/rs11242995\nSchirmer, M., Lehning, M. & Schweizer, J. 2009. Statistical forecasting of regional avalanche danger using simulated snow-cover data. Journal of Glaciology. 55(193):761-768. DOI: 10.3189/002214309790152429\nScottish Avalanche Information Service. 2024. Reviews of the winter season. Available: https://www.sais.gov.uk/sais-annual-reports/ [2025, September 13].\nScottish Avalanche Information Service. 2025. Avalanche information for the Scottish mountains. Available: https://www.sais.gov.uk/ [2025, September 5].\nScottish Government. 2011. Scotland’s marine atlas: Information for the National Marine Plan. Available: https://www.gov.scot/publications/scotlands-marine-atlas-information-national-marine-plan/pages/7/ [2025, September, 10].\nSharma, V., Kumar, S. & Sushil, R. 2023. A neural network model for automated prediction of avalanche danger level. Nat. Hazards Earth Syst. Sci. 23:2523-2530. DOI: 10.5194/nhess-23-2523-2023\nSingh, A. & Ganju, A. 2008. Artificial neural networks for snow avalanche forecasting in Indian Himalaya. Proceedings of the 12th International Conference of International Association for Computer Methods and Advances in Geomechanics (IACMAG). 1-6 October 2008. Gao, India. 1664-1670. Available: https://www.researchgate.net/publication/263651736_Artificial_Neural_Networks_for_Snow_Avalanche_Forecasting_in_Indian_Himalaya [2025, September 13].\nSpink, P.C. 1970. Scottish snowbeds in summer 1969. Weather. 25(5): 201-204. DOI: 10.1002/j.1477-8696.1970.tb03261.x\nTiwari, A., Arun, G. & Vishwakarma, B.D. 2021. Parameter importance assessment improves efficacy of machine learning methods for predicting snow avalanche sites in Leh-Manali Highway, India. Science of The Total Environment. 794. DOI: 10.1016/j.scitotenv.2021.148738\nTu, J.V. 1996. Advantages and disadvantages of using artificial neural network versus logistic regression for predicting medical outcomes. Journal of Clinical Epidemiology. 49(11):1225-1231. DOI: 10.1016/S0895-4356(96)00002-9\nWard, R.G.W. 1980. Avalanche hazard in the Cairngorm Mountains, Scotland. Journal of Glaciology. 26(94):31-41. DOI: 10.3189/S0022143000201020\nWard, R.G.W. 1984a. Avalanche prediction in Scotland: I. A survey of avalanche activity. Applied Geography. 4(2):91-108. DOI: 10.1016/0143-6228(84)90016-X\nWard, R.G.W. 1984b. Avalanche prediction in Scotland: II. Development of a predictive model. Applied Geography. 4(2):109-133. DOI: 10.1016/0143-6228(84)90017-1\nWard, R.G.W., Langmuir, E.D.G. & Beattie, B. 1985. Snow profiles and avalanche activity in the Cairngorm Mountains, Scotland. Journal of Glaciology. 31(107):18-27. DOI: 10.3189/S0022143000004949\nWebster, H. 2020. Avalanches in Scotland - interview with Mark Diggins. Walkhighlands. Available: https://www.walkhighlands.co.uk/news/avalanches-in-scotland-interview-with-mark-diggins/ [2025, September 10].\nWen, H., Wu, X., Liao, X., Wang, D., Huang, K. & Wunnemann, B. 2022. Application of machine learning methods for snow avalanche susceptibility mapping in the Parlung Tsangpo catchement, southeastern Qinghai-Tibet Plateau. Cold Regions Science and Technology. 198. DOI: 10.1016/j.coldregions.2022.103535\nWerritty, A. & Sugden, D. 2012. Climate change and Scotland: Recent trends and impacts. Earth and Environmental Science Transactions of The Royal Society of Edinburgh. 103(2):133-147. DOI: 10.1017/S1755691013000030\nWSL Institute for Snow and Avalanche Research SLF. 2016. Scotland - Avalanche warning experts exchange. Available: https://www.slf.ch/en/news/scotland-avalanche-warning-experts-exchange/?utm_source=chatgpt.com [2025, September 12]."
  },
  {
    "objectID": "plag_decl.html",
    "href": "plag_decl.html",
    "title": "Plagarism Declaration",
    "section": "",
    "text": "Department of Statistical Sciences Plagiarism Declaration form\nA copy of this form, completed and signed, to be attached to all coursework submissions to the Statistical Sciences Department. Submissions without this form will not be marked.\nCOURSE CODE: STA5073Z\nCOURSE NAME: Data Science for Industry\nSTUDENT NAMES: Don Gwese; Matthew Russell; Nina Lewis; Aidan Thomas; Sanana Mwanawina\nSTUDENT NUMBERS: GWSDON002; RSSMAT013; LWSNIN001; THMAID005; MWNSAN002\n\nI know that plagiarism is wrong. Plagiarism is to use another’s work and pretend that it is one’s own.\nI have used a generally accepted citation and referencing style. Each contribution to, and quotation in, this tutorial/report/project from the work(s) of other people has been attributed, and has been cited and referenced.\nThis tutorial/report/project is our own work.\nI have not allowed, and will not allow, anyone to copy our work with the intention of passing it off as his or her own work.\nI acknowledge that copying someone else’s assignment or essay, or part of it, is wrong, and declare that this is our own work.\n\nNote that agreement to this statement does not exonerate you from UCT’s Academic Misconduct policy (https://uct.ac.za/media/584410).\nSignature: D.Gwese; M.Russell; N.Lewis; A.Thomas; S.Mwanawina\nDate: 28 September 2025"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Home",
    "section": "",
    "text": "Welcome to My Project\n\nFull Report\nProject Template\nLLM Use"
  }
]