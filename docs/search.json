[
  {
    "objectID": "plag_decl.html",
    "href": "plag_decl.html",
    "title": "Plagarism Declaration",
    "section": "",
    "text": "Department of Statistical Sciences Plagiarism Declaration form\nA copy of this form, completed and signed, to be attached to all coursework submissions to the Statistical Sciences Department. Submissions without this form will not be marked.\nCOURSE CODE: STA5073Z\nCOURSE NAME: Data Science for Industry\nSTUDENT NAMES: Don Gwese; Matthew Russell; Nina Lewis; Aidan Thomas; Sanana Mwanawina\nSTUDENT NUMBERS: GWSDON002; RSSMAT013; LWSNIN001; THMAID005; MWNSAN002\n\nI know that plagiarism is wrong. Plagiarism is to use another‚Äôs work and pretend that it is one‚Äôs own.\nI have used a generally accepted citation and referencing style. Each contribution to, and quotation in, this tutorial/report/project from the work(s) of other people has been attributed, and has been cited and referenced.\nThis tutorial/report/project is our own work.\nI have not allowed, and will not allow, anyone to copy our work with the intention of passing it off as his or her own work.\nI acknowledge that copying someone else‚Äôs assignment or essay, or part of it, is wrong, and declare that this is our own work.\n\nNote that agreement to this statement does not exonerate you from UCT‚Äôs Academic Misconduct policy (https://uct.ac.za/media/584410).\nSignature: D.Gwese; M.Russell; N.Lewis; A.Thomas; S.Mwanawina\nDate: 28 September 2025"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Scottish Avalanche Forecasts: Neural Network Modelling",
    "section": "",
    "text": "STA5073Z ‚Äî Data Science for Industry (2025)\n\n\nThis website presents coursework completed in fulfillment of STA5073Z ‚Äì Data Science for Industry (2025).\nThe project uses 15 years of avalanche forecast data from the Scottish Avalanche Information Service (SAIS) to build and evaluate predictive neural network models. The site includes the full scientific report, a description of how we used large language models (LLMs) during the assignment, a signed plagiarism declaration, and the details of our final model."
  },
  {
    "objectID": "index.html#explore-the-project",
    "href": "index.html#explore-the-project",
    "title": "Scottish Avalanche Forecasts: Neural Network Modelling",
    "section": "Explore the Project",
    "text": "Explore the Project\n\n\nüìÑ Full Report\n\n\nScientific paper with data, methods, results, and discussion.\n\n\n\nView Report\n\n\n\nü§ñ LLM Use\n\n\nWhat we used LLMs for, prompt design, and reflections on performance.\n\n\n\nView LLM Use\n\n\n\n‚úÖ Plagiarism Declaration\n\n\nSigned statement of academic integrity for the submission.\n\n\n\nView Declaration\n\n\n\nüß† Final Model\n\n\nArchitecture, training details, evaluation, and feature importance.\n\n\n\nView Model"
  },
  {
    "objectID": "DS4i_writeup.html",
    "href": "DS4i_writeup.html",
    "title": "Exploring Neural Networks for Snow Avalanche Forecasting in Scotland: A Multi-Factorial Investigation",
    "section": "",
    "text": "Photo by Nicolas Cool (2016) on Unsplash"
  },
  {
    "objectID": "DS4i_writeup.html#abstract",
    "href": "DS4i_writeup.html#abstract",
    "title": "Exploring Neural Networks for Snow Avalanche Forecasting in Scotland: A Multi-Factorial Investigation",
    "section": "Abstract",
    "text": "Abstract\nThis study evaluates the potential of neural networks for avalanche forecasting in Scotland, a region where research has been sparse and limited in scope. Using a 15-year archive of avalanche forecasts from the Scottish Avalanche Information Service (SAIS), we compared three architectures: a feedforward multilayer perceptron (FFNN-MLP), a convolutional neural network (CNN), and a recurrent neural network (RNN). Each was selected for its theoretical strengths. Given the sequential nature of avalanche risk, we hypothesised that the CNN and RNN would outperform the FFNN-MLP.\nContrary to this expectation, the FFNN-MLP achieved the best overall accuracy (75%) and the most balanced detection across hazard levels. The CNN (65% accuracy) and RNN (59% accuracy) both exhibited strong bias toward the majority ‚ÄúLow‚Äù and ‚ÄúModerate‚Äù classes, and performed poorly on higher-risk cases. Even the FFNN-MLP struggled to identify rare but critical events, with a ‚ÄúHigh‚Äù hazard sensitivity (0.56).\nThese findings highlight both the promise and limitations of applying neural networks directly to avalanche hazard forecasting. While the FFNN-MLP proved most effective in this context, improving detection of minority, high-risk categories remain essential. Future research should prioritise methods for addressing class imbalance, region-specific modelling, and ensemble strategies to better capture Scotland‚Äôs diverse avalanche dynamics under changing climate conditions."
  },
  {
    "objectID": "DS4i_writeup.html#introduction-and-literature-review",
    "href": "DS4i_writeup.html#introduction-and-literature-review",
    "title": "Exploring Neural Networks for Snow Avalanche Forecasting in Scotland: A Multi-Factorial Investigation",
    "section": "Introduction and Literature Review",
    "text": "Introduction and Literature Review\nSnow avalanches are a hazard in snow-prone mountains. Unlike more alpine regions, avalanches in Scotland rarely impact settlements and infrastructure, but pose risks to recreational users (Bain, 2024; Diggins, 2009; Ward, 1980; WSL Institute for Snow and Avalanche Research SLF [WSL], 2016). Rising popularity of winter activities such as climbing, walking, and skiing has increased exposure to avalanche-prone terrain, resulting in injuries and occasional fatalities each year (Scottish Avalanche Information Service [SAIS], 2024; Webster, 2020, WSL, 2016). These risks prompted studies of Scottish snowpack and forecasting strategies beginning in the 1970s (Langmuir, 1970; Spink, 1970; Beattie, 1976; Ward, 1980; Ward et al., 1985). However, early work was largely descriptive, relying on snow-pit measurements and expert judgement. Building on this, Ward (1984a and 1984b) applied one of the first predictive avalanche forecasting models for Scotland.\nAvalanche modelling is inherently complex, where predictions depend on multiple interacting factors (Choubin et al., 2019; Helbig et al., 2015; Hendrick et al., 2023; Pozdnoukhov et al., 2008; Singh & Ganju, 2008). This challenge is heightened in regions with variable weather (Sharma & Ganju, 1999). Early statistical methods, like nearest neighbours, supported forecasters by relating current weather to past avalanche events, however, were prone to overfitting and struggled with high-dimensional data (Blagovechshenskiy et al., 2023; Kala et al., 2025; Pozdnoukhov et al., 2018; Singh & Ganju, 2008). Using a dataset from Lochaber, Scotland, Pozdnoukhov et al.¬†(2008) reported that support vector machines (SVMs) performed similarly to nearest neighbours while effectively handling high-dimensional data and producing richer forecasts. SVMs have also proven successful in Iran, India, Switzerland, and Tibet (Choubin et al., 2019; Rahmati et al., 2019; Schirmer et al., 2009; Tiwari et al., 2021; Wen et al., 2022).¬†\nRecently, complex machine learning (ML) algorithms (regression trees, random forests, neural networks) have demonstrated strong predictive performance in avalanche forecasting (Blagovechshenskiy et al., 2023; Choubin et al., 2019; Gauthier et al., 2025; Hendrick et al., 2023; Rahmati et al., 2019; Singh & Ganju, 2018; Tiwari et al., 2021; Wen et al., 2022). Avalanche datasets are often high-dimensional, and using forecaster-led feature selection can be highly subjective (Helbig et al., 2015; Pozdnoukhov et al., 2008). Hybrid approaches combining ML tools with expert-led feature selection have shown practical value (Gauthier et al., 2025). However, fully data-driven forecasting methods, particularly neural networks (NNs), offer important advantages.\nNNs simulate brain processes through interconnected nodes, capturing complex, nonlinear relationships (Blagovechshenskiy et al., 2023; Sharma et al., 2023; Tu, 1996). By processing inputs simultaneously, patterns are learned without prior knowledge of which variables matter most (Blagovechshenskiy et al., 2023; Fromm & Schonberger, 2022; Sharma et al., 2023). Applications in Switzerland, Kazakhstan, and India show that NNs not only perform strongly, but can assess the relative importance of variables (Fromm & Schonberger; Sharma et al., 2023; Singh & Ganju, 2008). This makes them particularly well-suited for high-dimensional avalanche datasets and situations with limited domain knowledge.\nDespite advances in data-driven avalanche forecasting worldwide, research specific to Scotland remains limited in three ways:\n\nFew published studies exist (only 9 relevant peer-reviewed papers exist on Scopus, with the most recent study from 2011 (keywords: ( ‚Äúavalanche forecasting‚Äù OR ‚Äúavalanche prediction‚Äù ) AND Scotland)).\nThe restricted range of statistical approaches explored (only KNN and SVM) (Heierli et al., 2004; Pozdnoukhov et al., 2008; Pozdnoukhov et al., 2011; Purves et al., 2003).\nThe focus on only a few regions in Scotland (primarily the Cairngorms and Lochaber regions).\n\nThis limited interest likely stems from the highly localised nature of avalanches in Scotland, which generally occur in remote areas and affect relatively few people (Diggins, 2009; SAIS, 2024). Nevertheless, improved avalanche forecasting is increasingly important under changing climate conditions, which may alter snowpack properties and avalanche risk (Gauthier et al., 2025; Werritty & Sugden, 2013). A detailed discussion of climate change, however, is beyond this paper‚Äôs scope.\nThe present study aims to extend forecasting research in Scotland both spatially and methodologically by applying three NNs (convolutional (CNN), recurrent (RNN), and a feedforward multilater perceptron (FFNN-MLP)) trained on combined data from six regions of Scotland: Creah Meagaidh, Glencoe, Lochaber, Northern Cairngorms, Southern Cairngorms, and Torridon. Each model was chosen for its particular strengths: CNN is good at identifying spatial and short-term temporal patterns, RNN is suited to capturing longer-term sequential trends over time, and FNN-MLP works well with categorical predictors.\nAvalanche risk is sequential in nature, where current conditions are influenced by how weather and snowpack have developed over previous days, weeks, or months. For this reason, we hypothesise that the CNN and RNN will perform better than the FFNN-MLP, as these can account for these temporal patterns."
  },
  {
    "objectID": "DS4i_writeup.html#context-and-dataset",
    "href": "DS4i_writeup.html#context-and-dataset",
    "title": "Exploring Neural Networks for Snow Avalanche Forecasting in Scotland: A Multi-Factorial Investigation",
    "section": "Context and Dataset",
    "text": "Context and Dataset\nScotland has a mild, wet, temperate maritime climate influenced by the North Atlantic Ocean and persistent south-westerly winds (Pozdnoukhov et al., 2008; Scottish Government, 2011). These conditions produce rapid temperature changes, frequent heavy precipitation (snow or rain), and strong winds. With its highest peak, Ben Nevis (1345 m), Scotland‚Äôs mountains are low compared to alpine ranges (~4000 m), resulting in a snowpack that is typically shallower, wetter, and more variable (Britannica, 2025; Pozdnoukhov et al., 2008; WSL, 2016). Rapid freeze-thaw cycles, rain-on-snow, and wind redistribution further destabilise snow, creating avalanche risks that can appear and disappear within a short time (Diggins, 2009; Pozdnoukhov et al., 2008; Purves et al., 2003; Scottish Government, 2011). Snowpack conditions also differ regionally; the West is strongly maritime, with mild and wet winters, while the North and East are colder and drier (MetOffice, 2010). Scotland‚Äôs climate is thus highly variable spatially and temporally, adding to the complexity of avalanche risk prediction.\nThis paper uses a 15-year archive of avalanche forecasts from Scotland across the six areas of Creah Meagaidh, Glencoe, Lochaber, Northern Cairngorms, Southern Cairngorms, and Torridon, produced by the SAIS (2025). Figure¬†1 illustrates the spatial distribution of observations, with points marking individual events and labels indicating area locations. Total observations by area and hazard category (Table¬†1) reflect differences in observation frequency rather than absolute avalanche activity. Cairngorms (Northern and Southern) and Lochaber, located in the colder and drier North and East, have the most recorded observations, whereas Torridon in the maritime West has substantially fewer. ‚ÄúLow‚Äù and ‚ÄúModerate‚Äù hazard levels dominate across all areas, while ‚ÄúHigh‚Äù hazard events are rare and concentrated in Northern Cairngorms and Lochaber.\n\n\n\n\n\n\n\n\nFigure¬†1: Observed avalanche hazard (OAH): Scotland.\n\n\n\n\n\n\nNote: Point observations coloured by hazard level; SAIS areas labelled.\n\n\n\n\n\n\nTable¬†1: Overall total observations by area and hazard category.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArea\nLow\nModerate\nConsiderable -\nConsiderable +\nHigh\nTotal\n\n\n\n\nCreag Meagaidh\n636\n545\n413\n155\n63\n1812\n\n\nGlencoe\n716\n524\n405\n109\n58\n1812\n\n\nLochaber\n597\n498\n506\n116\n56\n1773\n\n\nNorthern Cairngorms\n655\n567\n501\n122\n46\n1891\n\n\nSouthern Cairngorms\n751\n546\n383\n72\n35\n1787\n\n\nTorridon\n678\n373\n85\n7\n0\n1143\n\n\n\n\n\n\n\n\n\nData span winters (November to April) from the December 2009 to March 2025. Predictors are categorised into: (1) position and topography, (2) weather, and (3) snowpack test, all collected at the forecast location. Each observation includes the date and the forecast area (metadata). For an overview of the predictors available, see Table¬†2 .\n\n\n\n\nTable¬†2: Overview of predictor variables available in the avalanche forecast dataset.\n\n\n\n\n\n\n\n\n\n\nPredictor_Group\nVariables\n\n\n\n\nMetadata\ndate, area\n\n\nPosition and Topography\nlongitude, latitude, altitude, aspect of slope, incline of slope\n\n\nWeather\nair temperature, wind direction, wind speed, cloud cover, precipitation code, snowdrift, total snow depth, foot penetration, ski penetration, rain observed at 900m elevation, summit air temperature, summit wind direction, summit wind speed\n\n\nSnowpack Test\nmax. temperature gradient, max. hardness gradient, no.settle, snow.index, insolation, crystals, wetness, AV.Cat, and snow temperature"
  },
  {
    "objectID": "DS4i_writeup.html#exploratory-data-analysis",
    "href": "DS4i_writeup.html#exploratory-data-analysis",
    "title": "Exploring Neural Networks for Snow Avalanche Forecasting in Scotland: A Multi-Factorial Investigation",
    "section": "Exploratory Data Analysis",
    "text": "Exploratory Data Analysis\n\na. Feature Map Correlations\nThe Scottish avalanche dataset contains several features that are moderately to strongly correlated, while others appear largely independent (Figure¬†2). The presence of correlated variables is not inherently problematic, since NNs can learn non-linear interactions. However, strong correlations can increase the risk of overfitting if the network memorises patterns rather than generalising to new data.\nThe highest correlation found was between summit.air.temperature and air.temperature (r = 0.85), indicating strong redundancy. Nevertheless, both variables were retained as each likely capture a different aspect relevant to forecasting. Air.temperature is an identified key driver of avalanche conditions and is expected to be an important predictor in the NNs (Fromm & Schonberger, 2022; Gauthier et al., 2025; Pozdnoukhouv et al., 2018; Souckova et al., 2022; Ward,1980). Exploration of air.temperature across observed.avalanche.hazard (OAH) levels further supports its importance in forecasting (Figure¬†3). Higher hazard categories are generally associated with lower observed air.temperatures, with ‚ÄúConsiderable-‚Äù, ‚ÄúConsiderable+‚Äù, and ‚ÄúHigh‚Äù hazards centred below 0 ¬∞C. ‚ÄúLow‚Äù hazard days show medians above freezing and wider variability. The transition at ‚ÄúModerate‚Äù hazard, where temperatures cluster around 0 ¬∞C, reflects conditions that can either stabilise or destabilise the snowpack through freeze-thaw cycles. These patterns highlight Scotland‚Äôs sensitivity to rapid temperature shifts and their impact on avalanche activity (Diggins, 2009; Pozdnoukhov et al., 2008).\nOther moderate correlations (0.5&lt;|r|&lt; 0.80) remain informative and manageable for NNs.\n\n\n\n\n\n\n\n\nFigure¬†2: Map of feature correlations.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure¬†3: Distribution of air temperature across observed avalanche hazard (OAH).\n\n\n\n\n\n\n\nb. Precipitation Type\nThe distribution of OAH levels varies across different precipitation types (Figure¬†4). ‚ÄúLow‚Äù hazard predominates under conditions of none (0) or trace (2) precipitation, whereas higher hazard categories become increasingly prevalent as snowfall intensity increases. Heavy snow (level 10) is associated with a substantially larger share of ‚ÄúConsiderable‚Äù and ‚ÄúHigh‚Äù hazard ratings (81.9%). This aligns with existing knowledge of the Scottish climate, where rapid weather shifts and frequent rain or snow-on-snow events contribute to unstable snowpack conditions (Diggins, 2009; Pozdnoukhov et al., 2008; Purves et al., 2003; Ward, 1980).\n\n\n\n\n\n\n\n\nFigure¬†4: Precipitation type vs observed avalanche hazard (OAH).\n\n\n\n\n\n\n\nc.¬†Seasonality\nMonthly OAH patterns reveal clear seasonal trends (Figure¬†5). Early (November-December) and late (April-May) months are dominated by ‚ÄúLow‚Äù and ‚ÄúModerate‚Äù hazards, while mid-season (Jan-Mar) sees more ‚ÄúConsiderable‚Äù and ‚ÄúHigh‚Äù hazards. Most forecasts occur between December-March, highlighting the period of greatest avalanche risk (SAIS, 2024). These patterns reflect cyclical snowpack development, with mid-winter weather instability driving higher-risk avalanches (Diggins, 2009; Podzdnoukhov et al., 2008; Purves et al., 2003).\n\n\n\n\n\n\n\n\nFigure¬†5: Seasonality of observed avalanche hazard (OAH): Nov‚ÄìMay.\n\n\n\n\n\n\n\nd.¬†FAH vs OAH\nLastly, Figure¬†6 shows the relationship between forecast.avalanche.hazard (FAH) (day t) and OAH (day t+1). Each column sums to 100%, showing how outcomes were distributed for each forecast level. ‚ÄúLow‚Äù forecasts were highly accurate (86.51%), while intermediate levels (‚ÄúModerate‚Äù, ‚ÄúConsiderable-‚Äù) had weaker agreement, often spilling into neighbouring categories. ‚ÄúHigh‚Äù forecasts were least reliable, frequently over-predicting extreme conditions.\nThe agreement between OAH and FAH are highest in stable ‚ÄúLow‚Äù-risk conditions and lowest at the extremes, with middle categories showing uncertainty.\n\n\n\n\n\n\n\n\nFigure¬†6: FAH (day t) ‚Üí OAH (day t+1) proportions.\n\n\n\n\n\n\nNote: Each FAH column sums to 100%; next-day alignment performed within Area (if available).\n\nClass imbalances will need to be addressed during model training to ensure the network does not simply default to predicting the most common outcomes."
  },
  {
    "objectID": "DS4i_writeup.html#data-cleaning-and-feature-engineering",
    "href": "DS4i_writeup.html#data-cleaning-and-feature-engineering",
    "title": "Exploring Neural Networks for Snow Avalanche Forecasting in Scotland: A Multi-Factorial Investigation",
    "section": "Data Cleaning and Feature Engineering",
    "text": "Data Cleaning and Feature Engineering\nFeature engineering focused on creating temporal variables to capture trends not natively handled by a feedforward neural network (FFNN). Given that EDA highlighted that OAH varies according to the season and time of year, several variables were derived.\nBefore removing observations with extensive missing values, we extracted the time of day, day of the week, month, and year from each date. Next, a categorical variable for each avalanche season (December-April, 2009-2025) was created to account for the observed year-on-year increase in risk.\nTable¬†3 presents the percentage of missing values for features with incomplete data. Av.Cat was removed because its meaning was unclear and it contained implausible values (e.g., NA, 3, -2, 2, 1, 4, 8800, -1, 0, 1021, 4400, -9999, 99, 88, 44, 5031, 121). Figure¬†7 shows no clear relationship between ski.penetration and OAH, except for ‚ÄúHigh‚Äù risk avalanches; we therefore excluded it since snowfall effects are likely better represented by other variables. Summit.wind.direction (12.35% missing) was also dropped for lack of a discernible trend.\n\n\n\n\nTable¬†3: Missing value exploration.\n\n\n\n\n\n\nVariable\nMissing (%)\n\n\n\n\nAV.Cat\n23.3718\n\n\nSki.Pen\n22.5283\n\n\nSummit.Wind.Dir\n12.3512\n\n\nCrystals\n9.2587\n\n\nSummit.Wind.Speed\n8.5184\n\n\nSummit.Air.Temp\n7.0659\n\n\nSnow.Index\n6.9815\n\n\nMax.Temp.Grad\n6.6535\n\n\nMax.Hardness.Grad\n5.8945\n\n\nWetness\n5.3978\n\n\nInsolation\n4.7512\n\n\nOAH\n4.2452\n\n\nSnow.Temp\n3.8422\n\n\nAspect\n3.3268\n\n\nNo.Settle\n2.7083\n\n\nTotal.Snow.Depth\n1.5275\n\n\nWind.Dir\n1.4806\n\n\nWind.Speed\n0.4967\n\n\nFoot.Pen\n0.3936\n\n\nIncline\n0.3374\n\n\nAir.Temp\n0.3186\n\n\nCloud\n0.2718\n\n\nAlt\n0.0562\n\n\n\nNote: \n\n\n\n All values rounded to 4 decimal places.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure¬†7: Boxplots of ski penetration and summit wind direction by avalanche hazard category.\n\n\n\n\n\n\nNote: Outliers have been excluded. Y-axis scale from 0 to 20 on the left, and 0 to 500 on the right.\n\n\nLastly, we dropped high-cardinality categorical variables, including osgrid and location. Osgrid appeared to be an ID variable with &gt;3000 unique values, which would make one-hot encoding overly complex and was unlikely to indicate risk. Location was also removed, since latitude, longitude, and area already captured spatial information more effectively.\nAfter removing variables, 36% of samples remained incomplete and were subsequently dropped. While retaining more observations would be ideal, the remaining ~7000 were sufficient for representative results."
  },
  {
    "objectID": "DS4i_writeup.html#modelling-methodology",
    "href": "DS4i_writeup.html#modelling-methodology",
    "title": "Exploring Neural Networks for Snow Avalanche Forecasting in Scotland: A Multi-Factorial Investigation",
    "section": "Modelling Methodology",
    "text": "Modelling Methodology\nThree main models were used in this project to exploit different structural properties of the data: a convolutional neural network (CNN), a recurrent neural network (RNN), and a feedforward multilayer perceptron neural network (FFNN-MLP). The FFNN-MLP was expected to handle categorical predictors (e.g., forecasted avalanche risk) effectively, while the CNNs and RNNs were targeted at uncovering temporal or sequential patterns.\nCNNs process data by applying convolutional filters (small weight matrices) that slide across the input space. Each filter computes dot products with local patches of the data, producing feature maps that emphasise spatial or temporal patterns based on the type of filters used. Non-linear activations are then applied, followed by pooling layers to down-sample the maps and reduce dimensionality while preserving key information. Multiple convolutional layers can be stacked, though two layers were found to balance complexity and generalisation. The below interactive animation aims to illustrate how the filtering process works.\n\n\n\nCredit: OpenAI (2025a and 2025b)\n\nRNNs are designed to capture sequential dependencies by incorporating feedback connections. At each time step, the hidden state depends not only on the current input but also on the state from the previous step. This architecture makes RNNs well suited for modelling temporal dynamics in sequential data.\nA FFNN-MLP consists of an input layer, hidden layers of perceptron nodes, and an output layer. Each hidden node computes a weighted sum of its inputs, applies a non-linear activation, and forwards the result to the next layer. The weights are initialised randomly and updated through backpropogation using an optimiser such as gradient descent. Activation functions (e.g., ReLu, sigmoid) introduce non-linearity enabling the model to capture complex relationships between predictors and the target.\n\nData Preprocessing of Neural Networks models\nTo prepare the data for CNN and RNN, we constructed a 3-dimensional array (number of samples √ó the time window length √ó the number of features). Each training sample was created by sliding a fixed window across the data. Within the array, columns represent time steps, and depth represent data features. Since we predict one day ahead, each sample represents the observations within the window, for the next day prediction. Data were standardised to avoid distortion from differing feature scales, and one-hot encoded so each categorical variable is split into multiple binary variables demarcating each category.\nTo preserve temporal trends for the CNN and RNN, the train and test split was not randomly shuffled. Instead, the first 80% of records were used for training, while the remainder was used for testing, similar to the approach of Fromm and Schonberger (2022).\n\n\nConvolutional Neural Network\nCNN and RNN hyperparameter selection involved two stages. Firstly, several candidate datasets were created with time windows of 3-days, 7-days, 30-days and 120-days. A narrow search was used, training many models to identify the most appropriate window based on validation accuracy, estimated using cross-validation with a 20% holdout set. All models were trained for 60 epochs, except the FFNN-MLP.\nIn the second stage, a broader grid-search was conducted on the chosen window, selecting the model with the highest validation accuracy. In all our tests presented for the CNN, we assessed architectures with two convolutional layers using the ReLU function in those latent layers, and the softmax function in our input layer to create the class predictions. More complex architectures did not perform meaningfully better. Measures to control for class imbalance were taken for both the CNN and RNN by calculating weights, inversely proportional to the frequency of observations of that class in the training set.\n\n\n\n\n\n\n\n\nFigure¬†8: CNN prediction window search.\n\n\n\n\n\nConsidering Figure¬†8 above, we chose a 7-day prediction window. Overall, most models performed best with 3-7 days, while 30- and 120-day windows were meaningfully worse. Validation accuracy was competitive between 3 and-7-days. However, we chose a 7-day window as we felt this was more likely to capture influential all season and spatial patterns for the next day prediction.\n\n\n\n\nTable¬†4: Expanded grid-search hyperparameters and values.\n\n\n\n\n\n\nhyperparameter\nvalues\n\n\n\n\nfilters (layer 1)\n16, 32, 64\n\n\nfilters (layer 2)\n16, 32, 64\n\n\nkernel size\n5, 15\n\n\ndropout (layer 1)\n0.5, 0.7\n\n\ndropout (layer 2)\n0.5, 0.7\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure¬†9: Top 30 configurations by validation accuracy for CNN.\n\n\n\n\n\nAfter completing the initial window search, an expanded grid-search using the hyperparameters presented in Table¬†4 was conducted. The validation accuracy for the top 30 models is presented in Figure¬†9 .The best performing model during the initial prediction window search (the first stage of the search) achieved a validation accuracy of 67, while the top model using the chosen window in the second stage of the search reached 69.1%. We stuck with this model, as validation accuracy had not improved meaningfully over the course of the expanded search. The final selected model involved 32 filters in the first layer, followed by a dropout of 70%, then 16 layers in the second layers, followed by dropout of 70% and global average pooling into the final output layer, and 5 Kernels (Filters).\n\n\nRecurrent Neural Network\nThe approach for the RNN was the same as the CNN discussed above. Firstly, a search over numerous candidate window sizes were completed, and a subsequent expanded search was done as well. In Figure¬†10 below, the validation accuracy of the models test is presented. We used a combination of two Long Short-Term memory layers (LSTM), with dropout and varied the LSTM units and dropout values in the grid-search.\n\n\n\n\n\n\n\n\nFigure¬†10: RNN prediction window search.\n\n\n\n\n\nThe RNN window search showed more competitive performance. In this case, a window of 30-days appears more robust overall, containing fewer worse performing models towards the extreme end. We therefore conducted an expanded grid-search using this window, the results of which are presented in Figure¬†11 below. The grid-search hyperparameters are listed in Table¬†5.\n\n\n\n\nTable¬†5: Expanded grid-search hyperparameters and values.\n\n\n\n\n\n\nHyperparameter\nValues\n\n\n\n\nunits (layer 1)\n16, 32, 64\n\n\nunits (layer 2)\n16, 32, 64\n\n\ndropout (layer 1)\n0.3, 0.5, 0.7\n\n\ndrop (layer 2)\n0.3, 0.5, 0.7\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure¬†11: Top 30 configurations by validation accuracy for RNN.\n\n\n\n\n\nFrom the expanded grid-search, the best performing model achieved a validation accuracy of 64.8%. We fitted this model to the full training data and used that to predict onto the test set. Overall, we were satisfied with this search as the validation accuracy had not improved drastically compared to the initial window search; for example, the best model in the first stage reached roughly 60%. The final RNN model therefore included 64 LSTM units in the first layer, 16 units in the second layer with 70% dropout in the first and 50% dropout in the second.\n\n\nFeedforward Multilayer Perceptron Neural Network\nThe FFNN-MLP architecture was optimised through a randomised grid-search to identify the most effective combination of hyperparameters. Candidate models has consistent input and output layers, while hidden layers varied. The input layer comprised 61 nodes, corresponding to the full set of hot-one encoded engineered predictor variables, plus bias.\nThe grid-search explored 2-4 hidden layers with neuron configurations such as [256, 128, 64], [512, 256, 128], and [128, 64]. The RectifierWithDropout activation promoted non-linearity and mitigated overfitting. The output layer had five neurons, with softmax to generate class probabilities.\nTo address class imbalance, each training observation was weighted inversely to its class frequency, increasing the influence of minority classes during optimisation.\nThe randomised grid-search evaluated a maximum of 216 candidate models, exploring a predefined hyperparameter space. Key hyperparameters and their search ranges are detailed in Table¬†6 .\n\n\n\n\nTable¬†6: FFNN-MLP Hyperparameter Grid.\n\n\n\n\n\n\nHyperparameter\nValues\n\n\n\n\nhidden\n(128,128); (128,64); (64,32,16); (64,32)\n\n\nactivation\nRectifierWithDropout\n\n\nl1\n0, 1e-4, 1e-5\n\n\nl2\n0, 1e-4, 1e-5\n\n\nrate\n0.01, 0.001\n\n\ninput dropout ratio\n0.1, 0.3, 0.5\n\n\n\n\n\n\n\n\n\n\nModels were trained for up to 200 epochs with early stopping, terminating if validation logloss did not improve by at least 1e-4 over 10 consecutive intervals."
  },
  {
    "objectID": "DS4i_writeup.html#results-and-discussion",
    "href": "DS4i_writeup.html#results-and-discussion",
    "title": "Exploring Neural Networks for Snow Avalanche Forecasting in Scotland: A Multi-Factorial Investigation",
    "section": "Results and Discussion",
    "text": "Results and Discussion\n\nConfusion Matrices\nFigure¬†12 displays the confusion matrices for the three models. FFNN-MLP has a clearer diagonal, indicating stronger performance. The confusion matrix for RNN is very diffused with widespread misclassifications. Table¬†7 confirms this: RNN had the lowest accuracy (59%) and FFNN-MLP had the highest (75%). FFNN-MLP accuracy just narrowly beats the accuracy of the forecasted values (FAH) (74%). Due to the training and testing split used for the CNN and RNN (where data was neither shuffled nor randomly assigned), ‚ÄúHigh‚Äù risks, being rare, did not end up appearing in the test set. Consequently, the models‚Äô ability to identify ‚ÄúHigh‚Äù risk cases could not be evaluated.\nThe diagonal of the RNN is the weakest of them all. Correct predictions are concentrated in ‚ÄúLow‚Äù (752), with smaller numbers for ‚ÄúModerate‚Äù (165) and ‚ÄúConsiderable+‚Äù (12). Most misclassifications involve higher-risk cases being predicted as ‚ÄúLow‚Äù or ‚ÄúModerate‚Äù, consistently underestimating hazard.\nSimilarly, almost all predictions are ‚ÄúLow‚Äù for CNN. There is a noticeable concentration on the diagonal for ‚ÄúLow‚Äù classifications (735), and a considerable concentration of ‚ÄúModerate‚Äù classifications (270). On the off-diagonal, misclassifications often occur between ‚ÄúLow‚Äù and ‚ÄúModerate‚Äù (205 ‚ÄúLow‚Äù predicted as ‚ÄúModerate‚Äù; 148 ‚ÄúModerate‚Äù predicted as ‚ÄúLow‚Äù), while higher risk categories are frequently confused as these two classes. This shows that the model fell for imbalance bias, favouring the majority class ‚ÄúLow‚Äù and second most abundance ‚ÄúModerate‚Äù.\nThe FFNN-MLP had some noticeable misclassifications between ‚ÄúConsiderable-‚Äù and ‚ÄúModerate‚Äù: 103 ‚ÄúModerate‚Äù cases were predicted as ‚ÄúConsiderable-‚Äù, and 41 ‚ÄúConsiderable-‚Äù instances were predicted as ‚ÄúModerate‚Äù. Similarly, 112 ‚ÄúLow‚Äù cases were predicted as ‚ÄúModerate, indicating confusion between these classes. Similar to what was found for CNN and RNN, high hazard levels are often underpredicted or misclassified, while the FFNN-MLP model is most accuracy for‚ÄùLow‚Äù (536) and ‚ÄúModerate‚Äù (369) cases.\n\n\n\n\n\n\n\n\nFigure¬†12: Confusion Matrices Comparing the Final CNN, RNN, and Feed-Forward Models\n\n\n\n\n\n\n\nOverall Model Performance\nIn Table¬†7 below, an overview of each model‚Äôs performance is presented. The FFNN-MLP performs best, achieving the highest accuracy, recall, specificity, and precision scores. The CNN serves as a middle ground, with acceptable accuracy but low recall. The RNN model performs the worst overall. Its recall and precision are fairly low which cast its practical reliability into doubt.\n\n\n\n\nTable¬†7: Overall Model performance Statistics\n\n\n\n\n\n\n\nAccuracy\nKappa\nRecall\nSpecificity\nPrecision\n\n\n\n\nFFNN_NLP\n0.7545\n0.645\n0.6182\n0.9341\n0.6335\n\n\nCNN\n0.6496\n0.353\n0.4066\n0.8802\n0.4391\n\n\nRNN\n0.5944\n0.222\n0.3681\n0.8494\n0.3113\n\n\n\nNote: \n\n\n\n\n\n\n\n All values rounded to 4 decimal places.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFFNN-MLP‚Äôs specificity was 0.9341, the highest observed, showing it is very good at ruling out cases that do not belong to a class. Its recall was 0.6182 (identified ~62% of true cases), with a higher ability to catch more actual danger levels compared to CNN and RNN. CNN and RNN specificity scores were 0.8802 and 0.8494 respectively, indicating a fair ability to avoid false alarms, but not up to par with FFNN-MLP. Notably, RNN‚Äôs precision was 0.3113, meaning that the RNN model is wrong more often than it is not. Contrast that with CNN‚Äôs 0.4391 precision (correct a little under half the time) and FFNN-MLP‚Äôs 0.6335 (correct around 63% of the time).\n\n\nModel Statistics by Class\nLooking at performance statistics by class can help us gain useful insight into each model‚Äôs strengths and weaknesses, providing a more detailed overview of model behaviour. Table¬†8, Table¬†9, and Table¬†10 summarise our model statistics by class.\n\n\n\n\nTable¬†8: CNN Statistics by Class\n\n\n\n\n\n\n\nSensitivity\nSpecificity\nPos Pred Value\nNeg Pred Value\nPrecision\nRecall\nF1\nPrevalence\nDetection Rate\nDetection Prevalence\nBalanced Accuracy\n\n\n\n\nClass: Low\n0.7737\n0.7180\n0.8086\n0.6733\n0.8086\n0.7737\n0.7907\n0.6063\n0.4690\n0.5801\n0.7458\n\n\nClass: Moderate\n0.5806\n0.7432\n0.4882\n0.8077\n0.4882\n0.5806\n0.5305\n0.2967\n0.1723\n0.3529\n0.6619\n\n\nClass: Considerable -\n0.0351\n0.9952\n0.3636\n0.9293\n0.3636\n0.0351\n0.0640\n0.0728\n0.0026\n0.0070\n0.5151\n\n\nClass: Considerable +\n0.2368\n0.9444\n0.0957\n0.9803\n0.0957\n0.2368\n0.1364\n0.0243\n0.0057\n0.0600\n0.5906\n\n\nClass: High\nNA\n1.0000\nNA\nNA\nNA\nNA\nNA\n0.0000\n0.0000\n0.0000\nNA\n\n\n\nNote: \n\n\n\n\n\n\n\n\n\n\n\n\n\n All values rounded to 4 decimal places.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTable¬†9: RNN Statistics by Class\n\n\n\n\n\n\n\nSensitivity\nSpecificity\nPos Pred Value\nNeg Pred Value\nPrecision\nRecall\nF1\nPrevalence\nDetection Rate\nDetection Prevalence\nBalanced Accuracy\n\n\n\n\nClass: Low\n0.7924\n0.5244\n0.7203\n0.6204\n0.7203\n0.7924\n0.7546\n0.6072\n0.4811\n0.6679\n0.6584\n\n\nClass: Moderate\n0.3556\n0.8098\n0.4412\n0.7485\n0.4412\n0.3556\n0.3938\n0.2969\n0.1056\n0.2393\n0.5827\n\n\nClass: Considerable -\n0.0000\n0.9986\n0.0000\n0.9276\n0.0000\n0.0000\nNaN\n0.0723\n0.0000\n0.0013\n0.4993\n\n\nClass: Considerable +\n0.3243\n0.9142\n0.0839\n0.9824\n0.0839\n0.3243\n0.1333\n0.0237\n0.0077\n0.0915\n0.6192\n\n\nClass: High\nNA\n1.0000\nNA\nNA\nNA\nNA\nNA\n0.0000\n0.0000\n0.0000\nNA\n\n\n\nNote: \n\n\n\n\n\n\n\n\n\n\n\n\n\n All values rounded to 4 decimal places.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTable¬†10: FFNN-MLP Statistics by Class\n\n\n\n\n\n\n\nSensitivity\nSpecificity\nPos Pred Value\nNeg Pred Value\nPrecision\nRecall\nF1\nPrevalence\nDetection Rate\nDetection Prevalence\nBalanced Accuracy\n\n\n\n\nClass: Low\n0.8208\n0.9792\n0.9658\n0.8845\n0.9658\n0.8208\n0.8874\n0.4165\n0.3418\n0.3540\n0.9000\n\n\nClass: Moderate\n0.7515\n0.8524\n0.6989\n0.8827\n0.6989\n0.7515\n0.7242\n0.3131\n0.2353\n0.3367\n0.8019\n\n\nClass: Considerable -\n0.7812\n0.8614\n0.5910\n0.9389\n0.5910\n0.7812\n0.6729\n0.2041\n0.1594\n0.2698\n0.8213\n\n\nClass: Considerable +\n0.1772\n0.9866\n0.4118\n0.9576\n0.4118\n0.1772\n0.2478\n0.0504\n0.0089\n0.0217\n0.5819\n\n\nClass: High\n0.5600\n0.9909\n0.5000\n0.9929\n0.5000\n0.5600\n0.5283\n0.0159\n0.0089\n0.0179\n0.7755\n\n\n\nNote: \n\n\n\n\n\n\n\n\n\n\n\n\n\n All values rounded to 4 decimal places.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCNN (Table¬†8) performance is best for the ‚ÄúLow‚Äù class (sensitivity = 0.7737, precision = 0.8086, F1 = 0.7907, balanced accuracy (BA) = 0.7458), dropping at higher-risk categories as prevalence decreases. For example, it performed poorly for ‚ÄúConsiderable-‚Äù and ‚ÄúConsiderable+‚Äù (sensitivity = 0.0351, 0.2368; precision = 0.3636, 0.0957; F1 = 0.0640, 0.1364; BA = 0.5151, 0.5906). RNN (Table¬†9) shows similar trends, with a slightly higher sensitivity for ‚ÄúLow‚Äù (0.7737) but lower F1 (0.7546) and BA (0.6584), and poorer performance for ‚ÄúModerate‚Äù (F1 = 0.3938; BA = 0.5827) and higher-risk classes (‚ÄúConsiderable-‚Äù: F1 = NA, BA = 0.4993; ‚ÄúConsiderable+‚Äù: F1 = 0.1333, BA = 0.6192). For both models, ‚ÄúHigh‚Äù could not be assessed due to zero prevalence.\nHowever, FFNN-MLP (Table¬†10) maintained strong performance for the common and moderate classes: ‚ÄúLow‚Äù (F1 = 0.8874, BA = 0.9), ‚ÄúModerate‚Äù (F1 = 0.7242, BA = 0.8019), and ‚ÄúConsiderable-‚Äù (F1 = 0.6729, BA = 0.8213), catching most of the true cases and rarely mislabelling other classes as these. Performance dropped for ‚ÄúConsiderable+‚Äù (F1 = 0.2478, BA = 0.5819), but unlike CNN and RNN, FFNN-MLP was able to better detect ‚ÄúHigh‚Äù (F1 = 0.5283, BA = 0.7755), showing improved identification of rarer, higher-risk categories.\nThis suggests model bias towards better-represented classes.\n\n\nFeature Importance\nIn Figure¬†13, the feature importance plot from the FFNN-MLP is shown, for the first 30 most important variables.\nThe avalanche forecast variables were highly influential. FAH dominates, with FAH.Moderate, FAH.Low, FAH.Considerable-, and FAH.Considerable+ as the top predictors. Altitude (Alt) ranks just below FAH, while snow.index and the seasonal variables we constructed (season12, season5, season3) have moderate influence. This suggests seasonal trends do matter, but no single season dominates the prediction. If avalanches were restricted to one specific time/season window, we would have seen that season stand out much more.\nGeographical area (Area.Torridon, Area.Glencoe) contributes minimally, indicating spatial differences are less important than FAH. Precipitation ranks lowest, adding little predictive power beyond snow index, which is clearly more influential.\nOverall, the FAH drives predictions, followed by altitude and snow.index, with seasonal, spatial, and precipitation factors also adding influence on the model.\n\n\n\n\n\n\n\n\nFigure¬†13: Variable importance for FFNN-MLP.\n\n\n\n\n\n\n\nImprovements & Future Contributions\nWhile the present study used only complete cases for modelling, avalanche forecasters in practice must often make predictions despite missing data. Imputation was considered, but fell outside the scope of evaluating NN performance. Developing an imputation model for this dataset could be a way of meaningfully improving its practical use, robustness, and accuracy overall. Secondly, incorporating climate change effects by accounting for evolving weather and snowpack conditions alongside historical data could be a helpful tactic for researchers. Our models show that seasonal patterns are influential in predicting the risk. One reason this may be, is due to the evolving impact of climate change on the snowpack and ice condition. Furthermore, one may consider exploring region-specific models to reflect Scotland‚Äôs highly localized weather patterns and improve forecast reliability. Lastly, we restricted our hypothesis to neural network models only, but these may not be best suited to this data. Exploring alternative model types, such as gradient boosting or hybrid deep learning ensembles, may handle categorical data better and allow for more accurate predictions."
  },
  {
    "objectID": "DS4i_writeup.html#conclusion",
    "href": "DS4i_writeup.html#conclusion",
    "title": "Exploring Neural Networks for Snow Avalanche Forecasting in Scotland: A Multi-Factorial Investigation",
    "section": "Conclusion",
    "text": "Conclusion\nOur objective requires reliable detection across all avalanche danger levels, not just the majority ones. In this study, the FFNN-MLP came closest to meeting this requirement, but even it struggled with the minority, high-risk cases. Future work should focus on reducing class imbalance, exploring region-specific models, and experimenting with ensemble approaches to ensure that high-risk avalanche conditions are consistently and accurately detected."
  },
  {
    "objectID": "DS4i_writeup.html#references",
    "href": "DS4i_writeup.html#references",
    "title": "Exploring Neural Networks for Snow Avalanche Forecasting in Scotland: A Multi-Factorial Investigation",
    "section": "References",
    "text": "References\n\nBain, D. 2024. Avalanche awareness in Scotland. Sports Scotland Glenmore Lodge. Available: https://www.glenmorelodge.org.uk/avalanche-awareness-in-scotland/ [2025, September 11].\nBeattie, B. 1976. The densification of a seasonal snowpack in the Cairngorms with relation to avalanches. B.Sc. (professional) dissertation. University of Lancaster.\nBlagovechshenskiy, V., Medeu, A., Gulyayeva, T., Zhdanov, V., Ranova, S., Kamalbekova, A. & Aldabergen, U. 2023. Application of artificial intelligence in the assessment and forecast of avalanche danger in the Ile Alatau Ridge. Water. 15(7). DOI: 10.3390/w15071438\nBritannica. 2026. Ben Nevis. Available: https://www.britannica.com/place/Ben-Nevis [2025, September 10].\nChoubin, B., Borji, M., Mosavi, A., Sajedi-Hosseini, F., Singh, V.P. & Shamshirband, S. 2019. Snow avalanche hazard prediction using machine learning methods. Journal of Hydrology. 577. DOI: 10.1016/j.jhydrol.2019.123929\nCool, N. 2016. Caution avalanche danger signage during winter. Unsplash. Available: https://unsplash.com/photos/caution-avalanche-danger-signage-during-winter-Cd2QnIKU6dk [2025, September 24].\nDiggins, M. 2009. The challenges for Scottish avalanche forecasters observing a maritime snowpack. Proceedings of the International Snow Science Workshop. 22-24. Available: https://arc.lib.montana.edu/snow-science/objects/issw-2009-0024-0026.pdf [2025, September 10].\nFromm, R. & Schonberger, C. 2022. Estimating the danger of snow avalanches with machine learning approach using a comprehensive snow cover model. Machine Learning with Applications. 10. DOI: 10.1016/j.mlwa.2022.100405\nGauthier, F., Laliberte, J. & Meloche, F. 2025. Assessing the predictive capability of several machine learning algorithms to forecast snow avalanches using numerical weather prediction model in eastern Canada. EGUsphere. DOI: 10.5194/egusphere-2025-1572\nHeierli, J., Purves, R.S., Felber, A. & Kowalski, J. 2004. Verification of nearest-neighbours interpretations in avalanche forecasting. Annals of Glaciology. 38:84-88. DOI: 10.3189/172756404781815095\nHelbig, N., van Herwijnen, A. & Jonas, T. 2015. Forecasting wet-snow avalanche probability in mountainous terrain. Cold Regions Science and Technology. 120:219-226. DOI: 10.1016/j.coldregions.2015.07.001\nHendrick, M., Techel, F., Volpi, M., Olvski, T., Perez-Guillen, van Herwijnen, A. & Schweizer, J. 2023. Automated prediction of wet-snow avalanche activity in the Swiss Alps. Journal of Glaciology. 69(277):1365-1378. DOI: 10.1017/jog.2023.24\nKala, M., Jain, S., Singh, A. & Krishnan, N.C. 2025. Addressing class imbalance in avalanche forecasting. Cold Regions Science and Technology. 231. DOI: 10.1016/j.coldregions.2024.104411\nLangmuir, E. 1970. Snow profiles in Scotland. Weather. 25(5):205-209. DOI: 10.1002/j.1477-8696.1970.tb03262.x\nMetOffice. 2010. UK regional climates. Available: https://www.metoffice.gov.uk/research/climate/maps-and-data/regional-climates/index [2025, September 11].\nOpenAI. (2025a). ChatGPT (Sept 23 version) [Large language model]. https://chatgpt.com/share/68d59a8c-efa4-8004-a295-616e83f37afb\nOpenAI. (2025b). ChatGPT (Sept 23 version) [Large language model]. https://chatgpt.com/share/68d59ad3-d50c-8004-b862-18966277b186\nPozdnoukhov, A., Purves, R.S. & Kanevski, M. 2008. Applying machine learning methods to avalanche forecasting. Annals of Glaciology. 49:107-113. DOI: 10.3189/172756408787814870\nPozdnoukhov, A., Matasci, G., Kanevski, M. & Purves, R.S. 2011. Spatio-temporal avalanche forecasting with Support Vector Machines. Nat. Hazards Earth Syst. Sci. 11:367-382. DOI: 10.5194/nhess-11-367-2011\nPurves, R.S., Morrison, K.W., Moss, G. & Wright, D.S.B. Nearest neighbours for avalanche forecasting in Scotland - Development, verification and optimisation of a model. Cold Regions Science and Technology. 37(3):343-355. DOI: 10.1016/S0165-232X(03)00075-2\nRahmati, O., Ghorbanzadeh, O., Teimurian, T., Mohammadi, F., Tiefenbacher, J.P., Falah, F., Pirasteh, S., Ngo, P.T.T. & Bui, D.T. 2019. Spatial modeling of snow avalanche using machine learning models and geo-environmental factors: Comparison of effectiveness in two mountain regions. Remote Sensing. 11(24). DOI: 10.3390/rs11242995\nSchirmer, M., Lehning, M. & Schweizer, J. 2009. Statistical forecasting of regional avalanche danger using simulated snow-cover data. Journal of Glaciology. 55(193):761-768. DOI: 10.3189/002214309790152429\nScottish Avalanche Information Service. 2024. Reviews of the winter season. Available: https://www.sais.gov.uk/sais-annual-reports/ [2025, September 13].\nScottish Avalanche Information Service. 2025. Avalanche information for the Scottish mountains. Available: https://www.sais.gov.uk/ [2025, September 5].\nScottish Government. 2011. Scotland‚Äôs marine atlas: Information for the National Marine Plan. Available: https://www.gov.scot/publications/scotlands-marine-atlas-information-national-marine-plan/pages/7/ [2025, September, 10].\nSharma, V., Kumar, S. & Sushil, R. 2023. A neural network model for automated prediction of avalanche danger level. Nat. Hazards Earth Syst. Sci. 23:2523-2530. DOI: 10.5194/nhess-23-2523-2023\nSingh, A. & Ganju, A. 2008. Artificial neural networks for snow avalanche forecasting in Indian Himalaya. Proceedings of the 12th International Conference of International Association for Computer Methods and Advances in Geomechanics (IACMAG). 1-6 October 2008. Gao, India. 1664-1670. Available: https://www.researchgate.net/publication/263651736_Artificial_Neural_Networks_for_Snow_Avalanche_Forecasting_in_Indian_Himalaya [2025, September 13].\nSpink, P.C. 1970. Scottish snowbeds in summer 1969. Weather. 25(5): 201-204. DOI: 10.1002/j.1477-8696.1970.tb03261.x\nTiwari, A., Arun, G. & Vishwakarma, B.D. 2021. Parameter importance assessment improves efficacy of machine learning methods for predicting snow avalanche sites in Leh-Manali Highway, India. Science of The Total Environment. 794. DOI: 10.1016/j.scitotenv.2021.148738\nTu, J.V. 1996. Advantages and disadvantages of using artificial neural network versus logistic regression for predicting medical outcomes. Journal of Clinical Epidemiology. 49(11):1225-1231. DOI: 10.1016/S0895-4356(96)00002-9\nWard, R.G.W. 1980. Avalanche hazard in the Cairngorm Mountains, Scotland. Journal of Glaciology. 26(94):31-41. DOI: 10.3189/S0022143000201020\nWard, R.G.W. 1984a. Avalanche prediction in Scotland: I. A survey of avalanche activity. Applied Geography. 4(2):91-108. DOI: 10.1016/0143-6228(84)90016-X\nWard, R.G.W. 1984b. Avalanche prediction in Scotland: II. Development of a predictive model. Applied Geography. 4(2):109-133. DOI: 10.1016/0143-6228(84)90017-1\nWard, R.G.W., Langmuir, E.D.G. & Beattie, B. 1985. Snow profiles and avalanche activity in the Cairngorm Mountains, Scotland. Journal of Glaciology. 31(107):18-27. DOI: 10.3189/S0022143000004949\nWebster, H. 2020. Avalanches in Scotland - interview with Mark Diggins. Walkhighlands. Available: https://www.walkhighlands.co.uk/news/avalanches-in-scotland-interview-with-mark-diggins/ [2025, September 10].\nWen, H., Wu, X., Liao, X., Wang, D., Huang, K. & Wunnemann, B. 2022. Application of machine learning methods for snow avalanche susceptibility mapping in the Parlung Tsangpo catchement, southeastern Qinghai-Tibet Plateau. Cold Regions Science and Technology. 198. DOI: 10.1016/j.coldregions.2022.103535\nWerritty, A. & Sugden, D. 2012. Climate change and Scotland: Recent trends and impacts. Earth and Environmental Science Transactions of The Royal Society of Edinburgh. 103(2):133-147. DOI: 10.1017/S1755691013000030\nWSL Institute for Snow and Avalanche Research SLF. 2016. Scotland - Avalanche warning experts exchange. Available: https://www.slf.ch/en/news/scotland-avalanche-warning-experts-exchange/?utm_source=chatgpt.com [2025, September 12]."
  },
  {
    "objectID": "llm_use.html",
    "href": "llm_use.html",
    "title": "LLM use",
    "section": "",
    "text": "Coming into this project, our team had little to no domain knowledge on avalanches, Scottish weather, or forecasting. With a large project scope and only three weeks to complete it, we used LLMs (ChatGPT, Gemini, and Llama) as a sort of personal tutor to help us get up to speed quickly. What and how each of us used LLMs for depended largely on our specific roles, including:\nAnother useful application was that we asked ChatGPT to produce an interactive animation to demonstrate how convolutional neural networks work. It produced this animation in html, although it took several iterations for the LLM to produce an outcome we were satisfied with. For example, one iteration of the animation had incorrect calculations for the dot product. Despite numerous attempts, the model could not rectify this issue in the code, perhaps due to the complexity and number of tokens required to process. To overcome this, we prompted ChatGPT to simply produce an animation that merely inputs the numbers into the graphic, rather than calculating the dot product directly. Then, we went into the file ourselves and changed the values to the correct workings (https://chatgpt.com/share/68d59a8c-efa4-8004-a295-616e83f37afb; https://chatgpt.com/share/68d59ad3-d50c-8004-b862-18966277b186).\nThe LLM significantly accelerated the workflow by automating routine coding tasks and providing concise drafts for report sections. Their main strengths were efficiency, breadth of knowledge across R packages, and the ability to reformat and summarise results in scientific language. Its most valuable use was adding detailed comments in different code chunks. However, there were important limitations. Some code suggestions required correction due to inefficiencies or use of incompatible functions, sometimes even hallucinating packages or referencing outdated functions which is a result of its old training data. Additionally, outputs occasionally overstated the reliability of results.\nFor version control issues, ChatGPT effectively guided us through Git errors. By copying the terminal output into the chat, we received:\nWithout this assistance, or using alternative resources such as YouTube videos, resolving merge conflicts would likely have taken considerably longer through trial and error. With ChatGPT, we were able to resume analysis promptly, making workflow more efficient.\nHowever, LLMs were less successful with system-specific tasks, such as resolving Keras installation errors. Two of our models relied on Keras, but some team members‚Äô computers lacked the necessary packages to integrate Python and R. This became an issue for the writing team, who needed to report on the modelling results but could not access the model predictions. When asked to assist, ChatGPT repeatedly suggested running library(keras), despite the error messages clearly stating that the module was missing. Attempts to follow installation instructions from this LLM were unsuccessful, including links provided for package installers that redirected to unrelated pages (https://chatgpt.com/share/68d796cf-43cc-8003-ba48-c98ff0b05b7d; https://chatgpt.com/share/68d79670-e654-8003-91b9-f09fb9ae00be).\nWhen it comes to Keras installation, ChatGPT seems to have a very specific way of going about this, and if that doesn‚Äôt work, it struggles to deviate and explore other solutions. In this case, it is worthwhile going to someone who has the technical expertise to fix the issue. They can identify issues on a computer that an LLM cannot infer from prompts or ask questions that uncover information that is completely overlooked on the LLM‚Äôs side.\nOverall, the above points demonstrate the importance of matching task complexity to the LLM‚Äôs capabilities. LLMs are valuable tools for scaffolding and accelerating tasks, but must be paired with critical evaluation, testing, and domain knowledge. In this project, final outputs were ultimately determined by our understanding of modelling, statistical validation, and careful checking of the LLM‚Äôs contributions. For accelerated understanding of domain knowledge, EDA plots, temporal trend suggestions, modelling assistance, and formatting, LLMs were exceptional at providing further information much more quickly than if we were to search for this ourselves. However, using it for the CNN interactive animation or resolving Keras installation issues, we had to work around LLM limitations, either simplifying tasks, or consulting other resources."
  },
  {
    "objectID": "llm_use.html#references",
    "href": "llm_use.html#references",
    "title": "LLM use",
    "section": "References",
    "text": "References\n\nOpenAI. (2025). ChatGPT (GPT-5). [Large language model].\nhttps://chatgpt.com/share/68d51b49-d840-8002-ac38-577a47687625\nOpenAI. (2025). ChatGPT (GPT-5). [Large language model].\nhttps://chatgpt.com/share/68d5946a-ef3c-8004-99a9-e2fb46c1d320\nOpenAI. (2025). ChatGPT (GPT-5). [Large language model].\nhttps://chatgpt.com/share/68d59a8c-efa4-8004-a295-616e83f37afb\nOpenAI. (2025). ChatGPT (GPT-5). [Large language model].\nhttps://chatgpt.com/share/68d59ad3-d50c-8004-b862-18966277b186\nOpenAI. (2025). ChatGPT (GPT-5). [Large language model].\nhttps://chatgpt.com/share/68d79583-1a28-8003-b629-12ce4bafd86f\nOpenAI. (2025). ChatGPT (GPT-5). [Large language model].\nhttps://chatgpt.com/share/68d796cf-43cc-8003-ba48-c98ff0b05b7d\nOpenAI. (2025). ChatGPT (GPT-5). [Large language model].\nhttps://chatgpt.com/share/68d79670-e654-8003-91b9-f09fb9ae00be\nOpenAI. (2025). ChatGPT (GPT-5). [Large language model].\nhttps://chatgpt.com/c/68d6aa64-0ae0-8322-81b2-8ba74f1cb58b\nOpenAI. (2025). ChatGPT (GPT-5). [Large language model].\nhttps://chatgpt.com/share/68d914d2-e42c-8001-b208-6946cf96a535\nOpenAI. (2025). ChatGPT (GPT-5). [Large language model].\nhttps://chatgpt.com/share/68d920ab-7f34-8012-8693-f1348cffb771"
  },
  {
    "objectID": "DS4i_writeup.html#summary-of-findings-and-conclusion",
    "href": "DS4i_writeup.html#summary-of-findings-and-conclusion",
    "title": "Exploring Neural Networks for Snow Avalanche Forecasting in Scotland: A Multi-Factorial Investigation",
    "section": "Summary of Findings and Conclusion",
    "text": "Summary of Findings and Conclusion\nIn this study, we compared the ability of CNN, RNN, and FFNN-MLP models in predicting avalanches hazard levels. This was a multi-factorial investigation, as many different variables contribute to avalanche outcomes. The nature of this problem is sequential, and we were able to preprocess that data to fit a time series context. We expected the CNN and RNN to outperform the FFNN-MLP since they have the added time series advantage.\nThe FFNN-MLP turned out to be the best-performing model overall. It showed the most balanced detection and achieved the best performance across hazard levels. The CNN struggled with the minority classes and mainly acted as a majority-class (‚ÄúLow‚Äù hazard) detector despite the controls put in place to re-weight the classes. This is unsatisfactory because in a real forecasting setting, it‚Äôs of great importance that we can detect the higher-risk cases. The RNN performed worst overall, missing nearly all the minority class instances (‚ÄúConsiderable+‚Äù, ‚ÄúConsiderable-‚Äù), and doing fairly well with the majority cases but still not as well as the other two models. This strongly disagrees with our initial hypothesis that sequential models would benefit from time series dependencies.\nA clear strength across all models is that they handled the majority class fairly well. On the other hand, all three models showed clear bias toward the most common categories. None of them did especially well with the higher-risk, minority classes. The highlights a weakness in applying these models directly without addressing the class imbalance and customising the class weights more in favour of the minority classes could help counteract this measure."
  }
]