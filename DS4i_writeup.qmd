---
title: "DS4I - Project Writeup"
format: html
execute:
  echo: false
  warning: false
  message: false
  results: hide
editor: visual
---

```{r setup}


#libraries 

library(kableExtra)
library(tidyverse)
library(ggplot2)
library(ggpubr)
library(lubridate)
library(caret)

#theme for ggplot2, let me know if you do not like this theme and we can change it :)

#Theme for ggplot2 
theme_set(
  theme_bw(base_size = 9) +
    theme(
      plot.title   = element_text(hjust = 0.5, size = 9),
      panel.border = element_rect(colour = "black", fill = NA, linewidth = 0.5),
      # keep grids but make them subtle
      panel.grid.major = element_line(colour = "grey85", linewidth = 0.3),
      panel.grid.minor = element_line(colour = "grey92", linewidth = 0.2),
      # base-R style text/ticks
      axis.text  = element_text(colour = "black"),
      axis.title = element_text(colour = "black"),
      axis.ticks = element_line(colour = "black", linewidth = 0.3)
    )
)



#making geompoint have hollow circles
update_geom_defaults(
  "point",
  list(size = 1, alpha = 0.9, shape = 21, colour = "black")
)


```

```{r}
#| echo: false
knitr::include_graphics("av.jpg")

```

[Photo by Nicolas Cool on Unsplash]{style="font-size:12px"}

## Introduction

## Literature Review

## Exploratory Data Analysis

```{r}



```

## Feature Engineering

```{r read_data}


data = read.csv("scotland_avalanche_forecasts_2009_2025.csv")

# row_unique_counts <- apply(
#   df[ , sapply(df, is.character)],  
#   1,                                
#   function(row) length(unique(row)) 
# )


data <- data %>% 
  mutate(across(where(is.character), as.factor))

c <- colnames(data)
r <- data.frame(var = character(),
                no = numeric())

for (i in c){
  
  if (is.factor(data[, i])){
  
  x <- length(unique(data[,i]))
  
  r <- rbind(r, data.frame(var = i,
                           no = x))
    
  }
  
  
}

6 + 6 + 7 + 7 



```

```{r missing_val}

#Getting table of missing values


col.n <- colnames(data)
n <- nrow(data)

missing_df <- data.frame(Variable = character(),
                         "Missing (%)" = numeric(),
                         check.names = F)


for (i in col.n){
  
  x <- sum(is.na(data[,i]))
  
  perc <- (x/n) * 100
  
  missing_df <- rbind(missing_df,
                      data.frame(
                        Variable = i,
                        `Missing (%)` = perc
                      , check.names = F)
                      )
  
}
 
missing_df <- missing_df %>% 
  filter(`Missing (%)` > 0) %>% 
  arrange(desc(`Missing (%)`),)


```

```{r tablex}



kable(missing_df, 
      format = "html",
      digits = 2,
      caption = "Missing Values") %>%
  kable_styling(
    bootstrap_options = c("hover"),
    full_width = FALSE
  ) %>%
  footnote(general = "")


```

```{r check_missing_fnx}

# a few functions to help me evaluate the extent of missing values

#Find percentage of missing values per Column
find_missing <- function(a){
  
  
col.n <- colnames(a)
n <- nrow(a)
missing_df <- data.frame(Variable = character(),
                         "Missing (%)" = numeric(),
                         check.names = F)


for (i in col.n){
  
  x <- sum(is.na(a[,i]))
  
  perc <- (x/n) * 100
  
  missing_df <- rbind(missing_df,
                      data.frame(
                        Variable = i,
                        `Missing (%)` = perc
                      , check.names = F)
                      )
  
}
 
missing_df <- missing_df %>% 
  filter(`Missing (%)` > 0) %>% 
  arrange(desc(`Missing (%)`),)

return(missing_df)

}

#Find total % missing values
find_total_missing <- function(b){
  
  
  count_na <- 0
for (i in 1:nrow(b)){
  if (any(is.na(data[i, ]))){count_na <- count_na +1}
}


count_na/nrow(b)
  
  return(count_na*100/nrow(b) )
}

```

```{r imputation}
#Come back to this later

# 
# #I am not grouping by Date, OsGrid, Obs or OAH (As thats our target variable)
# groupings <- groupings[-c(1, 3, 5, 7)]
# 
# #Cell mean imputation
# 
# #Imputing Training data first
# train.imp <- train %>%
#   group_by(Area, FAH) %>% #cant group by precip code, or location, too many missing
#   mutate(
#     across(where(is.integer),
#            ~ replace_na(., as.integer(round(mean(., na.rm = TRUE))))),
#     across(where(is.double),
#            ~ replace_na(., mean(., na.rm = TRUE)))
#   ) %>% #after this, some variables 
#   ungroup() %>% 
#   mutate(Aspect = round(mean(Aspect, na.rm = T),), #remaining, just taking the mean
#          Max.Temp.Grad = round(mean(Max.Temp.Grad, na.rm = T)),
#          Max.Hardness.Grad = round(mean(Max.Hardness.Grad, na.rm  = T))) 
# 
# train.imp <- train.imp[-which(is.na(train.imp$OAH)),] #removing remaining missing
#   
# 
# #Imputing Test data in the same manner
# test.imp <- test %>%
#   group_by(Area, FAH) %>% #cant group by precip code, or location, too many missing
#   mutate(
#     across(where(is.integer),
#            ~ replace_na(., as.integer(round(mean(., na.rm = TRUE))))),
#     across(where(is.double),
#            ~ replace_na(., mean(., na.rm = TRUE)))
#   ) %>% #after this, some variables 
#   ungroup() %>% 
#   mutate(Aspect = round(mean(Aspect, na.rm = T),), #remaining, just taking the mean
#          Max.Temp.Grad = round(mean(Max.Temp.Grad, na.rm = T)),
#          Max.Hardness.Grad = round(mean(Max.Hardness.Grad, na.rm  = T))) 
# 
# test.imp <- train.imp[-which(is.na(test.imp$OAH)),] #removing remaining missing


```

```{r date_time}

#adding date variable
data$Date <- as.POSIXct(data$Date, format="%Y-%m-%d %H:%M:%S", tz="UTC")


data$month     <- lubridate::month(data$Date)
data$hour      <- lubridate::hour(data$Date)
data$year      <- lubridate::year(data$Date)

#make a day of the season variable

#season begins from December to april



data <- data %>% 
  mutate(season_year = if_else(month >= 12, year, year -1),
         season_id = dense_rank(season_year),
         season = paste0("season", season_id),
         season = as.factor(season)) %>% 
  group_by(season, Area) %>% 
  mutate(dayofseason = row_number()) %>% 
  arrange(Date, Area)

  

36 - 4 + 26

```

```{r drop variables}

#Drop the top 3 variables wtih the most missing values
data <- data %>% 
  select(- c(AV.Cat, Ski.Pen, Summit.Wind.Dir, Obs, Location, OSgrid))

data <- na.omit(data) #remove remaining missing

#data <- data[,!(names(data)) == "Obs"]



```

```{r split_data}

set.seed(1)
train.idx <- sample(1:nrow(data), 
                    round(nrow(data)*0.8))

train <- data[train.idx,] #Training data
test <- data[-train.idx,] #Testing Data


```


```{r Keras_data_preprocessing_function}

library(keras3)
library(tensorflow)
library(reticulate)

# reticulate::install_miniconda()   # sets up a private Python just for R
# library(keras3)
# install_tensorflow()


#for this to work I need to convert the data into an array 
# form is (N (length), L (Time frame), F (Features))

#I promise that this code isnt made by Chatgpt I am just using a lot of comments otherwise I dont know what the #$!* is going on :)


#Function to let me test for numerous window sizes


pre_data <- function(w = 3, h = 1){

#--------Data Preparation-------------------------------------

T <- length(data$Date) #time period length

data_nn <- data %>%
  select(-c(Date)) #removing date column

L <- w #Rolling window length = 3. So what I am decided is that It can use the previus 3 days on record to train itself
H <- h # predict 1 day ahead


#number of samples we can have, so number of time slices, each one iterated one step ahead by H (which is just 1 day in our case)
N_samp <- T - L - H + 1



#----------------One hot encoding--------

#This part is quite a process. So big picture, the numeric data needs to be standardised, but when we do one-hot-encoding, it becomes quite messy to avoid standardising the dummy variables. SO we are going to make indices for the numeric columns so we can standardise only them later

#Taking our predictors and covariates into one df, and our labels into another

X.variables <- data_nn %>%
  select(-OAH)   #x_variables = everything in data other than OAH column

X.variables <- model.matrix(~., data = X.variables)[,-1] # One Hot encoding

y.variable <- data_nn[["OAH"]]  #y_variable = only our OAH column

numeric.variables <- names(data_nn)[sapply(data_nn, is.numeric)]
numeric.variables <- setdiff(numeric.variables, "OAH")

numeric.indexes <- which(colnames(X.variables) %in% numeric.variables) #okay so the colnames are the same so we can find the indexes in x variables which are numeric with this code <--

#dummy variable indexes
dummy.indexes <- setdiff(1:(ncol(X.variables)), numeric.indexes)


#Number features

F_ <- ncol(X.variables)

#--------------Making Array---------------------#


#Arrays - this is the confusing part. Keras needs an array for this to work. In our case we can think of this as turning our data into a stack of data sheets. each sheet is for a particular feature. THen there are N_sampl columns, and each is shifted by L

X <- array( NA_real_, #just an empty array, which will subsequently be filled
           dim = c(N_samp,L, F_  ),
           )

y <- integer(N_samp)




for (i in 1:N_samp){ #here we are filling in the array with our data

  index.window <- i:(i + L - 1) #index wind = ith value in N_samp to I + window minus 1

  X[i, , ] <- X.variables[index.window, , drop = F] #filling in the predictors with the corresponding indexes into that window
  y[i] <- y.variable[i + L + H -1] #we predict the ith + L + H y values.

}



#------------Training and testing split-----------------------------

#We want to preserve time series patterns, so no shuffling

#old version where everything was in order
idx <- 1:N_samp
number.training <- floor(N_samp * 0.8) #train = first 80% of features

tr <- idx[1:number.training] #Training indices
te <- idx[(number.training + 1):N_samp] #Testing indices

#making 1 = 0 to see if that helps

y <- y - 1 # Now categories are {0,1,2,3,4,5,}

#test for randomness being better (didnt work)
# idx <- 1:N_samp
# set.seed(1)
# tr <- sample(idx, floor(0.8*N_samp))
# te <- idx[-tr]



X.train <- X[tr, , , drop = F] #training predictors data array
X.test <- X[te, , , drop = F ] #testing predictors array
y.train <- y[tr] #training target
y.test <- y[te]


#Class weights
class_counts <- table(y.train)
total <- sum(class_counts)
n_classes <- length(class_counts)


class_weights <- total / (n_classes * class_counts)

# keras expects a named list: names = class index as character
class_weights <- as.list(class_weights)
names(class_weights) <- as.character(0:(n_classes-1))



#----------------Scaling--------------------

#Remeber that we made indexes for our numeric data, which is the data we want to scale.

#We also need to scale the training data, and scale the testing data according to the training data mu and sd


if (length(numeric.indexes) > 0){

  mu.numeric <- apply(X.train[, , numeric.indexes, drop = F], 3, mean, na.rm = T)

  sd.numeric <- apply(X.train[ , , numeric.indexes, drop = F], 3, sd, na.rm = T)

   sd.numeric[sd.numeric == 0 | is.na(sd.numeric)] <- 1 #make sure SD == 1

  for (j in seq_along(numeric.indexes)){

    k <- numeric.indexes[j]
    X.train[, , k] <- (X.train[ , , k] - mu.numeric[j])/sd.numeric[j]
    X.test[, , k] <- (X.test[ , , k] - mu.numeric[j])/sd.numeric[j]
  }


}

# Numeric cols should ne ~N(0,1)
round(apply(X.train[ , , numeric.indexes, drop = FALSE], 3, mean), 3)
round(apply(X.train[ , , numeric.indexes, drop = FALSE], 3, sd),   3)



#Getting there !

#Since we are doing a multiclass prediction we need to use softmax
#This means using the tocategorical() function on y

y.test <- to_categorical(y.test, num_classes = 6)


y.train <- to_categorical(y.train, num_classes = 6)

list(y.test, y.train, X.train, X.test, class_weights, F_)





}
```



```{r keras_grid_search}
#testing c-d-c

#vary window size hasnt made significant changes
#Adding weights to control for imbalance didnt help

keras_search <- function(window){

W_ <- pre_data(w = window, h = 1)

y.test <- W_[[1]]
y.train <- W_[[2]]
X.train <- W_[[3]]
X.test <- W_[[4]]
cl_weights <- W_[[5]]
F_ <- W_[[6]]
# ---------------- Grid search for your existing architecture ----------------

# Repro seeds (same as you set)
set.seed(1)
tf$keras$utils$set_random_seed(123L)

# Define the grid (adjust values if you want a bigger/smaller search)
grid <- expand.grid(
  filters1   = c( 32, 64),
  filters2   = c( 32, 64),
  kernel_sz  = c(2, 5, 15),
  drop1      = c(0.3, 0.5),
  drop2      = c(0.3, 0.5),
  stringsAsFactors = FALSE
)

results <- data.frame(
  filters1 = integer(), filters2 = integer(),
  kernel_sz = integer(), drop1 = double(), drop2 = double(),
  val_accuracy = double(), test_loss = double(), test_accuracy = double(),
  stringsAsFactors = FALSE
)

histories <- vector("list", nrow(grid))  # optional: keep training histories

for (i in seq_len(nrow(grid))) {
  cat(sprintf("\n[%d/%d] f1=%d, f2=%d, k=%d, d1=%.2f, d2=%.2f\n",
              i, nrow(grid),
              grid$filters1[i], grid$filters2[i],
              grid$kernel_sz[i], grid$drop1[i], grid$drop2[i]))

  # Clear TF/Keras state between runs to avoid graph/memory accumulation
  tensorflow::tf$keras$backend$clear_session()

  # Re-seed before each run to keep trials comparable
  set.seed(1)
  tf$keras$utils$set_random_seed(123L)

  # ----- Build your SAME model, with grid parameters plugged in -----
  input <- layer_input(shape = c(dim(X.train)[2], F_)) # (L, F)

  output <- input %>%
    layer_conv_1d(filters = grid$filters1[i],
                  kernel_size = grid$kernel_sz[i],
                  padding = "same", activation = "relu") %>%
    layer_dropout(rate = grid$drop1[i]) %>%
    layer_conv_1d(filters = grid$filters2[i],
                  kernel_size = grid$kernel_sz[i],
                  padding = "same", activation = "relu") %>%
    layer_global_average_pooling_1d() %>%
    layer_dropout(rate = grid$drop2[i]) %>%
    layer_dense(units = 6, activation = "softmax")

  model <- keras_model(inputs = input, outputs = output)

  model %>% compile(
    optimizer = "adam",
    loss = "categorical_crossentropy",
    metrics = "accuracy"
  )

  # ----- Train (identical settings you had) -----
  history <- model %>% fit(
    X.train, y.train,
    epochs = 60,
    batch_size = 64,
    verbose = 0,
    validation_split = 0.2,
    shuffle = FALSE
    # = cl_weights
  )

  # Collect validation accuracy from the last epoch
  val_acc <- tail(history$metrics$val_accuracy, 1)
  if (length(val_acc) == 0) {
    # Fallback name in case of different metric key
    val_acc <- tail(history$metrics$val_acc, 1)
  }

  # Evaluate on test set 
  test_metrics <- model %>% evaluate(X.test, y.test, verbose = 0)

  # test_metrics is usually a named numeric vector: c(loss=..., accuracy=...)
  test_loss <- as.numeric(test_metrics[[1]])
  test_acc  <- if (!is.null(names(test_metrics)) && "accuracy" %in% names(test_metrics)) {
    as.numeric(test_metrics[["accuracy"]])
  } else {
    as.numeric(test_metrics[[2]])
  }

  # Store results
  results[i, ] <- list(
    grid$filters1[i], grid$filters2[i],
    grid$kernel_sz[i], grid$drop1[i], grid$drop2[i],
    as.numeric(val_acc), test_loss, test_acc
  )
  histories[[i]] <- history
}

# Sort by best validation accuracy 
results <- results[order(-results$val_accuracy), ]

print(results)

# The best 
best <- results[1, ]

rm(W_) #For saving memory
gc()

return(results)

}



```

```{r keras_search}
#Window = 3 

#res_window_3 <- keras_search(window = 3)

#save(res_window_3, file =  "res_window_3")

#Window  = 7
#res_window_7 <- keras_search(window = 7)
#save(res_window_7, file =  "res_window_7")

#Window = 30
#res_window_30 <- keras_search(window = 30)
#save(res_window_30, file =  "res_window_30")
#window = 

#Window = 120
#res_window_120 <- keras_search(window = 120)
#save(res_window_120, file =  "res_window_120")

load("res_window_3")
load("res_window_7")
load("res_window_30")
load("res_window_120")

res_window_3 <- res_window_3 %>% 
  mutate(model_no = row_number(),
         window  = 3)

res_window_7 <- res_window_7 %>% 
  mutate(model_no = row_number(),
         window  = 7)

res_window_30 <- res_window_30 %>% 
  mutate(model_no = row_number(),
         window  = 30)

res_window_120 <- res_window_120%>% 
  mutate(model_no = row_number(),
         window  = 120)


results_cnn_search <- rbind(res_window_3, res_window_30, res_window_7,  res_window_120)
```


```{r cnn_window_vis}
#graph of validation accuracy
ggplot(data = results_cnn_search, aes( x  = model_no, y  = val_accuracy, colour = as.factor(window)))+
  geom_point()+
  labs(x = "Model Number (arranged by  validation accuracy)")



```


```{r}
#we will go with a window of roughly  7  days
rm(res_window_120,  res_window_3, res_window_30)


#I  am going to do one more expanded search, just with a window  of 7 days

# res_window_7[which.max(res_window_7$val_accuracy),]
# 
# #serach
# 
# 
# 
# W_ <- pre_data(w = 3, h = 1)
# 
# y.test <- W_[[1]]
# y.train <- W_[[2]]
# X.train <- W_[[3]]
# X.test <- W_[[4]]
# cl_weights <- W_[[5]]
# F_ <- W_[[6]]
# # ---------------- Grid search for your existing architecture ----------------
# 
# # Repro seeds 
# set.seed(1)
# tf$keras$utils$set_random_seed(123L)
# 
# # Define the grid (
# grid <- expand.grid(
#   filters1   = c( 16, 32, 64),
#   filters2   = c( 16, 32, 64),
#   kernel_sz  = c(5,15),
#   drop1      = c(0.5,0.7),
#   drop2      = c(0.5, 0.7),
#   stringsAsFactors = FALSE
# )
# 
# results <- data.frame(
#   filters1 = integer(), filters2 = integer(),
#   kernel_sz = integer(), drop1 = double(), drop2 = double(),
#   val_accuracy = double(), test_loss = double(), test_accuracy = double(),
#   stringsAsFactors = FALSE
# )
# 
# histories <- vector("list", nrow(grid))  # optional: keep training histories
# 
# for (i in seq_len(nrow(grid))) {
#   cat(sprintf("\n[%d/%d] f1=%d, f2=%d, k=%d, d1=%.2f, d2=%.2f\n",
#               i, nrow(grid),
#               grid$filters1[i], grid$filters2[i],
#               grid$kernel_sz[i], grid$drop1[i], grid$drop2[i]))
# 
#   # Clear TF/Keras state between runs to avoid graph/memory accumulation
#   tensorflow::tf$keras$backend$clear_session()
# 
#   # Re-seed before each run to keep trials comparable
#   set.seed(1)
#   tf$keras$utils$set_random_seed(123L)
# 
#   # ----- Build your SAME model, with grid parameters plugged in -----
#   input <- layer_input(shape = c(dim(X.train)[2], F_)) # (L, F)
# 
#   output <- input %>%
#     layer_conv_1d(filters = grid$filters1[i],
#                   kernel_size = grid$kernel_sz[i],
#                   padding = "same", activation = "relu") %>%
#     layer_dropout(rate = grid$drop1[i]) %>%
#     layer_conv_1d(filters = grid$filters2[i],
#                   kernel_size = grid$kernel_sz[i],
#                   padding = "same", activation = "relu") %>%
#     layer_global_average_pooling_1d() %>%
#     layer_dropout(rate = grid$drop2[i]) %>%
#     layer_dense(units = 6, activation = "softmax")
# 
#   model <- keras_model(inputs = input, outputs = output)
# 
#   model %>% compile(
#     optimizer = "adam",
#     loss = "categorical_crossentropy",
#     metrics = "accuracy"
#   )
# 
#   # ----- Train (identical settings you had) -----
#   history <- model %>% fit(
#     X.train, y.train,
#     epochs = 60,
#     batch_size = 64,
#     verbose = 0,
#     validation_split = 0.2,
#     shuffle = FALSE
#     # = cl_weights
#   )
# 
#   # Collect validation accuracy from the last epoch
#   val_acc <- tail(history$metrics$val_accuracy, 1)
#   if (length(val_acc) == 0) {
#     # Fallback name in case of different metric key
#     val_acc <- tail(history$metrics$val_acc, 1)
#   }
# 
#   # Evaluate on test set 
#   test_metrics <- model %>% evaluate(X.test, y.test, verbose = 0)
# 
#   # test_metrics is usually a named numeric vector: c(loss=..., accuracy=...)
#   test_loss <- as.numeric(test_metrics[[1]])
#   test_acc  <- if (!is.null(names(test_metrics)) && "accuracy" %in% names(test_metrics)) {
#     as.numeric(test_metrics[["accuracy"]])
#   } else {
#     as.numeric(test_metrics[[2]])
#   }
# 
#   # Store results
#   results[i, ] <- list(
#     grid$filters1[i], grid$filters2[i],
#     grid$kernel_sz[i], grid$drop1[i], grid$drop2[i],
#     as.numeric(val_acc), test_loss, test_acc
#   )
#   histories[[i]] <- history
# }
# 
# # Sort by best validation accuracy 
# results <- results[order(-results$val_accuracy), ]
# 
# print(results)
# 
# final_cnn_search  <-  results
save(final_cnn_search, file =  "final_cnn_serach")


load("final_cnn_serach")

# The best 
best <- final_cnn_search[1, ]

best

gc()
```


```{r}


res <- final_cnn_search %>%
  mutate(
    run_id = row_number(),
    label  = sprintf("f1=%d f2=%d k=%d d1=%.1f d2=%.1f",
                     filters1, filters2, kernel_sz, drop1, drop2)
  ) %>%
  arrange(desc(val_accuracy)) %>%
  mutate(rank = row_number(),
         top5 = rank <= 5)

library(scales)


top_n <- 30 
ggplot(res %>% slice(1:top_n),
       aes(x = reorder(label, val_accuracy), y = val_accuracy, fill = top5)) +
  geom_col() +
  coord_flip() +
  geom_text(aes(label = percent(val_accuracy, accuracy = 0.1)),
            hjust = -0.1, size = 3) +
  scale_y_continuous(labels = percent_format(accuracy = 1), expand = expansion(mult = c(0, .1))) +
  scale_fill_manual(values = c("grey80", "grey40"), guide = "none") +
  labs(title = sprintf("Top %d configs by validation accuracy", top_n),
       x = "Config", y = "Val accuracy")

```


```{r}
#best model  had 16 filters with 2 layers,  and kernel size 15,  0.7 dropout between layers

#Fit  final model on full  training data

  input <- layer_input(shape = c(dim(X.train)[2], F_)) # (L, F)

  output <- input %>%
    layer_conv_1d(filters = 16, kernel_size = 15, padding = "same", activation = "relu") %>%
    layer_dropout(rate = 0.7) %>%
    layer_conv_1d(filters = 16, kernel_size = 15, padding = "same", activation = "relu") %>%
    layer_global_average_pooling_1d() %>%
    layer_dropout(rate = 0.7) %>%
    layer_dense(units = 6, activation = "softmax")

  model <- keras_model(inputs = input, outputs = output)

  model %>% compile(
    optimizer = "adam",
    loss = "categorical_crossentropy",
    metrics = "accuracy"
  )

    history <- model %>% fit(
    X.train, y.train,
    epochs = 60,
    batch_size = 64,
    verbose = 0,
    validation_split = 0,
    shuffle = FALSE
    # = cl_weights
  )

final_cnn_model  <- model    
save(final_cnn_model,  file = "final_cnn_model")
load("final_cnn_model")

test_metrics <- model %>% evaluate(X.test, y.test, verbose = 0)

test_metrics$accuracy


```



```{r  RNN}
#sam as above but for RNN model
keras_search_rnn <- function(window){

W_ <- pre_data(w = window, h = 1)

y.test <- W_[[1]]
y.train <- W_[[2]]
X.train <- W_[[3]]
X.test <- W_[[4]]
cl_weights <- W_[[5]]
F_ <- W_[[6]]
# ---------------- Grid search for your existing architecture ----------------

# Repro seeds (same as you set)
set.seed(1)
tf$keras$utils$set_random_seed(123L)

# Define the grid (adjust values if you want a bigger/smaller search)
grid <- expand.grid(
  filters1   = c( 32, 64),
  filters2   = c( 32, 64),
  kernel_sz  = c(5, 15),
  drop1      = c(0.3, 0.5),
  drop2      = c(0.3, 0.5),
  stringsAsFactors = FALSE
)

results <- data.frame(
  filters1 = integer(), filters2 = integer(),
  kernel_sz = integer(), drop1 = double(), drop2 = double(),
  val_accuracy = double(), test_loss = double(), test_accuracy = double(),
  stringsAsFactors = FALSE
)

histories <- vector("list", nrow(grid))  # optional: keep training histories

for (i in seq_len(nrow(grid))) {
  cat(sprintf("\n[%d/%d] f1=%d, f2=%d, k=%d, d1=%.2f, d2=%.2f\n",
              i, nrow(grid),
              grid$filters1[i], grid$filters2[i],
              grid$kernel_sz[i], grid$drop1[i], grid$drop2[i]))

  # Clear TF/Keras state between runs to avoid graph/memory accumulation
  tensorflow::tf$keras$backend$clear_session()

  # Re-seed before each run to keep trials comparable
  set.seed(1)
  tf$keras$utils$set_random_seed(123L)

  # ----- Build model as an RNN (LSTM) using the same grid fields -----
  input <- layer_input(shape = c(dim(X.train)[2], F_)) # (L, F)

  output <- input %>%
    layer_lstm(units = grid$filters1[i], return_sequences = TRUE) %>%
    layer_dropout(rate = grid$drop1[i]) %>%
    layer_lstm(units = grid$filters2[i], return_sequences = FALSE) %>%
    layer_dropout(rate = grid$drop2[i]) %>%
    layer_dense(units = 6, activation = "softmax")

  model <- keras_model(inputs = input, outputs = output)

  model %>% compile(
    optimizer = "adam",
    loss = "categorical_crossentropy",
    metrics = "accuracy"
  )

  # ----- Train (identical settings you had) -----
  history <- model %>% fit(
    X.train, y.train,
    epochs = 60,
    batch_size = 64,
    verbose = 0,
    validation_split = 0.2,
    shuffle = FALSE
    # = cl_weights
  )

  # Collect validation accuracy from the last epoch
  val_acc <- tail(history$metrics$val_accuracy, 1)
  if (length(val_acc) == 0) {
    # Fallback name in case of different metric key
    val_acc <- tail(history$metrics$val_acc, 1)
  }

  # Evaluate on test set 
  test_metrics <- model %>% evaluate(X.test, y.test, verbose = 0)

  # test_metrics is usually a named numeric vector: c(loss=..., accuracy=...)
  test_loss <- as.numeric(test_metrics[[1]])
  test_acc  <- if (!is.null(names(test_metrics)) && "accuracy" %in% names(test_metrics)) {
    as.numeric(test_metrics[["accuracy"]])
  } else {
    as.numeric(test_metrics[[2]])
  }

  # Store results
  results[i, ] <- list(
    grid$filters1[i], grid$filters2[i],
    grid$kernel_sz[i], grid$drop1[i], grid$drop2[i],
    as.numeric(val_acc), test_loss, test_acc
  )
  histories[[i]] <- history
}

# Sort by best validation accuracy 
results <- results[order(-results$val_accuracy), ]

print(results)

# The best 
best <- results[1, ]

rm(W_) #For saving memory
gc()

return(results)

}



```

```{r}



#Window = 3 

rnn_window_3 <- keras_search(window = 3)

save(rnn_window_3, file =  "res_window_3")

#Window  = 7
rnn_window_7 <- keras_search(window = 7)
save(rnn_window_7, file =  "res_window_7")

#Window = 30
rnn_window_30 <- keras_search(window = 30)
save(rnn_window_30, file =  "res_window_30")
 

#Window = 120
rnn_window_120 <- keras_search(window = 120)
save(rnn_window_120, file =  "res_window_120")

load("rnn_window_3")
load( "rnn_window_7")
load("rnn_window_30")
load("rnn_window_120")

rnn_window_3 <- res_window_3 %>% 
  mutate(model_no = row_number(),
         window  = 3)

rnn_window_7 <- res_window_7 %>% 
  mutate(model_no = row_number(),
         window  = 7)

rnn_window_30 <- res_window_30 %>% 
  mutate(model_no = row_number(),
         window  = 30)

rnn_window_120 <- res_window_120%>% 
  mutate(model_no = row_number(),
         window  = 120)


```



## Modelling

## Discussion
