---
title: "DS4I - Project Writeup"
format: html
execute:
  echo: false
  warning: false
  message: false
  results: hide
editor: visual
---

```{r setup}


#libraries 

library(kableExtra)
library(tidyverse)
library(ggplot2)
library(ggpubr)
library(lubridate)
library(caret)

#theme for ggplot2, let me know if you do not like this theme and we can change it :)

#Theme for ggplot2 
theme_set(
  theme_bw(base_size = 9) +
    theme(
      plot.title   = element_text(hjust = 0.5, size = 9),
      panel.border = element_rect(colour = "black", fill = NA, linewidth = 0.5),
      # keep grids but make them subtle
      panel.grid.major = element_line(colour = "grey85", linewidth = 0.3),
      panel.grid.minor = element_line(colour = "grey92", linewidth = 0.2),
      # base-R style text/ticks
      axis.text  = element_text(colour = "black"),
      axis.title = element_text(colour = "black"),
      axis.ticks = element_line(colour = "black", linewidth = 0.3)
    )
)



#making geompoint have hollow circles
update_geom_defaults(
  "point",
  list(size = 1, alpha = 0.9, shape = 21, colour = "black")
)


data = read.csv("scotland_avalanche_forecasts_2009_2025.csv")


```

```{r}
#| echo: false
knitr::include_graphics("av.jpg")

```

[Photo by Nicolas Cool on Unsplash]{style="font-size:12px"}

## Introduction

## Literature Review

Snow avalanches are a recognised hazard in snow-prone mountain regions worldwide. Unlike in many alpine regions, avalanches in Scotland rarely impact large settlements and infrastructure, however, pose significant risk to recreational users in the Scottish Highlands (Diggins, 2009; Ward, 1980). Winter activities, such as climbing, walking, and skiing are increasingly popular, leading to greater human exposure to avalanche-prone terrain and resulting in numerous injuries and occasional fatalities each year (Scottish Avalanche Information Service \[SAIS\], 2024). This has prompted several studies to examine Scottish snowpack conditions and avalanche forecasting strategies, with the earliest studies conducted between 1970 and 1980 (Langmuir, 1970; Spink, 1970; Beattie, 1976; Ward, 1980; Ward & Beattie, 1985). However, these early studies were largely descriptive, relying on snow-pit measurements and expert judgements to identify snow properties and weather conditions correlated with avalanche risk. Building on this foundation, Ward (1984) applied one of the first predictive forecasting models for avalanches in Scotland.

Numerous studies emphasise the inherent complexity of avalanche modelling where predictions are based on multiple interacting factors (Choubin et al., 2019; Herwijen et al., 2016; Hendrick et al., 2023; Pozdnoukhov et al., 2008; Singh & Ganju, 2008). This complexity is only heightened in regions with highly variable weather patterns (Sharma & Ganju, 1999). In response, early statistical methods such as nearest neighbours became a popular technique to support (not replace) forecasters in predicting avalanche outcomes through relating current weather conditions to past avalanche events (Blagovechshenskiy et al., 2023; Kala et al., 2025; Pozdnoukhov et al., 2018; Singh & Ganju, 2008). However, this approach is susceptible to overfitting and struggles with complex and high-dimensional data (Kala et al., 2025; Pozdnoukhov et al., 2018; Singh & Ganju, 2018). Using a dataset from Lochaber, Scotland, Pozdnoukhov et al. (2008) reported that support vector machines (SVMs) showed a broadly similar performance to a baseline nearest neighbours method, however, had the added advantage of handling high dimensionality datasets and produced more descriptive forecasts. SVMs have been explored and shown success in aiding avalanche risk forecasting in other regions including Iran, India, Switzerland, and Tibet (Choubin et al., 2019; Rahmati et al., 2019; Schirmer et al., 2009; Tiwari et al., 2021; Wen et al., 2022). 

Over recent years, more complex machine learning (ML) algorithms such as regression trees, random forests, and neural networks have demonstrated strong predictive performance in avalanche forecasting (Blagovechshenskiy et al., 2023; Choubin et al., 2019; Gauthier et al., 2025; Hendrick et al., 2023; Rahmati et al., 2019; Singh & Ganju, 2018; Tiwari et al., 2021; Wen et al., 2022). Avalanche datasets are often high-dimensional, and using human forecaster judgement to select which features to include can be highly subjective and variable (Helbig et al., 2015; Pozdnoukhov et al., 2008). Hybrid approaches, which combine ML with expert-guided feature selection, have shown practical value (Gauthier et al., 2025). However, fully data-driven forecasting methods, particularly neural networks, offer important advantages.

Simulating the work of a human brain, neural networks are deep learning algorithms consisting of interconnected nodes/neurons that can model complex and nonlinear relationships between inputs and outputs (Blagovechshenskiy et al., 2023; Sharma et al., 2023; Tu, 1996). Their ability to process all input variables simultaneously allows them to implicitly learn interactions and patterns in the data without prior knowledge on which variables are most important (Blagovechshenskiy et al., 2023; Fromm & Schonberger, 2022; Sharma et al., 2023). Studies in avalanche forecasting from regions such as Switzerland, Kazakhstan, and  India have demonstrated that neural networks not only achieve strong predictive performance, but can also assess the relative importance of each variable to avalanche risk (Fromm & Schonberger; Sharma et al., 2023; Singh & Ganju, 2008). This ability makes them particularly well suited for high-dimensional datasets, such as those encountered in avalanche forecasting, as well as in contexts where prior domain knowledge is limited or when the most relevant predictors are unknown.

Despite advances in data-driven avalanche forecasting worldwide, research specific to Scotland remains limited. This scarcity lies in three domains: 

\(1\) The small number of published forecasting studies (only 9 relevant peer-reviewed papers exist on Scopus, with the most recent study conducted in 2011 (keywords: ( "avalanche forecasting" OR "avalanche prediction" ) AND Scotland)

\(2\) The restricted range of statistical approaches explored (only KNN and SVM) (Joachim et al., 2004; Pozdnoukhov et al., 2008; Pozdnoukhov et al., 2011; Purves et al., 2003)

\(3\) The focus on only a few regions in Scotland (primarily the Cairngorms and Lochaber regions)

This limited research interest likely stems from the highly localised nature of avalanche risk in Scotland, which generally occurs in remote areas and impacts only a small subset of individuals (Diggins, 2009; SAIS, 2024). Nevertheless, improved avalanche forecasting is argued to be increasingly important under changing climate conditions and global warming, which may alter snowpack properties and avalanche risk and frequency (Gauthier et al., 2025; Werritty & Sugden, 2013). A detailed discussion of climate change, however, is beyond the scope of this paper. As such, the present study aims to extend forecasting research in Scotland both spatially and methodologically by applying a neural network (or two or three?????) across the six distinct regions of Creah Meagaidh, Glencoe, Lochaber, Northern Cairngorms, Southern Cairngorms, and Torridon.

\<insert paragraph explaining modelling approaches of prev papers + this project’s modelling approaches. I.e. the neural net architecture\>

Hypothesis???? We can workshop together

## Context and Dataset

Scotland experiences a mild, wet, and temperate maritime climate due to the strong influence of the North Atlantic Ocean and persistent south-westerly winds (Pozdnoukhov et al., 2008; Scottish Government, 2011). These conditions result in rapid temperature changes, frequent heavy precipitation (falling as either snow or rain in the winter), and strong winds. With the highest peak, Ben Nevis in Lochaber (1345 m), mountains are low compared to alpine ranges (\~ 4000 m), resulting in a snowpack that is typically shallower, wetter, and more variable in comparison (Britannica, 2025; Pozdnoukhov et al., 2008; WSL Institute for Snow and Avalanche Research SLF, 2016). Aspects such as rapid freeze-thaw cycles, rain-on-snow, and wind redistribution further destabilise snow structure and stability, creating avalanche risks that can arise and disappear within a short time (Diggins, 2009; Pozdnoukhov et al., 2008; Purves et al., 2003; Scottish Government, 2011). Snowpack conditions also differ regionally; while the West is characterised as strongly maritime, with mild and wet winters, the North and East experience colder and drier conditions (MetOffice, 2010). As such, Scotland’s climate is highly variable both spatially and temporally, adding to the complexity of predicting avalanche risk.

This paper uses a 15-year archive of avalanche forecasts from Scotland across the six areas of Creah Meagaidh, Glencoe, Lochaber, Northern Cairngorms, Southern Cairngorms, and Torridon, produced by the Scottish Avalanche Information Service (2025). Figure ? illustrates the spatial distribution of avalanche observations over this time frame, with points marking individual events and labels indicating the locations of these six areas. The overall total observations by area and hazard category (Table ?) reflects the difference in observation frequency rather than absolute avalanche activity: Cairngorms (Northern and Southern) and Lochaber, located in the colder and drier North and East, have the most recorded observations, whereas Torridon in the maritime West has substantially fewer. Across all areas, Low and Moderate hazard levels are the most frequently recorded, while High hazard events are comparatively rare and concentrated primarily in Northern Cairngorms and Lochaber.

```{r map, echo = FALSE}



library(dplyr)
library(ggplot2)
library(ggrepel)
library(maps)

# palette 
hazard_cols <- c(
  "Low"            = "#d4f0f0",
  "Moderate"       = "#91bfdb",
  "Considerable -" = "#fc8d59",
  "Considerable +" = "#d73027",
  "High"           = "#7f0000"
)
hazard_levels <- names(hazard_cols)
data <- data %>% mutate(OAH = factor(as.character(OAH), levels = hazard_levels, ordered = TRUE))

# label positions 
area_labels <- data %>%
  filter(!is.na(longitude), !is.na(latitude), !is.na(Area)) %>%
  group_by(Area) %>%
  summarise(lon = median(longitude, na.rm = TRUE),
            lat = median(latitude,  na.rm = TRUE),
            .groups = "drop")

# tight bounding box from points
d_pts <- data %>% filter(!is.na(longitude), !is.na(latitude), !is.na(OAH))
pad_x <- 0.25; pad_y <- 0.20
xlim_use <- range(d_pts$longitude, na.rm = TRUE) + c(-pad_x, pad_x)
ylim_use <- range(d_pts$latitude,  na.rm = TRUE) + c(-pad_y, pad_y)

# plot
ggplot() +
  borders("world", regions = "UK", fill = "grey92", colour = "grey70") +
  geom_point(
    data = d_pts,
    aes(x = longitude, y = latitude, fill = OAH),
    size = 1.8, alpha = 0.85
  ) +
  ggrepel::geom_text_repel(
    data = area_labels,
    aes(x = lon, y = lat, label = Area),
    size = 3, seed = 941,
    box.padding = 0.15, point.padding = 0.1,
    min.segment.length = 0, max.overlaps = 20
  ) +
  scale_fill_manual(values = hazard_cols, drop = FALSE, name = "OAH") +
  coord_quickmap(xlim = xlim_use, ylim = ylim_use, expand = FALSE) +
  labs(
    title = " Figure 1: Observed Avalanche Hazard (OAH) — Scotland",
    subtitle = "Point observations coloured by hazard level; SAIS areas labelled",
    x = NULL, y = NULL
  ) +
  theme_minimal(base_size = 12) +
  theme(
    panel.grid.major = element_line(color = "grey95"),
    legend.position  = "right",
    plot.title       = element_text(face = "bold")
  )


```

```{r tot-obs-table, echo = FALSE}

library(dplyr)
library(tidyr)
library(knitr)

area_hazard_table <- data %>%
  group_by(Area) %>%
  filter(!is.na(Area), !is.na(OAH)) %>%
  count(Area, OAH) %>% #count all observations per area per avalanche category
  pivot_wider(
    names_from = OAH, #hazard levels as columns
    values_from = n,
    values_fill = 0
  ) %>%

  rowwise() %>% #total overall instead of across seasons
  mutate(Total = sum(c_across(all_of(c("Low", "Moderate", "Considerable -", "Considerable +", "High"))))) %>%
  ungroup()



kable(area_hazard_table, caption = "\\emph{Overall total observations by Area and hazard category.}")



```

Data cover the Scottish winters (November to April) from the December 2009 to March 2025. Predictor variables are categorised into three main groups, including (1) position and topography, (2) weather, and (3) snowpack test, all collected at the forecast location. Each observation also includes the date and the forecast area (metadata). For an overview of the predictors available, see Table ? below.

```{r pred_tab, echo = FALSE}

pred_table <- data.frame(Predictor_Group = c("Metadata", "Position and Topography", "Weather", "Snowpack Test"), Variables = c("date, area", "longitude, latitude, altitude, aspect of slope, incline of slope", "air temperature, wind direction, wind speed, cloud cover, precipitation code, snowdrift, total snow depth, foot penetration, ski penetration, rain observed at 900m elevation, summit air temperature, summit wind direction, summit wind speed", "max. temperature gradient, max. hardness gradient, no.settle, snow.index, insolation, crystals, wetness, AV.Cat, and snow temperature"))

pred_table %>%
  kable(caption = "\\emph{Overview of predictor variables available in the avalanche forecast dataset.}") %>%
  column_spec(1, width = "8em") %>%
  column_spec(2, width = "30em") %>%
  kable_styling(latex_options = c("striped", "hold_position"))


```

## Exploratory Data Analysis

### a. Feature Map Correlations

```{r feature-map-correlations-code}


library(dplyr)
library(ggplot2)

dat <- data
dat <- dplyr::ungroup(dat)  # in case it's a grouped tibble

#  Keep only numeric columns, drop seasonal values
num_df <- dat %>%
  select(where(is.numeric)) %>%
  select(
    -any_of(c("fah_i", "oah_i", "season_id")),   # <- explicitly drop season_id
    -matches("^(dayofseason|season(_year|_d)?|seasonality|month|year|hour)$", ignore.case = TRUE)
  )

#  must have at least 2 numeric vars
if (ncol(num_df) < 2L) stop("Not enough numeric columns for correlation analysis.")

#  Drop zero-variance columns
ok_cols <- vapply(num_df, function(x) sd(x, na.rm = TRUE) > 0, logical(1))
num_df  <- num_df[, ok_cols, drop = FALSE]

#  Correlation matrix (Pearson; pairwise complete)
corr_mat <- cor(num_df, use = "pairwise.complete.obs", method = "pearson")

# Long form 
corr_long <- as.data.frame(as.table(corr_mat)) %>%
  rename(var1 = Var1, var2 = Var2, r = Freq) %>%
  filter(as.character(var1) != as.character(var2)) %>%
  mutate(abs_r = abs(r))

# Heatmap — colours only
p_corr <- ggplot(as.data.frame(as.table(corr_mat)),
                 aes(Var1, Var2, fill = Freq)) +
  geom_tile() +
  scale_fill_gradient2(limits = c(-1, 1)) +
  labs(title = "Figure 2: Map of Feature Correlations",
       x = NULL, y = NULL, fill = "r") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1),
        panel.grid = element_blank())

```

```{r top-pairs-table, echo = FALSE, include = FALSE}

# Table for top pairs of highly correlated variables
library(dplyr)

top_pairs <- corr_long %>%
  rowwise() %>%
  mutate(pair = paste(sort(c(var1, var2)), collapse = "_")) %>%
  ungroup() %>%
  distinct(pair, .keep_all = TRUE) %>%
  arrange(desc(abs_r)) %>%
  slice(1:5) %>%
  select(var1, var2, r, abs_r)

if (knitr::is_html_output()) {
 
  DT::datatable(
    top_pairs,
    caption = htmltools::tags$caption(
      style = 'caption-side: top; text-align: left;',
      'Most highly correlated variable pairs (numeric variables only)'
    ),
    options = list(pageLength = 10, order = list(list(4, 'desc'))),
    rownames = FALSE,
    class = 'compact stripe hover'
  ) |>
    DT::formatRound(c('r', 'abs_r'), 3)
} else {
  # Static table that shows NOW in the editor preview
  knitr::kable(
    top_pairs,
    caption = "Most highly correlated variable pairs (numeric variables only)",
    digits = 3,
    align = c('l','l','r','r')
  )
}


```

The Scottish avalanche dataset contains several features that are moderately to strongly correlated, while others appear largely independent (Figure ?). This pattern suggests a potential redundancy. From a modelling perspective, highly correlated features can increase complexity and obscure unique contributors, however, can also capture the multifactorial nature of avalanche risk. In the context of neural networks, the presence of correlated variables is not inherently problematic, since the model can learn non-linear interactions. However, strong correlations can increase the risk of overfitting if the network memorises patterns rather than generalising to new data.

The highest correlation found was between summit air temperature and air temperature (r = `r top_pairs[1,4]`), indicating strong redundancy. Nevertheless, both variables were retained as each likely capture a different aspect relevant to forecasting (i.e., summit conditions versus broader conditions). Air temperature is an identified key driver of avalanche conditions and is expected to be an important predictor in the final neural network (Fromm & Schonberger, 2022; Gauthier et al., 2025; Pozdnoukhouv et al., 2018; Souckova et al., 2022; Ward,1980). Exploration of air temperature across observed avalanche hazard (OAH) levels further supports its importance in forecasting (Figure ?). Higher hazard categories are generally associated with lower air temperatures, with “Considerable -”, “Considerable +”, and “High” hazards centred below 0 °C. “Low” hazard days show medians above freezing and wider variability. The transition at “Moderate” hazard, where temperatures cluster around 0 °C, reflects conditions that can either stabilise or destabilise the snowpack through freeze-thaw cycles. These patterns highlight Scotland's sensitivity to rapid temperature shifts and their impact on avalanche activity (Diggins, 2009; Pozdnoukhov et al., 2008).

Other correlations with 0.5\<\|r\|\< 0.80 are moderately strong but remain informative and manageable, as neural networks can accommodate interdependent inputs.

```{r correlation-plot, echo = FALSE}
p_corr
```

```{r temperature-plot}

# Plot 1: Boxplot of Air.Temp by OAH (Figure 5)
data %>%
  filter(!is.na(Air.Temp), !is.na(OAH)) %>%
  ggplot(aes(x = OAH, y = Air.Temp, fill = OAH)) +
  geom_boxplot(outlier.alpha = 0.3) +
  scale_fill_manual(values = hazard_cols, drop = FALSE, guide = "none") +
  labs(
    title = "Figure 3: Distribution of Air Temperature across Observed Avalanche Hazard (OAH)",
    x = "Observed Avalanche Hazard (OAH)",
    y = "Air Temperature (°C)"
  ) +
  theme(
    plot.title = element_text(size = 12, face = "bold") # smaller than before
  )

```

### b. Precipitation Type

```{r precipitation-plot-code, echo = FALSE}


library(dplyr)
library(ggplot2)
library(scales)
library(stringr)
library(forcats)

df <- dplyr::ungroup(data)  

#  Columns
precip_col <- names(df)[str_detect(tolower(names(df)), "precip")] %>% .[1]
stopifnot(length(precip_col) == 1)
oah_col <- names(df)[tolower(names(df)) == "oah"] %>% .[1]
stopifnot(length(oah_col) == 1)

# Hazard levels + palette
hazard_levels <- c("Low","Moderate","Considerable -","Considerable +","High")
df[[oah_col]] <- factor(df[[oah_col]], levels = hazard_levels, ordered = TRUE)
hazard_cols <- c(
  "Low"            = "#d4f0f0",
  "Moderate"       = "#91bfdb",
  "Considerable -" = "#fc8d59",
  "Considerable +" = "#d73027",
  "High"           = "#7f0000"
)

#Summarise counts/proportions, drop NA in precip
oah_precip <- df %>%
  filter(!is.na(.data[[precip_col]]), !is.na(.data[[oah_col]])) %>%
  count(precip = .data[[precip_col]], oah = .data[[oah_col]], name = "n") %>%
  group_by(precip) %>%
  mutate(prop = n / sum(n)) %>%
  ungroup()

#  Extract leading numbers to identify bins; 
oah_precip <- oah_precip %>%
  mutate(bin_num = suppressWarnings(as.numeric(str_extract(as.character(precip), "^-?\\d+")))) %>%
  filter(!is.na(bin_num), bin_num <= 10)

# Order precipitation bins numerically
lvl_order <- oah_precip %>%
  distinct(precip, bin_num) %>%
  arrange(bin_num, precip) %>%
  pull(precip) %>%
  unique()

oah_precip <- oah_precip %>%
  mutate(precip = factor(precip, levels = lvl_order, ordered = TRUE))


```

```{r prop-stat-heavy-snow, echo = FALSE}
# cumulative prop for highest hazard level
heavy_snow <- oah_precip %>%
  filter(bin_num == 10)

cumulative_high <- heavy_snow %>%
  filter(oah %in% c("Considerable -", "Considerable +", "High")) %>%
  summarise(cum_prop = sum(prop)) %>%
  pull(cum_prop)
```

The distribution of OAH levels varies clearly across different precipitation types (Figure ?). Low hazard predominates under conditions of none (0) or trace (2) precipitation, whereas higher hazard categories become increasingly prevalent as snowfall intensity increases. Heavy snow (level 10) is associated with a substantially larger share of “Considerable” and “High” hazard ratings (`r cumulative_high*100` %). This aligns with existing knowledge of the Scottish climate, where rapid weather shifts and frequent rain or snow-on-snow events contribute to unstable snowpack conditions (Pozdnoukhov et al., 2008; Purves et al., 2003).

```{r precip-plot, echo = FALSE}
#  Plot
ggplot(oah_precip, aes(x = precip, y = prop, fill = oah)) +
  geom_col(position = "fill", width = 0.9) +
  geom_text(aes(label = percent(prop, accuracy = 1)),
            position = position_fill(vjust = 0.5),
            size = 3, colour = "white", fontface = "bold") +
  scale_y_continuous(labels = percent_format(accuracy = 1)) +
  scale_fill_manual(values = hazard_cols, drop = FALSE) +
  labs(
    title = "Figure 4: Precipitation type vs Observed Avalanche Hazard (OAH)",
    x = "Precipitation type/level",
    y = "Proportion within precipitation type",
    fill = "OAH"
  ) +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 30, hjust = 1))
```

### c. Seasonality

```{r seasons-plot}

library(dplyr)
library(ggplot2)
library(patchwork)


hazard_cols <- c(
  "Low"            = "#d4f0f0",
  "Moderate"       = "#91bfdb",
  "Considerable -" = "#fc8d59",
  "Considerable +" = "#d73027",
  "High"           = "#7f0000"
)

# Ensure OAH ordered Low -> High
data <- data %>%
  mutate(
    OAH = factor(OAH,
                 levels = c("Low","Moderate","Considerable -","Considerable +","High"))
  )

# Define season year: 

data <- data %>%
  mutate(
    month_num = lubridate::month(Date),
    season_year = if_else(month_num >= 11, lubridate::year(Date), lubridate::year(Date) - 1),
    month_in_season = factor(
      month_num,
      levels = c(11, 12, 1, 2, 3, 4, 5),
      labels = c("Nov","Dec","Jan","Feb","Mar","Apr","May")
    )
  )

# Aggregate counts and proportions per month (across all years)
oah_monthly <- data %>%
  filter(!is.na(OAH), month_in_season %in% levels(month_in_season)) %>%
  count(month_in_season, OAH, name = "n") %>%
  group_by(month_in_season) %>%
  mutate(month_total = sum(n),
         prop = n / month_total) %>%
  ungroup()

# A helper frame for the counts-only series
month_totals <- oah_monthly %>%
  distinct(month_in_season, month_total)


# Top: proportions
p_prop <- ggplot(oah_monthly, aes(x = month_in_season, y = prop, fill = OAH)) +
  geom_col(position = "fill", width = 0.85) +
  scale_fill_manual(values = hazard_cols, drop = FALSE) +
  labs(x = NULL, y = "Proportion of forecasts", fill = "OAH") +
  theme_minimal(base_size = 12) +
  theme(
    legend.position = "right",
    plot.margin = margin(t = 5, r = 10, b = 20, l = 10)   # extra space below
  )

# Bottom: counts 
max_n <- max(month_totals$month_total, na.rm = TRUE)
p_counts <- ggplot(month_totals, aes(x = month_in_season, y = month_total)) +
  geom_col(width = 0.85, fill = "grey70") +
  geom_text(aes(label = month_total), vjust = -0.25, size = 3.5) +
  scale_y_continuous(limits = c(0, max_n * 1.18), expand = expansion(mult = c(0, 0.02))) +
  labs(x = "Month (Nov–May)", y = "Number of forecasts") +
  theme_minimal(base_size = 12) +
  theme(
    plot.margin = margin(t = 22, r = 10, b = 5, l = 10)    # extra space above
  )


```

Monthly patterns of OAH across the extended avalanche season (November to May) reveal clear seasonal trends (Figure ?). Early (November-December) and late (April-May) months are dominated by low and moderate hazards, while mid-season months (Jan-Mar) contain a higher proportion of considerable and high hazards. Most forecasts occur between December and March, highlighting the period of greatest avalanche risk (SAIS, 2024). These patterns reflect the cyclical nature of snowpack development, with mid-winter weather instability driving higher-risk avalanches (Diggins, 2009; Podzdnoukhov et al., 2008; Purves et al., 2003). For modelling, this seasonality indicates that hazard levels are unevenly distributed, however, neural networks can accommodate complex, changing relationships between weather conditions and other important variables.

```{r season-plots, echo = FALSE}
# Combine with a title 
(p_prop / p_counts) +
  plot_layout(heights = c(1, 1)) +
  plot_annotation(
    title = "Figure 5. Seasonality of Observed Avalanche Hazard (OAH): Nov–May",
    theme = theme(
      plot.title = element_text(size = 15, face = "bold"),
      plot.subtitle = element_text(size = 11)
    )
  )
```

### d. FAH vs OAH

```{r OAH-FAH-plot-code, echo = FALSE}
library(dplyr)
library(tidyr)
library(ggplot2)
library(scales)
library(stringr)

# helpers 
find_col <- function(df, patterns) {
  nm <- names(df)
  hits <- sapply(patterns, function(p) nm[grepl(p, nm, ignore.case = TRUE)], simplify = FALSE)
  hit <- unique(unlist(hits))
  if (length(hit)) hit[1] else NULL
}

parse_datetime_safe <- function(x) {
  if (inherits(x, "POSIXct") || inherits(x, "Date")) return(as.POSIXct(x))
  if (!is.character(x)) x <- as.character(x)
  as.POSIXct(x, tz = "UTC",
             tryFormats = c("%Y-%m-%d %H:%M:%S",
                            "%Y-%m-%d %H:%M",
                            "%Y-%m-%d",
                            "%d/%m/%Y %H:%M:%S",
                            "%d/%m/%Y %H:%M",
                            "%d/%m/%Y",
                            "%m/%d/%Y %H:%M:%S",
                            "%m/%d/%Y %H:%M",
                            "%m/%d/%Y"))
}

#  hazard levels + normaliser
hazard_levels <- c("Low","Moderate","Considerable -","Considerable +","High")

norm_hazard <- function(x){
  x0 <- str_squish(tolower(as.character(x)))
  case_when(
    x0 == "low" ~ "Low",
    x0 == "moderate" ~ "Moderate",
    x0 %in% c("considerable -","considerable−","considerable–","considerable minus") ~ "Considerable -",
    x0 %in% c("considerable +","considerable plus")                                   ~ "Considerable +",
    x0 == "high" ~ "High",
    TRUE ~ NA_character_
  )
}

# ensure FAH/OAH columns exist 
if (!"fah" %in% names(dat)) {
  FAH_col <- find_col(dat, c("^FAH$", "forecast.*hazard"))
  if (!is.null(FAH_col)) dat <- mutate(dat, fah = .data[[FAH_col]])
}
if (!"oah" %in% names(dat)) {
  OAH_col <- find_col(dat, c("^OAH$", "observed.*hazard"))
  if (!is.null(OAH_col)) dat <- mutate(dat, oah = .data[[OAH_col]])
}

#  find Date and Area 
date_col <- find_col(dat, c("^date$", "forecast.*date", "datetime", "time"))
area_col <- find_col(dat, c("^area$", "region", "zone"))

if (is.null(date_col)) stop("No date-like column found. Expected something like 'Date', 'date', or 'datetime'.")
dat <- dat %>% mutate(.Date = parse_datetime_safe(.data[[date_col]]))
if (all(is.na(dat$.Date))) stop("Could not parse dates in the detected date column.")

#  next-day alignment: -
if (!is.null(area_col)) {
  dat_shifted <- dat %>%
    mutate(.Area = .data[[area_col]]) %>%
    arrange(.Area, .Date) %>%
    group_by(.Area) %>%
    mutate(oah_nextday_raw = lead(oah, 1)) %>%
    ungroup()
} else {
  dat_shifted <- dat %>%
    arrange(.Date) %>%
    mutate(oah_nextday_raw = lead(oah, 1))
}

# tidy and normalise labels 
df <- dat_shifted %>%
  transmute(
    FAH = factor(norm_hazard(fah), levels = hazard_levels),
    OAH_next = factor(norm_hazard(oah_nextday_raw), levels = hazard_levels)
  ) %>%
  drop_na(FAH, OAH_next)

# contingency + proportions (each FAH column = 100%) 
tab_counts <- with(df, table(OAH_next, FAH))
tab_prop   <- prop.table(tab_counts, 2)


# Figure 6 
df_prop <- as.data.frame(tab_prop) %>%
  rename(p = Freq, OAH = OAH_next) %>%
  mutate(on_diag = as.character(OAH) == as.character(FAH))

```

Lastly, Figure 6 shows the relationship between forecast avalanche hazard (FAH) on day *t* and observed avalanche hazard (OAH) on day *t+1*. Each column sums to 100%, showing how outcomes were distributed given a specific forecast level. Forecasts of "Low" hazard were highly accurate, with `r tab_prop[1,1]*100` % of cases confirmed the following day. This indicates that forecasters were most successful in predicting stable snowpack conditions and low hazard scenarios.

For intermediate levels such as “Moderate” and “Considerable –” the agreement weakens. Only about half of “Moderate” forecasts matched the following day’s observations, with a substantial spillover into “Low” and “Considerable –”. “Considerable –” forecasts split almost evenly between being observed as “Moderate,” “Considerable –,” and “Considerable +.” This pattern shows that there is a challenge in differentiating between neighbouring hazard categories, which may highlight the subjectivity and sensitivity of these intermediate hazard thresholds.

Forecasts of “High” hazard showed weaker reliability. Fewer than one-quarter of these forecasts corresponded to an observed “High” hazard the next day. Most “High” forecasts aligned with “Considerable –” or “Considerable +” outcomes. This shows that there is a tendency towards over-prediction of extreme conditions, or difficulty in anticipating when weather and snowpack instabilities will escalate into truly high-risk scenarios.

```{r OAH-FAH-plot, echo = FALSE}
ggplot(df_prop, aes(x = FAH, y = OAH, fill = p)) +
  geom_tile(color = "white") +
  geom_tile(data = dplyr::filter(df_prop, on_diag),
            color = "black", linewidth = 0.9, fill = NA) +
  geom_text(aes(label = percent(p, accuracy = 1)), size = 3.2) +
  scale_fill_gradient(low = "white", high = "#238b45", labels = percent) +
  labs(
    title = "Figure 6. FAH (day t) → OAH (day t+1) proportions",
    subtitle = "Each FAH column sums to 100%; next-day alignment performed within Area (if available)",
    x = "Forecast Avalanche Hazard (FAH, day t)",
    y = "Observed Avalanche Hazard (OAH, day t+1)",
    fill = "Proportion"
  ) +
  theme_minimal(base_size = 12) +
  theme(axis.text.x = element_text(angle = 40, hjust = 1),
        plot.title = element_text(face = "bold", size = 14))
```

The plot shows that the forecast (FAH) is highest in stable, low-risk conditions and lowest at the extremes, with the middle categories marked by uncertainty. This implies that avalanche hazard ratings are best treated as ordinal outcomes for modelling purposes, where the cost of misclassification is not uniform and depends on how far apart the categories are. Neural networks can capture these patterns well when framed as ordinal classfiers. They can learn that misclassifications most often occur between neighbouring hazard levels, instead of treating all errors equally. Class imbalances will also need to be addressed during model training to ensure the network does not simply default to predicting the most common outcomes.

### e. Missing Values

```{r}

```

## Data Cleaning and Feature Engineering

Feature engineering mainly consisted of constructing temporal variables for the models. In particular, a feed forward neural network does not natively capture any time series trends. Given that the above exploratory analysis highlights that observed avalanche risk varies according to the season, and time of year, several variable were constructed to take this into account.

<br>

Firstly, before removing any observations that contained large amount of missing values, we constructed constructed several variable marking the samples time of day, day of the week, month and year from the date. Next, we created a variable to represent the subsequent seasons in the data, as we observed a trend that avalanche risk appeared to be increasing year on year. The first season of avalanche recordings begins in December in 2009 and ends in April. We constructed a categorical variable to represent each season from 2009 to 2025 beginning from December into the following year.

```{r read_data}


data = read.csv("scotland_avalanche_forecasts_2009_2025.csv")



data <- data %>% 
  mutate(across(where(is.character), as.factor))

c <- colnames(data)
r <- data.frame(var = character(),
                no = numeric())

for (i in c){
  
  if (is.factor(data[, i])){
  
  x <- length(unique(data[,i]))
  
  r <- rbind(r, data.frame(var = i,
                           no = x))
    
  }
  
  
}





```

```{r missing_val}

#Getting table of missing values


col.n <- colnames(data)
n <- nrow(data)

missing_df <- data.frame(Variable = character(),
                         "Missing (%)" = numeric(),
                         check.names = F)


for (i in col.n){
  
  x <- sum(is.na(data[,i]))
  
  perc <- (x/n) * 100
  
  missing_df <- rbind(missing_df,
                      data.frame(
                        Variable = i,
                        `Missing (%)` = perc
                      , check.names = F)
                      )
  
}
 
missing_df <- missing_df %>% 
  filter(`Missing (%)` > 0) %>% 
  arrange(desc(`Missing (%)`),)

#sum(!complete.cases(data))


```

```{r}



kable(missing_df, format = "html",
      digits = 4,
      caption = "Missing Value Exploration") %>%
  kable_styling(
    bootstrap_options = c("hover"),
    full_width = FALSE
  ) %>%
  footnote(general = "All values rounded to 4 decimal places.")

x1 <- unique(data$AV.Cat)

```

Missing values were also a challenge. Table ? presents the percentage of missing values for each feature in the data that had misisng values. *Av.Cat* was the first variable we felt should be eliminated. We could not find what this variable represented, and it had implausible values ( For example : `r x1` ). Shown in Figure ? below, we observed no discernible trend in the relationship between Ski penetration and OAH, save for High risk avalanches. Overall, we removed this feature, as the contribution of snow precipitation to the risk would likely be well represented by other variables in the data. Lastly, we removed Summit wind direction as well, (roughly 12% missing). Similarly, to *Ski.pen*, we notice no discernible trend, and decided to drop this feature as well.

```{r}


# 2) Plot (note the exact column name)
px <- ggplot(data, aes(x = OAH, y = Ski.Pen)) +
  geom_boxplot(na.rm = TRUE)+
  labs(title = "Figure ?")+
  ylim(0, 20)

px1 <- ggplot(data, aes(x = OAH, y = Summit.Wind.Dir) )+
  geom_boxplot(na.rm = TRUE)+
  labs(title = "Figure ?")+
  ylim(0,500)

p_all <- ggarrange(px, px1,
                   ncol = 2,
                   nrow = 1,
                   labels = c("A", "B"))

annotate_figure(p_all,
                bottom = text_grob("Note: Outliers have been excluded. Y-axis scale from 0 to 20 on the left, and 0 to 500 on the right.",
                                   hjust = 0, x = 0,       # left
                                   size = 7, color = "black"))



```

Lastly, we also dropped categorical variables that had very high cardinality, which included _OsGrid_ and _Location_. _Osgrid_ appeared to be some kind of ID variable with over 3000 unique values. When one hot encoding this would make our data set unnecessarily complex, and we surmised that this variable likely would not be indicative of risk. Likewise, we also dropped Location for similar reasoning. We had the latitiude and longitude of included in the data as well as the _Area_ indicative which we expected to be more indicative of avalanche risk, so we dropped this feature as well to reduce unnecessary complexity. 
```{r}

```



```{r}

data_edit <- data %>% 
  select(-c(AV.Cat, Summit.Wind.Dir, Ski.Pen))


comp <- round(sum(!complete.cases(data_edit))/sum(complete.cases(data_edit)) * 100)

```

Having dropped the above variables, we were left with roughly `r comp` % non-complete samples in the data. We dropped the remaining samples that contained missing values to maintain complete cases for the models. Ideally, it would be better to maintain more observations. However, we felt we still had a large enough sample to maintain representative results (around 7000 observations). At a later stage, we fit our best performing model to a simply imputed data set and achieve an accuracy roughly equivalent to the complete cases data set. 

```{r date_time}



#adding date variable
data$Date <- as.POSIXct(data$Date, format="%Y-%m-%d %H:%M:%S", tz="UTC")


data$month     <- lubridate::month(data$Date)
data$hour      <- lubridate::hour(data$Date)
data$year      <- lubridate::year(data$Date)

#make a day of the season variable

#season begins from December to april



data <- data %>% 
  mutate(season_year = if_else(month >= 12, year, year -1),
         season_id = dense_rank(season_year),
         season = paste0("season", season_id),
         season = as.factor(season)) %>% 
  group_by(season, Area) %>% 
  mutate(dayofseason = row_number()) %>% 
  arrange(Date, Area)





```

```{r drop variables}

#Drop the top 3 variables wtih the most missing values
data <- data %>% 
  select(- c(AV.Cat, Ski.Pen, Summit.Wind.Dir, Obs, Location, OSgrid))

data <- na.omit(data) #remove remaining missing



```

```{r split_data}

set.seed(1)
train.idx <- sample(1:nrow(data), 
                    round(nrow(data)*0.8))

train <- data[train.idx,] #Training data
test <- data[-train.idx,] #Testing Data

#levels(train$OAH)
#levels(test$OAH)
```

### Modeling Methodology

There are three main models used in this project. Firstly, a feed forward neural network (FFN), a convolutional neural network (CNN) and a recurrent neural network (RNN). The idea behind the feed forward neural network is that it would likely leverage the categorical variables, like forecasted avalanche risk, well in predicting the target. However, CNN and RNN were more likely to leverage any time series trends present in the data.

A feed forward neural network makes use of a series of nodes. The basic structure of neural networks is first input nodes, variable layers of perceptron nodes and output nodes. The perceptron nodes (or hidden layers) contain activation functions and weights. The first hidden layer nodes take the weighted sum of the input layer, apply the activation function to this input and multiply that by the weight. Subsequent layers take the previous layers calculations as input. The weights determine the amount of influence that part of the system has on the algorithm, while the activation functions introduce non-linearity. To determine the weights, they are first randomly assigned, and then back propagation is completed during training with an optimizer like gradient descent to assign appropriate values.

Convolutional neural networks work differently, by sliding filters across the data which allow the algorithm to consider broader trends. The filter is essentially a matrix of weights, and the dot product between these weights and the data is calculated. These individual values are taken across the feature space, in turn highlighting any local patterns. Using multiple filters at once during training allows for the algorithm to learn various patterns simultaneously. After completing the above convolution, activation functions are applied to the resultant feature map to introduce non-linearity. Typically, a process called pooling is also applied, which reduces the dimensionality of the feature space, while preserving the most informative trends. One can stack numerous layers of convolutions to learn deeper trends in the data, although we found that 2 layers were a good balance between the model’s complexity and ability to generalize.

```{r}


#| echo: false
#| results: asis



htmltools::includeHTML("cnn_filter_animation_quarto_ready_html.html")
```

\[add section on RNN\]

### Data Preprocessing of Neural Networks models

To prepare the data for CNN and RNN, we constructed an array with 3 dimensions which include the number of samples, the time window length and the number features. Each training sample is created by sliding a fixed window across the data. Within the array, the columns represent individual time steps, and the depth represent the features of the data. Since our prediction is one day ahead, each training example represents the observations within the window, for the next day prediction. Furthermore, the data was standardized to ensure the algorithms were not distorted by the different scales of the features. Lastly, the data was also one hot encoded, such that each categorical variable is split into multiple binary variable demarcating each category.

To preserve the temporal trends of the data, for the CNN and RNN the train and test split was not randomly shuffled. Instead, the first 80% of avalanche records were used as training examples, and the remaining became the test set.


## Convolutional Neural Network

The selection of the convolution neural network and recurrent neural netowrk parameters involved two stages. Firstly, several candidate data sets were created with various time windows. In our cases, we tested a window of 3 days, 7 days, 30 days and 120 days. We kept the search parameters more narrow in this stage, and estimated many models in order to assess what prediction window would be most appropriate, according to validation accuracy. To estimate validation accuracy we used cross validation with 20% holdout set. All models were trained for 60 epochs save for the Feed forward Neural Network.

In the second stage, a more broad grid search was conducted on the chosen window, and the best model was chosen according to the highest validation accuracy. In all our tests presented for the CNN, we assessed architectures with two convolutional layers using the Relu function in those latent layers, and the softmax function in our input layer to create the class predictions. More complex architectures did not perform better, or meaningfully better. 


```{r Keras_data_preprocessing_function}

# library(keras3)
# library(tensorflow)
# library(reticulate)

# reticulate::install_miniconda()   # sets up a private Python just for R
# library(keras3)
# install_tensorflow()


#for this to work I need to convert the data into an array 
# form is (N (length), L (Time frame), F (Features))

#I promise that this code isnt made by Chatgpt I am just using a lot of comments otherwise I dont know what the #$!* is going on :)


#Function to let me test for numerous window sizes


pre_data <- function(w = 3, h = 1){

#--------Data Preparation-------------------------------------

T <- length(data$Date) #time period length

data_nn <- data %>%
  select(-c(Date)) #removing date column

L <- w #Rolling window length = 3. So what I am decided is that It can use the previus 3 days on record to train itself
H <- h # predict 1 day ahead


#number of samples we can have, so number of time slices, each one iterated one step ahead by H (which is just 1 day in our case)
N_samp <- T - L - H + 1



#----------------One hot encoding--------

#This part is quite a process. So big picture, the numeric data needs to be standardised, but when we do one-hot-encoding, it becomes quite messy to avoid standardising the dummy variables. SO we are going to make indices for the numeric columns so we can standardise only them later

#Taking our predictors and covariates into one df, and our labels into another

X.variables <- data_nn %>%
  select(-OAH)   #x_variables = everything in data other than OAH column

X.variables <- model.matrix(~., data = X.variables)[,-1] # One Hot encoding

y.variable <- data_nn[["OAH"]]  #y_variable = only our OAH column

numeric.variables <- names(data_nn)[sapply(data_nn, is.numeric)]
numeric.variables <- setdiff(numeric.variables, "OAH")

numeric.indexes <- which(colnames(X.variables) %in% numeric.variables) #okay so the colnames are the same so we can find the indexes in x variables which are numeric with this code <--

#dummy variable indexes
dummy.indexes <- setdiff(1:(ncol(X.variables)), numeric.indexes)


#Number features

F_ <- ncol(X.variables)

#--------------Making Array---------------------#


#Arrays - this is the confusing part. Keras needs an array for this to work. In our case we can think of this as turning our data into a stack of data sheets. each sheet is for a particular feature. THen there are N_sampl columns, and each is shifted by L

X <- array( NA_real_, #just an empty array, which will subsequently be filled
           dim = c(N_samp,L, F_  ),
           )

y <- integer(N_samp)




for (i in 1:N_samp){ #here we are filling in the array with our data

  index.window <- i:(i + L - 1) #index wind = ith value in N_samp to I + window minus 1

  X[i, , ] <- X.variables[index.window, , drop = F] #filling in the predictors with the corresponding indexes into that window
  y[i] <- y.variable[i + L + H -1] #we predict the ith + L + H y values.

}



#------------Training and testing split-----------------------------

#We want to preserve time series patterns, so no shuffling

#old version where everything was in order
idx <- 1:N_samp
number.training <- floor(N_samp * 0.8) #train = first 80% of features

tr <- idx[1:number.training] #Training indices
te <- idx[(number.training + 1):N_samp] #Testing indices

#making 1 = 0 to see if that helps

y <- y - 1 # Now categories are {0,1,2,3,4,5,}

n.classes <- length(unique(y))

#test for randomness being better (didnt work)
# idx <- 1:N_samp
# set.seed(1)
# tr <- sample(idx, floor(0.8*N_samp))
# te <- idx[-tr]



X.train <- X[tr, , , drop = F] #training predictors data array
X.test <- X[te, , , drop = F ] #testing predictors array
y.train <- y[tr] #training target
y.test <- y[te]


#Class weights
class_counts <- table(y.train)
total <- sum(class_counts)
n_classes <- length(class_counts)


class_weights <- total / (n_classes * class_counts)

# keras expects a named list: names = class index as character
class_weights <- as.list(class_weights)
names(class_weights) <- as.character(0:(n_classes-1))



#----------------Scaling--------------------

#Remeber that we made indexes for our numeric data, which is the data we want to scale.

#We also need to scale the training data, and scale the testing data according to the training data mu and sd


if (length(numeric.indexes) > 0){

  mu.numeric <- apply(X.train[, , numeric.indexes, drop = F], 3, mean, na.rm = T)

  sd.numeric <- apply(X.train[ , , numeric.indexes, drop = F], 3, sd, na.rm = T)

   sd.numeric[sd.numeric == 0 | is.na(sd.numeric)] <- 1 #make sure SD == 1

  for (j in seq_along(numeric.indexes)){

    k <- numeric.indexes[j]
    X.train[, , k] <- (X.train[ , , k] - mu.numeric[j])/sd.numeric[j]
    X.test[, , k] <- (X.test[ , , k] - mu.numeric[j])/sd.numeric[j]
  }


}


round(apply(X.train[ , , numeric.indexes, drop = FALSE], 3, mean), 3)
round(apply(X.train[ , , numeric.indexes, drop = FALSE], 3, sd),   3)



#Getting there !

#Since we are doing a multiclass prediction we need to use softmax
#This means using the tocategorical() function on y

y.test <- to_categorical(y.test, num_classes = n.classes)


y.train <- to_categorical(y.train, num_classes = n.classes)

list(y.test, y.train, X.train, X.test, class_weights, F_)





}
```

```{r keras_grid_search}
#testing c-d-c

#vary window size hasnt made significant changes
#Adding weights to control for imbalance didnt help

keras_search <- function(window){

W_ <- pre_data(w = window, h = 1)

y.test <- W_[[1]]
y.train <- W_[[2]]
X.train <- W_[[3]]
X.test <- W_[[4]]
cl_weights <- W_[[5]]
F_ <- W_[[6]]
# ---------------- Grid search for existing architecture ----------------


set.seed(1)
tf$keras$utils$set_random_seed(123L)

# Define the grid (adjust values if you want a bigger/smaller search)
grid <- expand.grid(
  filters1   = c( 32, 64),
  filters2   = c( 32, 64),
  kernel_sz  = c(2, 5, 15),
  drop1      = c(0.3, 0.5),
  drop2      = c(0.3, 0.5),
  stringsAsFactors = FALSE
)

results <- data.frame(
  filters1 = integer(), filters2 = integer(),
  kernel_sz = integer(), drop1 = double(), drop2 = double(),
  val_accuracy = double(), test_loss = double(), test_accuracy = double(),
  stringsAsFactors = FALSE
)

histories <- vector("list", nrow(grid))  # optional: keep training histories

for (i in seq_len(nrow(grid))) {
  cat(sprintf("\n[%d/%d] f1=%d, f2=%d, k=%d, d1=%.2f, d2=%.2f\n",
              i, nrow(grid),
              grid$filters1[i], grid$filters2[i],
              grid$kernel_sz[i], grid$drop1[i], grid$drop2[i]))

  # Clear TF/Keras state between runs to avoid graph/memory accumulation
  tensorflow::tf$keras$backend$clear_session()

  
  set.seed(1)
  tf$keras$utils$set_random_seed(123L)

 
  input <- layer_input(shape = c(dim(X.train)[2], F_)) # (L, F)

  output <- input %>%
    layer_conv_1d(filters = grid$filters1[i],
                  kernel_size = grid$kernel_sz[i],
                  padding = "same", activation = "relu") %>%
    layer_dropout(rate = grid$drop1[i]) %>%
    layer_conv_1d(filters = grid$filters2[i],
                  kernel_size = grid$kernel_sz[i],
                  padding = "same", activation = "relu") %>%
    layer_global_average_pooling_1d() %>%
    layer_dropout(rate = grid$drop2[i]) %>%
    layer_dense(units = 5, activation = "softmax")

  model <- keras_model(inputs = input, outputs = output)

  model %>% compile(
    optimizer = "adam",
    loss = "categorical_crossentropy",
    metrics = "accuracy"
  )

  # ----- Train -----
  history <- model %>% fit(
    X.train, y.train,
    epochs = 60,
    batch_size = 64,
    verbose = 0,
    validation_split = 0.2,
    shuffle = FALSE
    # = cl_weights
  )

  # collect validation accuracy from the last epoch
  val_acc <- tail(history$metrics$val_accuracy, 1)
  if (length(val_acc) == 0) {
   
    val_acc <- tail(history$metrics$val_acc, 1)
  }

  # Evaluate on test set 
  test_metrics <- model %>% evaluate(X.test, y.test, verbose = 0)

  # test_metrics is usually a named numeric vector: c(loss=..., accuracy=...)
  test_loss <- as.numeric(test_metrics[[1]])
  test_acc  <- if (!is.null(names(test_metrics)) && "accuracy" %in% names(test_metrics)) {
    as.numeric(test_metrics[["accuracy"]])
  } else {
    as.numeric(test_metrics[[2]])
  }

  # store results
  results[i, ] <- list(
    grid$filters1[i], grid$filters2[i],
    grid$kernel_sz[i], grid$drop1[i], grid$drop2[i],
    as.numeric(val_acc), test_loss, test_acc
  )
  histories[[i]] <- history
}

# sort by best validation accuracy 
results <- results[order(-results$val_accuracy), ]

print(results)

# The best 
best <- results[1, ]

rm(W_) #For saving memory
gc()

return(results)

}



```

```{r keras_search}
#Window = 3 
# 
# res_window_3 <- keras_search(window = 3)
# save(res_window_3, file =  "res_window_3")
# 
# # #Window  = 7
# res_window_7 <- keras_search(window = 7)
# save(res_window_7, file =  "res_window_7")
# 
# #Window = 30
# res_window_30 <- keras_search(window = 30)
# 
# #Window = 120
# res_window_120 <- keras_search(window = 120)
# save(res_window_120, file =  "res_window_120")

load("res_window_3")
load("res_window_7")
load("res_window_30")
load("res_window_120")


res_window_3 <- res_window_3 %>% 
  mutate(model_no = row_number(),
         window  = 3)

res_window_7 <- res_window_7 %>% 
  mutate(model_no = row_number(),
         window  = 7)

res_window_30 <- res_window_30 %>% 
  mutate(model_no = row_number(),
         window  = 30)

res_window_120 <- res_window_120%>% 
  mutate(model_no = row_number(),
         window  = 120)


results_cnn_search <- rbind(res_window_3, res_window_30, res_window_7,  res_window_120)


```




```{r cnn_window_vis}
#graph of validation accuracy
ggplot(data = results_cnn_search, aes( x  = model_no, y  = val_accuracy, colour = as.factor(window)))+
  geom_point()+
  labs(x = "Model Number (arranged by  validation accuracy)") + 
  labs(title = "Figure ?: CNN Prediction Window Search",
       y = "Validation Accuracy",
       colour = "Window Length")


```
Considering Figure ? above, we decided on a prediction window of 7 days. Overall, out of the models tested, most perform best with a window of roughly 3 to 7 days, while 30 days and 120 days were shown to be meaningfully worse than former two options. Overall, validation accuracy between 3 to 7 days was competitive. However, 7 days appeared to be more robust, as the set of models with a 3 day prediction contained performances that achieved a much lower accuracy at the extreme end. 
<br>
<br>



```{r  expanded_search}
# #we will go with a window of roughly  7  days

# rm(res_window_120,  res_window_3, res_window_30)
# 
# W_ <- pre_data(w = 7, h = 1)
# 
# y.test <- W_[[1]]
# y.train <- W_[[2]]
# X.train <- W_[[3]]
# X.test <- W_[[4]]
# cl_weights <- W_[[5]]
# F_ <- W_[[6]]

# 
# # Repro seeds
# set.seed(1)
# tf$keras$utils$set_random_seed(123L)
# 
# # Define the grid (
grid <- expand.grid(
  filters1   = c( 16, 32, 64),
  filters2   = c( 16, 32, 64),
  kernel_sz  = c(5,15),
  drop1      = c(0.5,0.7),
  drop2      = c(0.5, 0.7),
  stringsAsFactors = FALSE
)
# 
# results <- data.frame(
#   filters1 = integer(), filters2 = integer(),
#   kernel_sz = integer(), drop1 = double(), drop2 = double(),
#   val_accuracy = double(), test_loss = double(), test_accuracy = double(),
#   stringsAsFactors = FALSE
# )
# 
# histories <- vector("list", nrow(grid))  # optional: keep training histories
# 
# for (i in seq_len(nrow(grid))) {
#   cat(sprintf("\n[%d/%d] f1=%d, f2=%d, k=%d, d1=%.2f, d2=%.2f\n",
#               i, nrow(grid),
#               grid$filters1[i], grid$filters2[i],
#               grid$kernel_sz[i], grid$drop1[i], grid$drop2[i]))
# 
#   # Clear TF/Keras state between runs to avoid graph/memory accumulation
#   tensorflow::tf$keras$backend$clear_session()
# 
# 
#   set.seed(1)
#   tf$keras$utils$set_random_seed(123L)
# 
# 
#   input <- layer_input(shape = c(dim(X.train)[2], F_)) # (L, F)
# 
#   output <- input %>%
#     layer_conv_1d(filters = grid$filters1[i],
#                   kernel_size = grid$kernel_sz[i],
#                   padding = "same", activation = "relu") %>%
#     layer_dropout(rate = grid$drop1[i]) %>%
#     layer_conv_1d(filters = grid$filters2[i],
#                   kernel_size = grid$kernel_sz[i],
#                   padding = "same", activation = "relu") %>%
#     layer_global_average_pooling_1d() %>%
#     layer_dropout(rate = grid$drop2[i]) %>%
#     layer_dense(units = 5, activation = "softmax")
# 
#   model <- keras_model(inputs = input, outputs = output)
# 
#   model %>% compile(
#     optimizer = "adam",
#     loss = "categorical_crossentropy",
#     metrics = "accuracy"
#   )
# 
#   # ----- Train -----
#   history <- model %>% fit(
#     X.train, y.train,
#     epochs = 60,
#     batch_size = 64,
#     verbose = 0,
#     validation_split = 0.2,
#     shuffle = FALSE
#     # = cl_weights
#   )
# 
#   # Collect validation accuracy from the last epoch
#   val_acc <- tail(history$metrics$val_accuracy, 1)
#   if (length(val_acc) == 0) {
#     # Fallback name in case of different metric key
#     val_acc <- tail(history$metrics$val_acc, 1)
#   }
# 
#   # Evaluate on test set
#   test_metrics <- model %>% evaluate(X.test, y.test, verbose = 0)
# 
#   # test_metrics is usually a named numeric vector:
#   test_loss <- as.numeric(test_metrics[[1]])
#   test_acc  <- if (!is.null(names(test_metrics)) && "accuracy" %in% names(test_metrics)) {
#     as.numeric(test_metrics[["accuracy"]])
#   } else {
#     as.numeric(test_metrics[[2]])
#   }
# 
#   # Store results
#   results[i, ] <- list(
#     grid$filters1[i], grid$filters2[i],
#     grid$kernel_sz[i], grid$drop1[i], grid$drop2[i],
#     as.numeric(val_acc), test_loss, test_acc
#   )
#   histories[[i]] <- history
# }
# 
# # Sort by best validation accuracy
# results <- results[order(-results$val_accuracy), ]
# 
# print(results)
# 
# final_cnn_search  <-  results
# save(final_cnn_search, file =  "final_cnn_serach")


load("final_cnn_serach")

# The best 
best <- final_cnn_search[1, ]

#best
# 
# gc()
```






```{r}

grid <- data.frame(
  hyperparameter = c("filters1", "filters2", "kernel_sz", "drop1", "drop2"),
  values = I(list(
    c(16, 32, 64),
    c(16, 32, 64),
    c(5, 15),
    c(0.5, 0.7),
    c(0.5, 0.7)
  ))
)



kable(grid, format = "html",
      digits = 4,
      caption = "Expanded Grid Search") %>%
  kable_styling(
    bootstrap_options = c("hover"),
    full_width = FALSE
  ) 


```

```{r  final_cnn_search_viz}


res <- final_cnn_search %>%
  mutate(
    run_id = row_number(),
    label  = sprintf("f1=%d f2=%d k=%d d1=%.1f d2=%.1f",
                     filters1, filters2, kernel_sz, drop1, drop2)
  ) %>%
  arrange(desc(val_accuracy)) %>%
  mutate(rank = row_number(),
         top5 = rank <= 5)

library(scales)


top_n <- 30
ggplot(res %>% slice(1:top_n),
       aes(x = reorder(label, val_accuracy), y = val_accuracy, fill = top5)) +
  geom_col() +
  coord_flip() +
  geom_text(aes(label = percent(val_accuracy, accuracy = 0.1)),
            hjust = -0.1, size = 3) +
  scale_y_continuous(labels = percent_format(accuracy = 1), expand = expansion(mult = c(0, .1))) +
  scale_fill_manual(values = c("lightblue", "#1f78b4"), guide = "none") +
  labs(title = sprintf("Figure ?: Top %d configs by validation accuracy", top_n),
       x = "Config", y = "Val accuracy")

x1 <- round(max(results_cnn_search$val_accuracy)*100)

```


After completing the above search, an expanded search with the hyperamaters presented in table ? was completed.The validation accuracy of the top 30 models is presented in Figure ? above. The Best performance overall was a validation accuracy of roughly 69%. We stuck with this model, as validation accuracy had not improved meaningfully over the course of the expanded search. The best performing model during the prediction window search acheived a validation accuracy of `r x1`. The final selected model involved 32 filters in the first layer, followed by a dropout of 70%, then 16 layers in the second layers, followed by dropout of 70% and global average pooling into the the final output layer. 
<br>
<br>


```{r fiiting_final_cnn}
# #best model  had 16 filters with 2 layers,  and kernel size 15,  0.7 dropout between layers




#   input <- layer_input(shape = c(dim(X.train)[2], F_)) # (L, F)
# 
#   output <- input %>%
#     layer_conv_1d(filters = 32, kernel_size = 15, padding = "same", activation = "relu") %>%
#     layer_dropout(rate = 0.7) %>%
#     layer_conv_1d(filters = 16, kernel_size = 15, padding = "same", activation = "relu") %>%
#     layer_global_average_pooling_1d() %>%
#     layer_dropout(rate = 0.5) %>%
#     layer_dense(units = 5, activation = "softmax")
# 
#   model <- keras_model(inputs = input, outputs = output)
# 
#   model %>% compile(
#     optimizer = "adam",
#     loss = "categorical_crossentropy",
#     metrics = "accuracy"
#   )
# 
#     history <- model %>% fit(
#     X.train, y.train,
#     epochs = 60,
#     batch_size = 64,
#     verbose = 0,
#     validation_split = 0,
#     shuffle = FALSE
#     # = cl_weights
#   )
# 
# final_cnn_model  <- model
# 
# 
# 
# 
# 
# test_metrics <- model %>% evaluate(X.test, y.test, verbose = 0)
# 
# test_metrics$accuracy
# 
# 
# 
# pred_cnn  <- predict(final_cnn_model, X.test)
# 
# pred_cnn <- max.col(pred_cnn)
# 
# save(pred_cnn, file   = "pred_cnn")

load("pred_cnn")

```

## Reccurrent Neural Network 

The approach for the Reccurent Neural Network was the same as the CNN discussed above. Firstly a search over numerous candidate wndow sizes was completed, and a subsequent expanded search was done as well. In Figure ? below, the validation accuracy of the models test is presented. We used a combination of two Long Short-Term memory layers (LSTM), with dropout and varied the LSTM units and dropout values in the grid search.

<br>
<br>


```{r  RNN_search_function}

#sam as above but for RNN model



 keras_search_rnn <- function(window){

  W_ <- pre_data(w = window, h = 1)

  y.test <- W_[[1]]
  y.train <- W_[[2]]
  X.train <- W_[[3]]
  X.test <- W_[[4]]
  cl_weights <- W_[[5]]
  F_ <- W_[[6]]

  set.seed(1)
  tf$keras$utils$set_random_seed(123L)


  grid <- expand.grid(
    units1 = c(32, 64),   # LSTM units in layer 1
    units2 = c(32, 64),   # LSTM units in layer 2
    drop1  = c(0.3, 0.5),
    drop2  = c(0.3, 0.5),
    stringsAsFactors = FALSE
  )

  results <- data.frame(
    units1 = integer(), units2 = integer(),
    drop1 = double(), drop2 = double(),
    val_accuracy = double(),
    test_loss = double(), test_accuracy = double(),
    stringsAsFactors = FALSE
  )

  histories <- vector("list", nrow(grid))

  for (i in seq_len(nrow(grid))) {
    cat(sprintf("\n[%d/%d] u1=%d, u2=%d, d1=%.2f, d2=%.2f\n",
                i, nrow(grid),
                grid$units1[i], grid$units2[i],
                grid$drop1[i], grid$drop2[i]))

    tensorflow::tf$keras$backend$clear_session()
    set.seed(1)
    tf$keras$utils$set_random_seed(123L)

    input <- layer_input(shape = c(dim(X.train)[2], F_)) # (timesteps, features)

    output <- input %>%
      layer_lstm(units = grid$units1[i], return_sequences = TRUE) %>%
      layer_dropout(rate = grid$drop1[i]) %>%
      layer_lstm(units = grid$units2[i], return_sequences = FALSE) %>%
      layer_dropout(rate = grid$drop2[i]) %>%
      layer_dense(units = 5, activation = "softmax")

    model <- keras_model(inputs = input, outputs = output)

    model %>% compile(
      optimizer = "adam",
      loss = "categorical_crossentropy",
      metrics = "accuracy"
    )

    history <- model %>% fit(
      X.train, y.train,
      epochs = 60,
      batch_size = 64,
      verbose = 0,
      validation_split = 0.2,
      shuffle = FALSE
    )

    # val acc from last epoch
    val_acc <- tail(history$metrics$val_accuracy, 1)
    if (length(val_acc) == 0) val_acc <- tail(history$metrics$val_acc, 1)

    test_metrics <- model %>% evaluate(X.test, y.test, verbose = 0)
    test_loss <- as.numeric(test_metrics[[1]])
    test_acc  <- if (!is.null(names(test_metrics)) && "accuracy" %in% names(test_metrics)) {
      as.numeric(test_metrics[["accuracy"]])
    } else {
      as.numeric(test_metrics[[2]])
    }

    results[i, ] <- list(
      grid$units1[i], grid$units2[i],
      grid$drop1[i], grid$drop2[i],
      as.numeric(val_acc), test_loss, test_acc
    )
    histories[[i]] <- history
  }

  results <- results[order(-results$val_accuracy), ]
  print(results)

  rm(W_)
  gc()
  return(results)
}



```

```{r RNN_window_search}




#
# #Window = 3 
# 
# rnn_window_3 <- keras_search_rnn(window = 3)
# save(rnn_window_3, file =  "rnn_window_3")
# #Window  = 7
# rnn_window_7 <- keras_search_rnn(window = 7)
# save(rnn_window_7, file =  "rnn_window_7")
# #Window = 30
# rnn_window_30 <- keras_search_rnn(window = 30)
# save(rnn_window_30, file =  "rnn_window_30")
# #Window = 120
# rnn_window_120 <- keras_search_rnn(window = 120)
# save(rnn_window_120, file =  "rnn_window_120")


load("rnn_window_3")
load( "rnn_window_7")
load("rnn_window_30")
load("rnn_window_120")

rnn_window_3 <- rnn_window_3 %>% 
  mutate(model_no = row_number(),
         window  = 3)

rnn_window_7 <- rnn_window_7 %>% 
  mutate(model_no = row_number(),
         window  = 7)

rnn_window_30 <- rnn_window_30 %>% 
  mutate(model_no = row_number(),
         window  = 30)

rnn_window_120 <- rnn_window_120%>% 
  mutate(model_no = row_number(),
         window  = 120)

results_rnn_search <- rbind(rnn_window_3, rnn_window_30, rnn_window_7,  rnn_window_120)
ggplot(data = results_rnn_search, aes( x  = model_no, y  = val_accuracy, colour = as.factor(window)))+
  geom_point()+
  labs(x = "Model Number (arranged by  validation accuracy)")
#max(results_rnn_search$test_accuracy)

```
The window search for the recurrent neural network involved more competitive performances. In this case, a window of 30 days appears more robust overall, containing fewer worse performing models towards the extreme end. We therefore, conducted an exapnded grid search using that window, the results of which are presented in Figure ? below. The hypermarameters of for grid search is presented in the below table.
<br>
<br>


```{r}



grid_summary <- data.frame(
  Hyperparameter = c("units1", "units2", "drop1", "drop2"),
  Values = c(
    paste(c(16, 32, 64), collapse = ", "),
    paste(c(16, 32, 64), collapse = ", "),
    paste(c(0.3, 0.5, 0.7), collapse = ", "),
    paste(c(0.3, 0.5, 0.7), collapse = ", ")
  ),
  stringsAsFactors = FALSE
)



kable(grid_summary, format = "html",
      digits = 4,
      caption = "RNN: grid search parameters") %>%
  kable_styling(
    bootstrap_options = c("hover"),
    full_width = FALSE
  ) 


```






```{r}

library(keras3)

W_ <- pre_data(w = 30, h = 1) #using 30 days

y.test <- W_[[1]]
y.train <- W_[[2]]
X.train <- W_[[3]]
X.test <- W_[[4]]
cl_weights <- W_[[5]]
F_ <- W_[[6]]
# 
# set.seed(1)
# tf$keras$utils$set_random_seed(123L)


# grid <- expand.grid(
#   units1 = c(16,32, 64),   # LSTM units in layer 1
#   units2 = c(16, 32, 64),   # LSTM units in layer 2
#   drop1  = c(0.3, 0.5, 0.7),
#   drop2  = c(0.3, 0.5, 0.7),
#   stringsAsFactors = FALSE
# )
# 
# results <- data.frame(
#   units1 = integer(), units2 = integer(),
#   drop1 = double(), drop2 = double(),
#   val_accuracy = double(),
#   test_loss = double(), test_accuracy = double(),
#   stringsAsFactors = FALSE
# )
# 
# histories <- vector("list", nrow(grid))
# 
# for (i in seq_len(nrow(grid))) {
#   cat(sprintf("\n[%d/%d] u1=%d, u2=%d, d1=%.2f, d2=%.2f\n",
#               i, nrow(grid),
#               grid$units1[i], grid$units2[i],
#               grid$drop1[i], grid$drop2[i]))
# 
#   tensorflow::tf$keras$backend$clear_session()
#   set.seed(1)
#   tf$keras$utils$set_random_seed(123L)
# 
#   input <- layer_input(shape = c(dim(X.train)[2], F_)) # (timesteps, features)
# 
#   output <- input %>%
#     layer_lstm(units = grid$units1[i], return_sequences = TRUE) %>%
#     layer_dropout(rate = grid$drop1[i]) %>%
#     layer_lstm(units = grid$units2[i], return_sequences = FALSE) %>%
#     layer_dropout(rate = grid$drop2[i]) %>%
#     layer_dense(units = 6, activation = "softmax")
# 
#   model <- keras_model(inputs = input, outputs = output)
# 
#   model %>% compile(
#     optimizer = "adam",
#     loss = "categorical_crossentropy",
#     metrics = "accuracy"
#   )
# 
#   history <- model %>% fit(
#     X.train, y.train,
#     epochs = 60,
#     batch_size = 64,
#     verbose = 0,
#     validation_split = 0.2,
#     shuffle = FALSE
#   )
# 
#   # val acc from last epoch
#   val_acc <- tail(history$metrics$val_accuracy, 1)
#   if (length(val_acc) == 0) val_acc <- tail(history$metrics$val_acc, 1)
# 
#   test_metrics <- model %>% evaluate(X.test, y.test, verbose = 0)
#   test_loss <- as.numeric(test_metrics[[1]])
#   test_acc  <- if (!is.null(names(test_metrics)) && "accuracy" %in% names(test_metrics)) {
#     as.numeric(test_metrics[["accuracy"]])
#   } else {
#     as.numeric(test_metrics[[2]])
#   }
# 
#   results[i, ] <- list(
#     grid$units1[i], grid$units2[i],
#     grid$drop1[i], grid$drop2[i],
#     as.numeric(val_acc), test_loss, test_acc
#   )
#   histories[[i]] <- history
# }
# 
# results <- results[order(-results$val_accuracy), ]
# 
# rnn_final_search <- results
# 
# save(rnn_final_search, file = "rnn_final_search")
  
load("rnn_final_search")

```


```{r}



res <- rnn_final_search %>%
  mutate(
    run_id = row_number(),
    label  = sprintf("u1=%d u2=%d d1=%.1f d2=%.1f",
                     units1, units2, drop1, drop2)
  ) %>%
  arrange(desc(val_accuracy)) %>%
  mutate(rank = row_number(),
         top5 = rank <= 5)

library(scales)


top_n <- 30
ggplot(res %>% slice(1:top_n),
       aes(x = reorder(label, val_accuracy), y = val_accuracy, fill = top5)) +
  geom_col() +
  coord_flip() +
  geom_text(aes(label = percent(val_accuracy, accuracy = 0.1)),
            hjust = -0.1, size = 3) +
  scale_y_continuous(labels = percent_format(accuracy = 1), expand = expansion(mult = c(0, .1))) +
  scale_fill_manual(values = c("lightblue", "#1f78b4"), guide = "none") +
  labs(title = sprintf("Figure ?: Top %d configs by validation accuracy", top_n),
       x = "Config", y = "Val accuracy")

x1 <- round(max(results_rnn_search$val_accuracy)*100)

```
Out of the more expanded grid search, the best performing model achieved a validation accuracy of roughly 64%. We fitted this model to the full training data and used that to predict onto the test set. Overall, we were satisfied with this search as the validation accuracy had not improved drastically compared to the search across various windows done above. For example, the best performing model in the first stage of search achieved an accuracy of roughly `r x1`%. 

```{r final_rnn_search}


#rnn_final_search[1,]
#fitting final rnn mdoel = 64, 16 0.7.  0.5



    input <- layer_input(shape = c(dim(X.train)[2], F_)) # (timesteps, features)

    output <- input %>%
      layer_lstm(units = 64, return_sequences = TRUE) %>%
      layer_dropout(rate = 0.7) %>%
      layer_lstm(units = 16, return_sequences = FALSE) %>%
      layer_dropout(rate = 0.5) %>%
      layer_dense(units = 5, activation = "softmax")

    final_rnn_model <- keras_model(inputs = input, outputs = output)

    final_rnn_model %>% compile(
      optimizer = "adam",
      loss = "categorical_crossentropy",
      metrics = "accuracy"
    )



  # history <- final_rnn_model() %>% fit(
  #   X.train, y.train,
  #   epochs = 60,
  #   batch_size = 64,
  #   verbose = 0,
  #   validation_split = 0.2,
  #   shuffle = FALSE
  #   # = cl_weights
  # )

  # true_labels <- apply(y.test, 1, max)
pred_rnn  <- predict(final_rnn_model, X.test)



pred_rnn <- max.col(pred_rnn)



save(pred_rnn,  file = "pred_rnn")

load("pred_rnn")

#pred_rnn
```

## Modelling

```{r libs-and-seed}
# Load libraries for data wrangling, visualization, preprocessing, and modeling
library(dplyr)
library(tidyverse)
library(ggplot2)
library(janitor)
library(caTools)
library(caret)
library(e1071)
library(readr)
library(tidyverse)
library(janitor)
library(lubridate)
library(skimr)
library(naniar)
library(GGally)
library(corrplot)
library(factoextra)
library(cluster)
library(rsample)
library(recipes)
library(scales)
library(cowplot)
library(pROC)
library(reshape2)

# Load the required libraries so their functions are available for use.
library(xgboost)   # Core API for training and using XGBoost models.
library(caret)     # High-level interface for model training, tuning, and evaluation.
library(pROC)      # Provides functions for ROC curve plotting and AUC computation.
library(pdp)       # Used to generate Partial Dependence Plots for model interpretation.

# Set the random seed for reproducibility.
# Ensures that random processes like resampling and model training are consistent across runs.
set.seed(123)
```

Feed Forward Neural network

A feed-forward multilayer perceptron (MLP) neural network was developed for multi-class classification using the H2O deep learning framework. The objective was to predict a five level categorical outcome based on a high-dimensional feature set.


The MLP architecture was systematically optimized through a randomized discrete grid search to identify the most effective combination of hyperparameters. All candidate models were structured with a consistent input and output layer, while the hidden layer configuration was varied. The Input Layer Comprised 61 nodes, corresponding to the full set of hot one encoded engineered predictor variables, plus the bias. 
<br>
<br>

The grid search explored architectures containing between two and four hidden layers. Neuron configurations tested included dense layers such as [256, 128, 64], [512, 256, 128], and [128, 64]. The RectifierWithDropout activation function was initially considered to promote non-linearity and mitigate overfitting. The output layerContained five neurons, and employed a softmax activation function to generate a probability distribution over the classes.

<br>
<br>

To counteract the effects of an imbalanced class distribution in the training data, a class-balancing strategy was implemented. Each training observation was assigned a weight inversely proportional to its class frequency, thereby increasing the influence of minority classes during the optimization of the loss function.

<br>
<br>
```{r}
n1 <- 4*3*3*2*3
```


The randomized grid search evaluated a maximum of `r n1` candidate models, exploring a predefined hyperparameter space. Key hyperparameters and their search ranges are detailed in Table ? below.


```{r}


summary_df <- data.frame(
  Hyperparameter = c("hidden",
                     "activation",
                     "l1",
                     "l2",
                     "rate",
                     "input_dropout_ratio"),
  Values = c("(128,128); (128,64); (64,32,16); (64,32)",
             "RectifierWithDropout",
             "0, 1e-4, 1e-5",
             "0, 1e-4, 1e-5",
             "0.01, 0.001",
             "0.1, 0.3, 0.5"),
  stringsAsFactors = FALSE
)

summary_df


kable(summary_df, format = "html",
      digits = 4,
      caption = "Feed Forward Neural Network Grid Search Hyperparameters") %>%
  kable_styling(
    bootstrap_options = c("hover"),
    full_width = FALSE
  )




tbl_df <- summary_df %>%
  mutate(Values = gsub("; ", "<br>", Values))

kable(tbl_df, format = "html", escape = FALSE,
      caption = "Hyperparameter Grid") %>%
  kable_styling(
    bootstrap_options = c( "hover", "condensed"),
    full_width = FALSE, position = "left", font_size = 13
  ) %>%
  row_spec(0, color = "white") %>%   # header style
  column_spec(1, bold = TRUE, width = "22%") %>%             # Hyperparameter column
  column_spec(2, width = "60%", monospace = TRUE)

```


Each model was trained for a maximum of 200 epochs. An early stopping mechanism was employed to prevent overfitting, which terminated training if the validation set logloss did not improve by a tolerance of at least 1e-4 over 10 consecutive scoring intervals.

```{r h2o-initialise}
## ---------------------------
## H2O Deep Learning (Neural Network) workflow
## Built from existing objects: `train`, `test`, `predictor_cols`, `target`, `levels_y`, `num_class`
## This script uses the data preparation already done above and trains an H2O neural net,
## evaluates on the test set and shows how to tune hyperparameters with H2O Grid Search.
## ---------------------------


target <- "OAH"
predictor_cols <- setdiff(names(data), target) #Matt edit, replaced df with data, to be consistent as above


# Load h2o and initialize --------------------------------------------------
# Note: if h2o isn't installed, run: install.packages("h2o")
library(h2o)
# Start (or connect to) an H2O cluster. This creates an in-memory instance used for training.
# nthreads = -1 will use all available cores.
#h2o.init(nthreads = -1)

```

```{r turn-to-h2o-format}
# Convert existing R data.frames to H2O frames --------------------------------
# We use the `train` and `test` data.frames already prepared earlier in the pipeline.
# H2O works with its own frame objects; as.h2o() converts R data.frames into H2O frames.
# train_h2o <- as.h2o(train) #Matt Comment: now this uses my test and train made before modeling
# test_h2o  <- as.h2o(test)
```

```{r feature-modification-h2o}
# Define features (x) and response (y) -------------------------------------
# Using the same predictor list used for XGBoost so we reuse feature engineering already performed.
# x_vars <- predictor_cols  # character vector of predictor column names
# y_var  <- target          # the response column name ("oah")
# 
# 
# # Ensure response is treated as a categorical (factor) in H2O for classification
# train_h2o[[y_var]] <- as.factor(train_h2o[[y_var]])
# test_h2o[[y_var]]  <- as.factor(test_h2o[[y_var]])
```

```{r neural-net-weights}

#Matt Edit: This is causing NA's when I run it. I dont necessarily think we have to weight rare classes more, but weight things equally, so for now I am setting balance classes = T in H2o.

# # ---------------------------
# # H2O neural network with class weighting + randomized grid search
# # ---------------------------
# # This block expects the following objects already exist:
# #  - train (R data.frame), test (R data.frame)
# #  - predictor_cols (character vector), target (string), levels_y (factor levels), num_class (int)
# #  - h2o cluster started (h2o.init() called earlier)
# # If train_h2o/test_h2o are not present, the script will create them from train/test.
# 
# 
# # 1) Create a class-weight column (inverse frequency) to help under-represented classes -
# #    weight_i = N / (K * count(class_i))  -> standard balanced weight
# freq_table <- as.data.frame(table(train[[target]]))
# names(freq_table) <- c("class", "count")
# N_total <- nrow(train)
# K <- num_class
# 
# # map class -> weight
# class_counts <- setNames(freq_table$count, freq_table$class)
# class_weights_map <- N_total / (K * class_counts)   # higher weight for rarer classes
# 
# # create a vector of weights for each row in train, matching the order of 'train'
# weights_vector <- as.numeric(class_weights_map[as.character(train[[target]])])
# 
# # attach weights column into train H2O frame (column name: "class_weights")
# train_h2o[,"class_weights"] <- as.h2o(weights_vector)
# 
# # For reproducibility, add the weights to the R 'train' too (keeps them in sync)
# train$class_weights <- weights_vector
```

```{r neural-net-weighted-data-train-test-split}
# Split training H2O frame into train/validation for early stopping ---------
# We'll keep a validation frame to enable early stopping during H2O training.
# Use an 80/20 split of the existing training data.
#    We keep a validation frame so early stopping works.
# set.seed(123)
# 
# splits <- h2o.splitFrame(train_h2o, ratios = 0.8, seed = 123)
# 
# train_h2o_split <- splits[[1]]
# valid_h2o_split <- splits[[2]]
```

```{r neural-net-hyper-params}
# Define hyperparameter search space (RandomDiscrete)
#    Note: large search space with many models will run long; adjust max_models for runtime.
hyper_params <- list(
  hidden = list(
    c(128,128),
    c(128,64),
    c(64,32,16),
    c(64,32)),
  activation =  c("RectifierWithDropout"),
  l1 = c(0, 1e-4, 1e-5),
  l2 = c(0, 1e-4,1e-5),
  rate = c(0.01, 0.001),                # learning rates (smaller = slower but better generalization)
  input_dropout_ratio = c(0.1, 0.3, 0.5))     # apply some input dropout regularization
```

```{r neural-net-search-criteria}
# Because hidden_dropout_ratios must match hidden layer length, we will set a default dropout
# and not include it in the hyperparameter space (keeps models from failing due to mismatched lengths)

search_criteria <- list(
  strategy = "RandomDiscrete",
  max_models = 30,    # change to 40-60 if you want a longer run (4-5 hours) — careful with RAM
  seed = 123,
  max_runtime_secs = 60*60*4)  # optional: cap runtime to ~4 hours (comment out if you prefer model count)

```

```{r h2o-grid-search}
# Launch grid search using class weights (weights_column) ---------------------
#    Note: epochs increased to allow deeper training; early stopping will limit wasted epochs.


# grid_id <- "h2o_nn_balanced_grid"
# 
# grid <- h2o.grid(algorithm = "deeplearning",
#                  grid_id = grid_id,
#                  x = predictor_cols,
#                  y = target,
#                  training_frame = train_h2o_split,
#                  validation_frame = valid_h2o_split,
#                  # Use the weights column computed earlier:
#                  # weights_column = "class_weights",
#                  # # Make H2O use classes weighting; setting balance_classes=FALSE because we supply weights explicitly
#                  balance_classes = T,
#                  # Training control
#                  epochs = 200,                       # allow many epochs (early stopping will truncate)
#                  stopping_metric = "logloss",
#                  stopping_tolerance = 1e-4,
#                  stopping_rounds = 10,
#                  # optimization / regularization defaults; some are overridden by hyperparams
#                  distribution = "multinomial",
#                  #activation = "RectifierWithDropout",
#                  adaptive_rate = TRUE,
#                  # small-ish default l1/l2 (grid will explore others)
#                  l1 = 0, l2 = 0,
#                  # Use relatively small mini-batch default (H2O autodetects)
#                  reproducible = T,   # reproducible=TRUE may slow things on multi-core machines
#                  seed = 123,
#                  # hyperparameter space and search criteria:
#                  hyper_params = hyper_params,
#                  search_criteria = search_criteria)
# 
# # Review the grid search summary (sorted by validation logloss) ---------------
# grid_perf <- h2o.getGrid(grid_id = grid_id, sort_by = "logloss", decreasing = FALSE)
# save(grid_perf, file = "h2o_grid_perf")
# 
# 
# 
# 
# # Best model ID (first row in sorted grid)
# best_model_id <- as.character(grid_perf@model_ids[[1]])
# best_model    <- h2o.getModel(best_model_id)
# 
# 
# 
# dir.create("models", showWarnings = FALSE)
# 
# # Save the model into that folder
# saved_path <- h2o.saveModel(
#   object = best_model,
#   path   = "models",   # relative path, works on every machine
#   force  = TRUE
# )
# 
# cat("Model saved at:", saved_path, "\n")
# 
# 
# 
# # Find the first saved model folder inside "models/"
# model_dirs <- list.dirs("models", recursive = FALSE)



# best_model <-  h2o.loadModel("models/h2o_nn_balanced_grid_model_3") #loading h2o model
# load("h2o_grid_perf")
```

```{r get-best-model}
# # Pull the best model (lowest validation logloss) ----------------------------
# best_model_id <- grid_perf@model_ids[[1]]
# best_model <- h2o.getModel(best_model_id)
# 
# cat("Best model id:", best_model_id, "\n")
# print(best_model)
# 
# h2o.performance(best_model)
# 
# save(best_model, file = "FFN_final_model")

load("FFN_final_model")



#plot of performance

#perf.df <- as.data.frame(grid_perf@summary_table) 



# ggplot(data = perf.df, aes(x = hidden, y = logloss, colour = as.factor(input_dropout_ratio))) + 
#   geom_point(size = 2)+
#   labs(y = "logloss")
# 
# ggplot(data = perf.df, aes(x = hidden, y = logloss, colour = as.factor(l1))) + 
#   geom_point(size = 2)+
#   labs(y = "logloss")
# 
# ggplot(data = perf.df, aes(x = hidden, y = logloss, colour = as.factor(l2))) + 
#   geom_point(size = 2)+
#   labs(y = "logloss")


#h2o.performance(best_model)



# 
# ggplot(perf.df, aes(x = rate, y = logloss, color = factor(input_dropout_ratio))) +
#   geom_point() +
#   facet_wrap(~ hidden) +
#   theme_minimal() +
#   labs(title = "Learning Rate vs Logloss by Architecture & Dropout")


```

```{r best-model-eval}

# levels_y <- levels(train$OAH)
# # Evaluate the best model on the held-out test set ---------------------------
# 
# best_pred_h2o <- h2o.predict(best_model, test_h2o)   # returns 'predict' + per-class probabilities
# best_pred_df <- as.data.frame(best_pred_h2o)

# # predicted class labels (as factor)
# pred_labels_best <- factor(best_pred_df$predict)
# true_labels_best <- factor(as.vector(test_h2o[[target]]))
# 
# # confusion matrix & caret metrics
# cm_best <- confusionMatrix(pred_labels_best, true_labels_best)  #adding the  levels
# print(cm_best)
# cat("Test Accuracy (best H2O NN):", round(cm_best$overall['Accuracy'],4), "\n")
# 
# cm_best$overall
# mean(pred_labels_best==true_labels_best)
```

```{r per-class-auc}
# NEEDS DEBUGGIN!


# Compute per-class AUC (one-vs-all) using predicted probabilities ------------
# probability columns in best_pred_df must match levels_y order; H2O uses level names as columns
# prob_matrix <- as.matrix(best_pred_df[, levels_y])
# 
# auc_list <- sapply(seq_along(levels_y), function(i) {
#   cls <- levels_y[i]
#   bin_true <- as.numeric(true_labels_best == cls)
#   scores <- prob_matrix[, cls]
#   if (length(unique(bin_true)) < 2) return(NA)   # if a class not present in test
#   auc_val <- as.numeric(roc(bin_true, scores)$auc)
#   return(round(auc_val,4))
# })
# 
# names(auc_list) <- levels_y
# 
# print(auc_list)
```

```{r top-three-predicted-classes}
# NEEDS DEBUGGIN!

# # Top-K (e.g. Top-3) hit ratio - useful in practice -------------------------
# # calculate if true label is in top-3 predicted probabilities
# topk <- function(prob_row, k = 3) {
#   ord <- order(prob_row, decreasing = TRUE)
#   return(names(prob_row)[ord[1:k]])
# }
# 
# probs_df <- as.data.frame(prob_matrix)
# 
# in_topk <- mapply(function(i, true_lbl) {
#   topk_preds <- topk(as.numeric(probs_df[i, ]), k = 3)
#   true_lbl %in% topk_preds
# }, i = seq_len(nrow(probs_df)), true_lbl = as.character(true_labels_best))
# 
# 
# top3_hit_ratio <- mean(in_topk, na.rm = TRUE)
# 
# cat("Top-3 hit ratio (best model):", round(top3_hit_ratio,4), "\n")
```

```{r save-best-model}
# Save the best model to disk  -----------------------------------
# model_path <- h2o.saveModel(best_model, path = getwd(), force = TRUE)
# cat("Saved best model to:", model_path, "\n")

#------------------------------------ Notes  -----------------------------------------------------------
# - If you encounter OOM / memory errors: reduce `max_models` or reduce hidden layer sizes
#   or set max_runtime_secs in search_criteria to limit the run-time.
# - Because we used per-row weights, the model actively penalizes mistakes on rare classes.
# - If you want even stronger balancing, you can set balance_classes = TRUE as well,
#   or apply targeted over-sampling (SMOTE) on the R side before converting to H2O.
# - For an ensemble: keep a few of the top grid models and use h2o.stackedEnsemble() to combine them.
```

```{r best-model-metrics}
# ---------- Robust metrics, top-k, AUCs, calibration ----------

#Get predicted DF and true labels (from best_pred_df / test_h2o)
# If we don't have best_pred_df, compute it:
# best_pred_h2o <- h2o.predict(best_model, test_h2o)
# best_pred_df <- as.data.frame(best_pred_h2o)
# 
# pred_df <- best_pred_df   # data.frame containing "predict" and per-class probability columns named by levels_y
# true_labels <- factor(as.vector(test_h2o[[target]]))
# 
# 
# library(caret)
# cm_best <- confusionMatrix(table(pred_labels_best, true_labels_best))


# # Extract only probability columns (exclude "predict")
# prob_cols <- pred_df[, setdiff(colnames(pred_df), "predict")]
# 
# # Confusion matrix (caret)
# pred_factor <- factor(pred_df$predict)
# cm <- confusionMatrix(pred_factor, true_labels)
# print(cm)   # this is safe
```

```{r neural-net-per-class-metrices}
# # Per-class precision, recall, F1, macro-F1 (safe numeric conversions)
# byClass <- as.data.frame(cm$byClass)
# 
# # caret's byClass could have columns like 'Sensitivity','Specificity','Pos Pred Value' etc.
# # Ensure numeric columns are numeric:
# num_cols <- sapply(byClass, is.numeric)
# if (!all(num_cols)) {
#   # coerce columns that look like numbers but are factors/characters
#   for (cname in names(byClass)) {
#     byClass[[cname]] <- suppressWarnings(as.numeric(as.character(byClass[[cname]])))
#   }
# }
# 
# precision <- byClass$`Pos Pred Value`       # precision
# recall    <- byClass$Sensitivity            # recall
# f1 <- 2 * (precision * recall) / (precision + recall)
# f1[is.na(f1)] <- 0
# metrics_tbl <- data.frame(
#   class = rownames(byClass),
#   precision = round(precision,4),
#   recall = round(recall,4),
#   F1 = round(f1,4))
# 
# print(metrics_tbl)
# cat("Macro F1:", round(mean(f1, na.rm = TRUE),4), "\n")
```

```{r top-hits}
# # Top-K hit ratios (Top-1, Top-2, Top-3)
# topk_hit <- function(probs_df, true_labels, k=3) {
#   n <- nrow(probs_df)
#   hits <- logical(n)
#   for (i in seq_len(n)) {
#     row_probs <- as.numeric(probs_df[i, ])
#     names(row_probs) <- colnames(probs_df)
#     topk_names <- names(sort(row_probs, decreasing = TRUE))[1:k]
#     hits[i] <- as.character(true_labels[i]) %in% topk_names
#   }
#   mean(hits, na.rm = TRUE)
# }
# 
# cat("Top-1 hit:", round(topk_hit(prob_cols, true_labels, 1),4), "\n")
# cat("Top-2 hit:", round(topk_hit(prob_cols, true_labels, 2),4), "\n")
# cat("Top-3 hit:", round(topk_hit(prob_cols, true_labels, 3),4), "\n")
```

```{r calibration-plot-perclass}
# Calibration plot per class (reliability diagram) - example for one class
# calibration_plot_one <- function(class_label, prob_df, true_labels, n_bins = 10) {
#   scores <- prob_df[[class_label]]
#   true_bin <- as.numeric(true_labels == class_label)
#   df_cal <- data.frame(scores = scores, true = true_bin)
#   df_cal$bin <- cut(df_cal$scores, breaks = seq(0,1,length.out = n_bins+1), include.lowest = TRUE)
#   agg <- aggregate(cbind(mean_prob = scores, obs_rate = true) ~ bin, data = df_cal, FUN = mean)
#   agg$bin_mid <- (seq(0,1,length.out = n_bins+1)[-1] + seq(0,1,length.out = n_bins+1)[- (n_bins+1)])/2
#   ggplot(agg, aes(x = mean_prob, y = obs_rate)) +
#     geom_point() + geom_line() + geom_abline(intercept = 0, slope = 1, linetype = "dashed", color = "gray") +
#     labs(title = paste("Calibration for", class_label), x = "Mean predicted probability", y = "Observed frequency")
# }
# 
# # Example: plot calibration for 'Moderate' (change to any class)
# if ("Moderate" %in% colnames(prob_cols)) {
#   print(calibration_plot_one("Moderate", prob_cols, true_labels, n_bins = 10))
# }
```

```{r neural-net-save-metrics}
# Save metrics to file 
#write.csv(metrics_tbl, "h2o_nn_per_class_metrics.csv", row.names = FALSE)
```

```{r}

# imp_data <- as.data.frame(h2o.varimp(best_model)) %>% 
#    mutate(variable = reorder(variable, relative_importance),
#           rank = row_number()) %>% 
#   filter(rank <= 30)
# 
# save(imp_data, file = "imp_data")


load("imp_data")
#h2o.varimp_plot(best_model, num_of_features = 50)

ggplot(data = imp_data, aes(x = relative_importance, y = variable, fill = relative_importance))+
  geom_col()+
  labs(title = "Figure ?: Variable Importance for Feed Forward Neural Network",
       fill = "Relative Importance",
       x = "Relative Importance")
```

## Results

```{r sanana_work}

#Hi Sanana ! :)

#Here  are all the  model  and prediction objects


#Below are some workings on the prediction objects. Skip past to the below banner that says HERE



#pred_ffn_labels <- best_pred_df[,"predict"]
# pred_cnn
# pred_rnn

 true_labels <- test$OAH
# pred_ffn_labels <- factor(pred_ffn_labels, levels = levels(true_labels))
# save(pred_ffn_labels, file = "pred_ffn_labels")
# save(true_labels, file = "true_labels")

load("pred_ffn_labels")
load("true_labels")


# Map integers back to labels

load("pred_cnn")
load("pred_rnn")
pred_cnn_labels <- factor(levels(true_labels)[pred_cnn],
                   levels = levels(true_labels))



pred_rnn_labels <- factor(levels(true_labels)[pred_rnn + 1],
                   levels = levels(true_labels))




# temporal_labels <- max.col(y.test)
# 
# temporal_labels <- factor(levels(true_labels)[temporal_labels],
#                    levels = levels(true_labels))
# 
# save(temporal_labels, file = "temporal_labels")

load("temporal_labels")

#---------------------------------------------------HERE-------------------------------------------------------

#pred_rnn_labels <---- predictions for RNN loaded above
#pred_cnn_labels <---- predictions for cnn loaded above
#pred_ffn_labels <---- predictions for ffn loaded above


mean(pred_rnn_labels == temporal_labels)
#temporal_labels <----- test set labels for cnn and rnn
#true_labels <----------- test set for ffn (randomly shuffled)




```

## Discussion

```{r, echo = FALSE}
# Getting FFN in order
pred_ffn_labels <- best_pred_df[,"predict"]

true_labels <- test$OAH
pred_ffn_labels <- factor(pred_ffn_labels, levels = levels(true_labels))
ffn_cm <- confusionMatrix(pred_ffn_labels, true_labels)

# ----------------------------------------------------------
# Getting CNN and RNN in order
pred_cnn_labels
pred_rnn_labels

# Test set labels for cnn and rnn
temporal_labels

# Checking
class(pred_cnn_labels)
class(pred_rnn_labels)
class(temporal_labels)

length(pred_cnn_labels) # Why are you longer :/
length(pred_rnn_labels)
length(temporal_labels)

# "Fixing" CNN preds
pred_cnn_labels <- pred_cnn_labels[1:length(temporal_labels)]

# Create the confusion matrices
cnn_cm <- confusionMatrix(pred_cnn_labels, temporal_labels)
rnn_cm <- confusionMatrix(pred_rnn_labels, temporal_labels)

```

```{r}
library(gridExtra)

plot_cm <- function(cm, title) {
  cm_table <- as.data.frame(cm$table)
  ggplot(cm_table, aes(x = Reference, y = Prediction, fill = Freq)) +
    geom_tile() +
    geom_text(aes(label = Freq), color = "coral") +
    scale_fill_gradient(low = "white", high = "steelblue") +
    labs(title = title, x = "Actual", y = "Predicted") +
    theme_minimal()
}

p1 <- plot_cm(ffn_cm, "Feed-Forward NN Confusion Matrix")
p2 <- plot_cm(cnn_cm, "CNN Confusion Matrix")
p3 <- plot_cm(rnn_cm, "RNN Confusion Matrix")

grid.arrange(p1, p2, p3, nrow = 3)
```

```{r}
# Statistics by class
kable(as.data.frame(ffn_cm$byClass), caption = "FFNN Statistics by Class") %>% kable_styling()
kable(as.data.frame(cnn_cm$byClass), caption = "CNN Statistics by Class") %>% kable_styling()
kable(as.data.frame(rnn_cm$byClass), caption = "RNN Statistics by Class") %>% kable_styling()
```

```{r}
# Accuracy table summary
accuracy_tbl <- data.frame(
  Model = c("FFNN", "CNN", "RNN"),
  Accuracy = paste0(
    round(c(
      ffn_cm$overall["Accuracy"],
      cnn_cm$overall["Accuracy"],
      rnn_cm$overall["Accuracy"]
    ) * 100, 1), "%")
)

kable(accuracy_tbl, caption = "Overall Accuracy by Model") %>%
  kable_styling(full_width = FALSE)
```
