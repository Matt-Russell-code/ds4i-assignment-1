---
title: "DS4I - Project Writeup"
format: html
execute:
  echo: false
  warning: false
  message: false
  results: hide
editor: visual
---

```{r setup}


#libraries 

library(kableExtra)
library(tidyverse)
library(ggplot2)
library(ggpubr)

#theme for ggplot2, let me know if you do not like this theme and we can change it :)

#Theme for ggplot2 
theme_set(
  theme_bw(base_size = 9) +
    theme(
      plot.title   = element_text(hjust = 0.5, size = 9),
      panel.border = element_rect(colour = "black", fill = NA, linewidth = 0.5),
      # keep grids but make them subtle
      panel.grid.major = element_line(colour = "grey85", linewidth = 0.3),
      panel.grid.minor = element_line(colour = "grey92", linewidth = 0.2),
      # base-R style text/ticks
      axis.text  = element_text(colour = "black"),
      axis.title = element_text(colour = "black"),
      axis.ticks = element_line(colour = "black", linewidth = 0.3)
    )
)



#making geompoint have hollow circles
update_geom_defaults(
  "point",
  list(size = 1, alpha = 0.9, shape = 21, colour = "black")
)


```

```{r}
#| echo: false
knitr::include_graphics("av.jpg")

```

[Photo by Nicolas Cool on Unsplash]{style="font-size:12px"}

## Introduction

## Literature Review

## Exploratory Data Analysis

```{r}



```

## Feature Engineering

```{r read_data}

library(lubridate)
data = read.csv("scotland_avalanche_forecasts_2009_2025.csv")


```

```{r dates}

data$Date <- as.POSIXct(data$Date, format="%Y-%m-%d %H:%M:%S", tz="UTC")


data$month     <- month(data$Date)
data$dayofyear <- yday(data$Date)
data$hour      <- hour(data$Date)


#Cyclical enocoding


data$month_sin <- sin(2 * pi * data$month / 12)
data$month_cos <- cos(2 * pi * data$month / 12)


data$hour_sin <- sin(2 * pi * data$hour / 24)
data$hour_cos <- cos(2 * pi * data$hour / 24)


data$yday_sin <- sin(2 * pi * data$dayofyear / 366)
data$yday_cos <- cos(2 * pi * data$dayofyear / 366) #leap years

```

```{r}
train.idx <- sample(1:nrow(data), 
                    round(nrow(data)*0.8))

train <- data[train.idx,] #Training data
test <- data[-train.idx,] #Testing Data


```

```{r missing_val}

#Getting table of missing values


col.n <- colnames(data)
n <- nrow(data)

missing_df <- data.frame(Variable = character(),
                         "Missing (%)" = numeric(),
                         check.names = F)


for (i in col.n){
  
  x <- sum(is.na(data[,i]))
  
  perc <- (x/n) * 100
  
  missing_df <- rbind(missing_df,
                      data.frame(
                        Variable = i,
                        `Missing (%)` = perc
                      , check.names = F)
                      )
  
}
 
missing_df <- missing_df %>% 
  filter(`Missing (%)` > 0) %>% 
  arrange(desc(`Missing (%)`),)


```

```{r tablex}



kable(missing_df, 
      format = "html",
      digits = 2,
      caption = "Missing Values") %>%
  kable_styling(
    bootstrap_options = c("hover"),
    full_width = FALSE
  ) %>%
  footnote(general = "")


```

```{r check}


find_missing <- function(a){
missing_df <- data.frame(Variable = character(),
                         "Missing (%)" = numeric(),
                         check.names = F)


for (i in col.n){
  
  x <- sum(is.na(a[,i]))
  
  perc <- (x/n) * 100
  
  missing_df <- rbind(missing_df,
                      data.frame(
                        Variable = i,
                        `Missing (%)` = perc
                      , check.names = F)
                      )
  
}
 
missing_df <- missing_df %>% 
  filter(`Missing (%)` > 0) %>% 
  arrange(desc(`Missing (%)`),)

return(missing_df)

}



#Counting the number of NA's

#in terms of imputation, we can try use cell mean for all of the above
count_na <- 0
for (i in 1:nrow(data)){
  if (any(is.na(data[i, ]))){count_na <- count_na +1}
}


count_na/nrow(data)



#ideas 

#time series transformation
#geographic categorical variable
groupings <- c()
for (i in col.n){
  
  if(is.character(data[,i])){ groupings <- c(groupings, i)}
}

groupings

# We cannot group by our target OAH, or date


```

```{r imputation}

#Basic

#I am not grouping by Date, OsGrid, Obs or OAH (As thats our target variable)
groupings <- groupings[-c(1, 3, 5, 7)]

#Cell mean imputation

#any(is.na(train.imp[, -12]))


train.imp <- train %>%
  group_by(Area, FAH) %>% #cant group by precip code, or location, too many missing
  mutate(
    across(where(is.integer),
           ~ replace_na(., as.integer(round(mean(., na.rm = TRUE))))),
    across(where(is.double),
           ~ replace_na(., mean(., na.rm = TRUE)))
  ) %>% #after this, some variables 
  ungroup() %>% 
  mutate(Aspect = round(mean(Aspect, na.rm = T),), #remaining, just taking the mean
         Max.Temp.Grad = round(mean(Max.Temp.Grad, na.rm = T)),
         Max.Hardness.Grad = round(mean(Max.Hardness.Grad, na.rm  = T))) 

train.imp <- train.imp[-which(is.na(train.imp$OAH)),]
  
unique(train.imp$OAH)
#debug

#eijdoias


#trying same method with testing data


test.imp <- test %>%
  group_by(Area, FAH) %>% #cant group by precip code, or location, too many missing
  mutate(
    across(where(is.integer),
           ~ replace_na(., as.integer(round(mean(., na.rm = TRUE))))),
    across(where(is.double),
           ~ replace_na(., mean(., na.rm = TRUE)))
  ) %>% #after this, some variables 
  ungroup() %>% 
  mutate(Aspect = round(mean(Aspect, na.rm = T),), #remaining, just taking the mean
         Max.Temp.Grad = round(mean(Max.Temp.Grad, na.rm = T)),
         Max.Hardness.Grad = round(mean(Max.Hardness.Grad, na.rm  = T))) 

test.imp <- train.imp[-which(is.na(test.imp$OAH)),]




```

```         
```

```{r}



 

```
=============================
## Modelling
=============================

```{r}
# Install missing packages automatically (uncomment if needed)
required <- c("tidyverse","janitor","lubridate","skimr","naniar","GGally",
              "corrplot","factoextra","cluster","factoextra","rsample","recipes","scales")
missing <- required[!(required %in% installed.packages()[,"Package"])]
if(length(missing)) install.packages(missing, repos = "https://cloud.r-project.org")

library(tidyverse)   # data wrangling + ggplot
library(janitor)     # clean names
library(lubridate)   # dates
library(skimr)       # fast dataset summary
library(naniar)      # missingness visual
library(GGally)      # ggpairs
library(corrplot)    # correlation plot
library(factoextra)  # PCA & cluster visual
library(cluster)     # clustering tools
library(rsample)     # train/test splits
library(recipes)     # preprocessing pipeline
library(scales)      # axis formats
```

## [Load data and initial cleaning]{.underline}

```{r}
# 1. Read with readr (fast). Then clean column names to snake_case.

df_raw <- read_csv("scotland_avalanche_forecasts_2009_2025.csv")

# Clean col names
df <- df_raw %>% janitor::clean_names() # lowercase + underscores

# Parse date/time robustly. If time exists use ymd_hms, else try ymd.
df <- df %>%
  mutate(date = parse_date_time(date, orders = c("ymd HMS", "ymd HM"), tz = "Europe/London"))

# Quick check
glimpse(df$date)

# Quick structure check
head(df)

```

## Full skim (single call summary)

```{r}
# Thorough summary for initial understanding
#?skim
skimr::skim(df)
vis_miss(df) # Missingness
```

## Univariate numeric exploration (histograms, outliers)

```{r}
# Select numeric columns for plotting
num_vars <- df %>% select(where(is.numeric)) %>% names()
length(num_vars)

# If many numeric variables use facetted histograms (log scale option for skewed)
plot_df <- df %>% select(all_of(num_vars)) %>% pivot_longer(everything(), names_to = "var", values_to = "val")

# Plot: histograms by variable (facetted)
ggplot(plot_df, aes(x = val)) +
  geom_histogram(bins = 40, na.rm = TRUE) +
  facet_wrap(~var, scales = "free", ncol = 4) +
  labs(title = "Univariate distributions (numeric vars)") +
  theme_minimal()
```

## Univariate categorical exploration (FAH, OAH, Area, Obs)

```{r}
# Ensure key categorical variables are factors
df <- df %>%
  mutate(fah = as.factor(fah),
         oah = as.factor(oah),
         area = as.factor(area),
         obs = as.factor(obs))

# Frequency plots
p1 <- ggplot(df, aes(x = fah)) + geom_bar() + labs(title = "FAH (forecasted hazard) counts") + theme_minimal()
p2 <- ggplot(df, aes(x = oah)) + geom_bar() + labs(title = "OAH (observed hazard) counts") + theme_minimal()
p3 <- ggplot(df, aes(x = area)) + geom_bar() + labs(title = "Records per Area") + theme_minimal() + theme(axis.text.x = element_text(angle = 45, hjust = 1))

cowplot::plot_grid(p1, p2, p3, ncol = 1)
# p1
# p2
# p3

```

## Bivariate: FAH vs OAH contingency and simple agreement

```{r}
# Contingency table
tab <- table(df$fah, df$oah)
tab

# Convert to proportions by FAH (how forecasts translate to observed)
prop.table(tab, margin = 1)

# Simple agreement metric (raw)
agree_pct <- sum(diag(prop.table(tab))) # diagonal proportion
paste0("Raw agreement FAH==OAH: ", scales::percent(agree_pct))

```

## Bivariate numeric vs target (OAH): boxplots + statistical tests

```{r}
# Numeric variables vs oah (boxplots) - select a reasonable subset
num_for_target <- num_vars[!num_vars %in% c("longitude","latitude")] # drop raw coordinates from this view
num_for_target <- head(num_for_target, 12) # plot top 12 numeric for readability

plot_df2 <- df %>% select(oah, all_of(num_for_target)) %>% pivot_longer(-oah, names_to = "var", values_to = "val")

ggplot(plot_df2, aes(x = oah, y = val)) +
  geom_boxplot(outlier.size = 0.5, na.rm = TRUE) +
  facet_wrap(~var, scales = "free_y", ncol = 3) +
  labs(title = "Numeric variables by observed hazard (OAH)") +
  theme_minimal()

```

```{r}
# For each numeric variable compute a Kruskal-Wallis test against ordered OAH (non-parametric)
oah_ordered <- df %>% filter(!is.na(oah)) %>% mutate(oah_ord = as.numeric(factor(oah, ordered = TRUE)))
kruskal_results <- map_dfr(num_vars, function(var){
  dat <- df %>% select(all_of(var), oah) %>% filter(!is.na(oah) & !is.na(.data[[var]]))
  if(nrow(dat) < 50) return(tibble(variable = var, p_value = NA_real_, n = nrow(dat)))
  test <- kruskal.test(dat[[var]] ~ dat$oah)
  tibble(variable = var, p_value = test$p.value, n = nrow(dat))
}) %>% arrange(p_value)

kruskal_results %>% head(20)

```

## Correlation matrix (numeric variables) + variable selection guidance

```{r}
nums <- df %>% select(where(is.numeric))
# Keep variables with at least some non-missing values
nums <- nums %>% select(where(~sum(!is.na(.)) > (0.05 * nrow(df))))
cor_mat <- cor(stats::na.omit(nums), use = "pairwise.complete.obs")
corrplot::corrplot(cor_mat, method = "color", tl.cex = 0.7, number.cex = 0.7)

```

```{r}
# Identify pairs with |r| > 0.85 to consider dropping or combining
high_cor_pairs <- which(abs(cor_mat) > 0.85 & abs(cor_mat) < 1, arr.ind = TRUE)
if(nrow(high_cor_pairs) > 0) {
  apply(high_cor_pairs, 1, function(idx){
    cat(rownames(cor_mat)[idx[1]], "<->", colnames(cor_mat)[idx[2]], ":", cor_mat[idx[1], idx[2]], "\n")
  })
} else cat("No very high correlations (>0.85) found among numeric subset.\n")

```

## PCA on scaled numeric predictors (exploratory only)

```{r}
# Use numeric variables with reasonable completeness
nums_for_pca <- nums %>% select(where(~sum(!is.na(.)) > 0.6 * nrow(df)))
pca_df <- nums_for_pca %>% drop_na()  # PCA here uses complete cases; record row loss
cat("Rows used for PCA:", nrow(pca_df), "of", nrow(df), "\n")

pca_res <- prcomp(pca_df, center = TRUE, scale. = TRUE)
fviz_eig(pca_res, addlabels = TRUE, ncp = 15) + ggtitle("PCA: Scree plot")

# Show loadings for first two PCs
loadings <- as_tibble(pca_res$rotation[,1:4], rownames = "variable")
loadings %>% arrange(desc(abs(PC1))) %>% slice(1:10)

biplot(pca_res)

```

## Spatial quick-check (longitude / latitude vs OAH)

```{r}
if(all(c("longitude","latitude") %in% names(df))) {
  df %>% filter(!is.na(longitude) & !is.na(latitude) & !is.na(oah)) %>%
    ggplot(aes(x = longitude, y = latitude, color = oah)) +
    geom_point(alpha = 0.6, size = 1) +
    labs(title = "Spatial scatter: observations colored by OAH", x = "Longitude", y = "Latitude") +
    theme_minimal()
} else {
  cat("Longitude/Latitude not available or have been removed.")
}
```

## Time-based checks (seasonality / trends)

```{r}
# Ensure date exists
df %>% filter(!is.na(date)) %>%
  mutate(ym = lubridate::floor_date(date, "month")) %>%
  group_by(ym, oah) %>%
  summarise(n = n(), .groups = "drop") %>%
  ggplot(aes(x = ym, y = n, color = oah)) +
  geom_line() +
  labs(title = "Monthly counts by OAH over time", x = "Month", y = "Count") +
  theme_minimal()

```

## Quick predictor importance proxy (Spearman w/ OAH treated ordinal)

```{r}
# Convert OAH to ordinal numeric (careful: mapping preserved from factor order)
oah_levels <- levels(df$oah)
cat("OAH levels:", paste(oah_levels, collapse = ", "), "\n")
df_imp <- df %>% mutate(oah_ord = as.numeric(factor(oah, ordered = TRUE)))

importance_tbl <- map_dfr(num_vars, function(var){
  dat <- df_imp %>% select(all_of(var), oah_ord) %>% filter(!is.na(.data[[var]]) & !is.na(oah_ord))
  if(nrow(dat) < 50) return(tibble(variable = var, spearman = NA_real_, p = NA_real_, n = nrow(dat)))
  test <- cor.test(dat[[var]], dat$oah_ord, method = "spearman", exact = FALSE)
  tibble(variable = var, spearman = test$estimate, p = test$p.value, n = nrow(dat))
}) %>% arrange(desc(abs(spearman)))

importance_tbl %>% slice(1:20)

```






=======================
## Discussion
