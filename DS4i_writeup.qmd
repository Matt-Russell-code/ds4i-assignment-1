---
title: "DS4I - Project Writeup"
format:
  html:
    css: style.css
execute:
  echo: FALSE
  warning: FALSE
  message: FALSE
  results: hide
editor: visual
---

```{r setup}


#libraries 

library(kableExtra)
library(tidyverse)
library(ggplot2)
library(ggpubr)
library(lubridate)
library(caret)

#Theme for ggplot2 
theme_set(
  theme_bw(base_size = 9) +
    theme(
      plot.title   = element_text(hjust = 0.5, size = 9),
      panel.border = element_rect(colour = "black", fill = NA, linewidth = 0.5),
      # keep grids but make them subtle
      panel.grid.major = element_line(colour = "grey85", linewidth = 0.3),
      panel.grid.minor = element_line(colour = "grey92", linewidth = 0.2),
      # base-R style text/ticks
      axis.text  = element_text(colour = "black"),
      axis.title = element_text(colour = "black"),
      axis.ticks = element_line(colour = "black", linewidth = 0.3)
    )
)



#making geompoint have hollow circles
update_geom_defaults(
  "point",
  list(size = 1, alpha = 0.9, shape = 21, colour = "black")
)


data = read.csv("scotland_avalanche_forecasts_2009_2025.csv")


```

```{r}
#| echo: FALSE
knitr::include_graphics("av.jpg")

```

[Photo by Nicolas Cool on Unsplash]{style="font-size:12px"}

## Introduction

## Literature Review

Snow avalanches are a recognised hazard in snow-prone mountain regions worldwide. Unlike in the Alps, avalanches in Scotland rarely impact settlements and infrastructure, but pose significant risks to recreational users in the Scottish Highlands (Bain, 2024; Diggins, 2009; Ward, 1980; WSL Institute for Snow and Avalanche Research SLF \[WSL\], 2016). The rising popularity of winter activities such as climbing, walking, and skiing has increased exposure to avalanche-prone terrain, resulting in injuries and occasional fatalities each year (Scottish Avalanche Information Service \[SAIS\], 2024; Webster, 2020, WSL, 2016). These risks prompted studies of Scottish snowpack and forecasting strategies beginning in the 1970s (Langmuir, 1970; Spink, 1970; Beattie, 1976; Ward, 1980; Ward et al., 1985). However, early work was largely descriptive, relying on snow-pit measurements and expert judgement to link snow properties and weather to avalanche risk. Building on this, Ward (1984a and 1984b) applied one of the first predictive avalanche forecasting models for Scotland.

Avalanche modelling is inherently complex, where predictions depending on multiple interacting factors (Choubin et al., 2019; Helbig et al., 2015; Hendrick et al., 2023; Pozdnoukhov et al., 2008; Singh & Ganju, 2008). This challenge is heightened in regions with highly variable weather (Sharma & Ganju, 1999). Early statistical methods, such as nearest neighbours, supported forecasters by relating current weather to past avalanche events, however, were prone to overfitting and struggles with high-dimensional data (Blagovechshenskiy et al., 2023; Kala et al., 2025; Pozdnoukhov et al., 2018; Singh & Ganju, 2008). Using a dataset from Lochaber, Scotland, Pozdnoukhov et al. (2008) reported that support vector machines (SVMs) performed similarly to nearest neighbours while handling high-dimensional data more effectively and producing richer forecasts. SVMs for avalanche forecasting have also proven successful in Iran, India, Switzerland, and Tibet (Choubin et al., 2019; Rahmati et al., 2019; Schirmer et al., 2009; Tiwari et al., 2021; Wen et al., 2022). 

In recent years, more complex machine learning (ML) algorithms such as regression trees, random forests, and neural networks have demonstrated strong predictive performance in avalanche forecasting (Blagovechshenskiy et al., 2023; Choubin et al., 2019; Gauthier et al., 2025; Hendrick et al., 2023; Rahmati et al., 2019; Singh & Ganju, 2018; Tiwari et al., 2021; Wen et al., 2022). Avalanche datasets are often high-dimensional, and using forecaster-led feature selection can be highly subjective and variable (Helbig et al., 2015; Pozdnoukhov et al., 2008). Hybrid approaches, which combine ML tools (e.g., variable importance plots) with forecaster-led feature selection have shown practical value (Gauthier et al., 2025). However, fully data-driven forecasting methods, particularly neural networks (NNs), offer important advantages.

NNs simulate brain processes through interconnected nodes, capturing complex, nonlinear relationships (Blagovechshenskiy et al., 2023; Sharma et al., 2023; Tu, 1996). By processing all inputs simultaneously, they learn interactions and patterns without prior knowledge of which variables matter most (Blagovechshenskiy et al., 2023; Fromm & Schonberger, 2022; Sharma et al., 2023). Applications in Switzerland, Kazakhstan, and India show that NNs not only perform strongly but can also assess the relative importance of variables to avalanche risk (Fromm & Schonberger; Sharma et al., 2023; Singh & Ganju, 2008). This makes them particularly well suited for high-dimensional avalanche datasets and situations with limited domain knowledge or uncertain predictors.

Despite advances in data-driven avalanche forecasting worldwide, research specific to Scotland remains limited in three ways:

\(1\) Few published studies exist (only 9 relevant peer-reviewed papers exist on Scopus, with the most recent study conducted in 2011 (keywords: ( "avalanche forecasting" OR "avalanche prediction" ) AND Scotland))

\(2\) The restricted range of statistical approaches explored (only KNN and SVM) (Heierli et al., 2004; Pozdnoukhov et al., 2008; Pozdnoukhov et al., 2011; Purves et al., 2003)

\(3\) The focus on only a few regions in Scotland (primarily the Cairngorms and Lochaber regions)

This limited interest likely stems from the highly localised nature of avalanche risk in Scotland, which generally occurs in remote areas and affects relatively few people (Diggins, 2009; SAIS, 2024). Nevertheless, improved avalanche forecasting is increasingly important under changing climate conditions, which may alter snowpack properties and avalanche risk (Gauthier et al., 2025; Werritty & Sugden, 2013). A detailed discussion of climate change, however, is beyond this paper's scope.

The present study aims to extend forecasting research in Scotland both spatially and methodologically by applying three NNs (Feedforward, Convolutional, and Recurrent) trained on combined data from the following six regions of Scotland: Creah Meagaidh, Glencoe, Lochaber, Northern Cairngorms, Southern Cairngorms, and Torridon.

\<insert paragraph explaining modelling approaches of prev papers + this project’s modelling approaches. I.e. the neural net architecture\>

Hypothesis???? We can workshop together

## Context and Dataset

Scotland has a mild, wet, temperate maritime climate influenced by the North Atlantic Ocean and persistent south-westerly winds (Pozdnoukhov et al., 2008; Scottish Government, 2011). These conditions produce rapid temperature changes, frequent heavy precipitation (snow or rain), and strong winds. With its highest peak, Ben Nevis (1345 m), Scotland's mountains are low compared to alpine ranges (\~4000 m), resulting in a snowpack that is typically shallower, wetter, and more variable (Britannica, 2025; Pozdnoukhov et al., 2008; WSL, 2016). Rapid freeze-thaw cycles, rain-on-snow, and wind redistribution further destabilise snow, creating avalanche risks that can appear and disappear within a short time (Diggins, 2009; Pozdnoukhov et al., 2008; Purves et al., 2003; Scottish Government, 2011). Snowpack conditions also differ regionally; the West is strongly maritime, with mild and wet winters, while the North and East are colder and drier (MetOffice, 2010). Scotland’s climate is thus highly variable spatially and temporally, adding to the complexity of avalanche risk prediction.

This paper uses a 15-year archive of avalanche forecasts from Scotland across the six areas of Creah Meagaidh, Glencoe, Lochaber, Northern Cairngorms, Southern Cairngorms, and Torridon, produced by the SAIS (2025). @fig-map-obs illustrates the spatial distribution of observations, with points marking individual events and labels indicating area locations. Total observations by area and hazard category (@tbl-area-hazard) reflect differences in observation frequency rather than absolute avalanche activity. Cairngorms (Northern and Southern) and Lochaber, located in the colder and drier North and East, have the most recorded observations, whereas Torridon in the maritime West has substantially fewer. "Low" and "Moderate" hazard levels dominate across all areas, while "High" hazard events are rare and concentrated in Northern Cairngorms and Lochaber.

```{r}
#| echo: FALSE
#| label: fig-map-obs
#| fig-cap: "Observed avalanche hazard (OAH): Scotland."


library(dplyr)
library(ggplot2)
library(ggrepel)
library(maps)

# palette 
hazard_cols <- c(
  "Low"            = "#d4f0f0",
  "Moderate"       = "#91bfdb",
  "Considerable -" = "#fc8d59",
  "Considerable +" = "#d73027",
  "High"           = "#7f0000"
)
hazard_levels <- names(hazard_cols)
data <- data %>% mutate(OAH = factor(as.character(OAH), levels = hazard_levels, ordered = TRUE))

# label positions 
area_labels <- data %>%
  filter(!is.na(longitude), !is.na(latitude), !is.na(Area)) %>%
  group_by(Area) %>%
  summarise(lon = median(longitude, na.rm = TRUE),
            lat = median(latitude,  na.rm = TRUE),
            .groups = "drop")

# tight bounding box from points
d_pts <- data %>% filter(!is.na(longitude), !is.na(latitude), !is.na(OAH))
pad_x <- 0.25; pad_y <- 0.20
xlim_use <- range(d_pts$longitude, na.rm = TRUE) + c(-pad_x, pad_x)
ylim_use <- range(d_pts$latitude,  na.rm = TRUE) + c(-pad_y, pad_y)

# plot
map_OAH <- ggplot() +
  borders("world", regions = "UK", fill = "grey92", colour = "grey70") +
  geom_point(
    data = d_pts,
    aes(x = longitude, y = latitude, color = OAH, fill = OAH),
    size = 1.8, alpha = 0.85
  ) +
  ggrepel::geom_text_repel(
    data = area_labels,
    aes(x = lon, y = lat, label = Area),
    size = 3, seed = 941,
    box.padding = 0.15, point.padding = 0.1,
    min.segment.length = 0, max.overlaps = 20
  ) +
  scale_color_manual(values = hazard_cols, drop = FALSE, name = "OAH") +
  scale_fill_manual(values = hazard_cols, drop = FALSE, name = "OAH") +
  coord_quickmap(xlim = xlim_use, ylim = ylim_use, expand = FALSE) +
  labs(
    x = NULL, y = NULL
  ) +
  theme_minimal(base_size = 12) +
  theme(
    panel.grid.major = element_line(color = "grey95"),
    legend.position  = "right",
    plot.title       = element_text(face = "bold")
  )

final_map <- annotate_figure(map_OAH,
                bottom = text_grob("Note: Point observations coloured by hazard level; SAIS areas labelled.",
                                   hjust = 0, x = 0,
                                   size = 7, color = "black"))
final_map
```

<br>

```{r}
#| echo: FALSE
#| label: tbl-area-hazard
#| tbl-cap: "Overall total observations by area and hazard category."


library(dplyr)
library(tidyr)
library(knitr)

area_hazard_table <- data %>%
  group_by(Area) %>%
  filter(!is.na(Area), !is.na(OAH)) %>%
  count(Area, OAH) %>% #count all observations per area per avalanche category
  pivot_wider(
    names_from = OAH, #hazard levels as columns
    values_from = n,
    values_fill = 0
  ) %>%

  rowwise() %>% #total overall instead of across seasons
  mutate(Total = sum(c_across(all_of(c("Low", "Moderate", "Considerable -", "Considerable +", "High"))))) %>%
  ungroup()


kable(area_hazard_table)
```

<br>

Data cover the Scottish winters (November to April) from the December 2009 to March 2025. Predictor variables are categorised into three main groups, including (1) position and topography, (2) weather, and (3) snowpack test, all collected at the forecast location. Each observation also includes the date and the forecast area (metadata). For an overview of the predictors available, see @tbl-pred below.

```{r}
#| echo: FALSE
#| label: tbl-pred
#| tbl-cap: "Overview of predictor variables available in the avalanche forecast dataset."

pred_table <- data.frame(Predictor_Group = c("Metadata", "Position and Topography", "Weather", "Snowpack Test"), Variables = c("date, area", "longitude, latitude, altitude, aspect of slope, incline of slope", "air temperature, wind direction, wind speed, cloud cover, precipitation code, snowdrift, total snow depth, foot penetration, ski penetration, rain observed at 900m elevation, summit air temperature, summit wind direction, summit wind speed", "max. temperature gradient, max. hardness gradient, no.settle, snow.index, insolation, crystals, wetness, AV.Cat, and snow temperature"))

kable(pred_table)


```

## Exploratory Data Analysis

### a. Feature Map Correlations

```{r}
#| echo: FALSE


library(dplyr)
library(ggplot2)

dat <- data
dat <- dplyr::ungroup(dat)  # in case it's a grouped tibble

#ONLY CODE FOR CORRELATION PLOT

#  Keep only numeric columns, drop seasonal values
num_df <- dat %>%
  select(where(is.numeric)) %>%
  select(
    -any_of(c("fah_i", "oah_i", "season_id")),   # <- explicitly drop season_id
    -matches("^(dayofseason|season(_year|_d)?|seasonality|month|year|hour)$", ignore.case = TRUE)
  )

#  must have at least 2 numeric vars
if (ncol(num_df) < 2L) stop("Not enough numeric columns for correlation analysis.")

#  Drop zero-variance columns
ok_cols <- vapply(num_df, function(x) sd(x, na.rm = TRUE) > 0, logical(1))
num_df  <- num_df[, ok_cols, drop = FALSE]

#  Correlation matrix (Pearson; pairwise complete)
corr_mat <- cor(num_df, use = "pairwise.complete.obs", method = "pearson")

# Long form 
corr_long <- as.data.frame(as.table(corr_mat)) %>%
  rename(var1 = Var1, var2 = Var2, r = Freq) %>%
  filter(as.character(var1) != as.character(var2)) %>%
  mutate(abs_r = abs(r))

# Heatmap — colours only
p_corr <- ggplot(as.data.frame(as.table(corr_mat)),
                 aes(Var1, Var2, fill = Freq)) +
  geom_tile() +
  scale_fill_gradient2(limits = c(-1, 1)) +
  labs(
       x = NULL, y = NULL, fill = "r") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1),
        panel.grid = element_blank())

```

```{r}
#| echo: FALSE
#| include: FALSE

#ONLY CODE FOR BOXPLOTS

# Table for top pairs of highly correlated variables
library(dplyr)

top_pairs <- corr_long %>%
  rowwise() %>%
  mutate(pair = paste(sort(c(var1, var2)), collapse = "_")) %>%
  ungroup() %>%
  distinct(pair, .keep_all = TRUE) %>%
  arrange(desc(abs_r)) %>%
  slice(1:5) %>%
  select(var1, var2, r, abs_r)

if (knitr::is_html_output()) {
 
  DT::datatable(
    top_pairs,
    caption = htmltools::tags$caption(
      style = 'caption-side: top; text-align: left;',
      'Most highly correlated variable pairs (numeric variables only)'
    ),
    options = list(pageLength = 10, order = list(list(4, 'desc'))),
    rownames = FALSE,
    class = 'compact stripe hover'
  ) |>
    DT::formatRound(c('r', 'abs_r'), 3)
} else {
  # Static table that shows NOW in the editor preview
  knitr::kable(
    top_pairs,
    caption = "Most highly correlated variable pairs (numeric variables only)",
    digits = 3,
    align = c('l','l','r','r')
  )
}


```

The Scottish avalanche dataset contains several features that are moderately to strongly correlated, while others appear largely independent (@fig-feat-cor). From a modelling perspective, highly correlated features can increase complexity and obscure unique contributors, however, can also capture the multifactorial nature of avalanche risk. In the context of NNs, the presence of correlated variables is not inherently problematic, since the model can learn non-linear interactions. However, strong correlations can increase the risk of overfitting if the network memorises patterns rather than generalising to new data.

The highest correlation found was between *summit air temperature* and *air temperature* (r = `r round(top_pairs[1,4], 2)`), indicating strong redundancy. Nevertheless, both variables were retained as each likely capture a different aspect relevant to forecasting (i.e., summit conditions versus broader conditions). *Air temperature* is an identified key driver of avalanche conditions and is expected to be an important predictor in the NNs (Fromm & Schonberger, 2022; Gauthier et al., 2025; Pozdnoukhouv et al., 2018; Souckova et al., 2022; Ward,1980). Exploration of *air temperature* across *observed avalanche hazard (OAH)* levels further supports its importance in forecasting (@fig-box-OAH). Higher hazard categories are generally associated with lower observed *air temperatures*, with “Considerable -”, “Considerable +”, and “High” hazards centred below 0 °C. “Low” hazard days show medians above freezing and a wider variability. The transition at “Moderate” hazard, where temperatures cluster around 0 °C, reflects conditions that can either stabilise or destabilise the snowpack through freeze-thaw cycles. These patterns highlight Scotland's sensitivity to rapid temperature shifts and their impact on avalanche activity (Diggins, 2009; Pozdnoukhov et al., 2008).

Other correlations with 0.5\<\|r\|\< 0.80 are moderately strong but remain informative and manageable, as NNs can accommodate interdependent inputs.

```{r}
#| echo: FALSE
#| label: fig-feat-cor
#| fig-cap: "Map of feature correlations."

p_corr
```

<br>

```{r}
#| echo: FALSE
#| label: fig-box-OAH
#| fig-cap: "Distribution of air temperature across observed avalanche hazard (OAH)."


# Plot 1: Boxplot of Air.Temp by OAH (Figure 5)
data %>%
  filter(!is.na(Air.Temp), !is.na(OAH)) %>%
  ggplot(aes(x = OAH, y = Air.Temp, fill = OAH)) +
  geom_boxplot(outlier.alpha = 0.3) +
  scale_fill_manual(values = hazard_cols, drop = FALSE, guide = "none") +
  labs(
    x = "Observed Avalanche Hazard (OAH)",
    y = "Air Temperature (°C)"
  ) +
  theme(
    plot.title = element_text(size = 12, face = "bold") # smaller than before
  )

```

### b. Precipitation Type

```{r}

#| echo: FALSE

library(dplyr)
library(ggplot2)
library(scales)
library(stringr)
library(forcats)

df <- dplyr::ungroup(data)  

#  Columns
precip_col <- names(df)[str_detect(tolower(names(df)), "precip")] %>% .[1]
stopifnot(length(precip_col) == 1)
oah_col <- names(df)[tolower(names(df)) == "oah"] %>% .[1]
stopifnot(length(oah_col) == 1)

# Hazard levels + palette
hazard_levels <- c("Low","Moderate","Considerable -","Considerable +","High")
df[[oah_col]] <- factor(df[[oah_col]], levels = hazard_levels, ordered = TRUE)
hazard_cols <- c(
  "Low"            = "#d4f0f0",
  "Moderate"       = "#91bfdb",
  "Considerable -" = "#fc8d59",
  "Considerable +" = "#d73027",
  "High"           = "#7f0000"
)

#Summarise counts/proportions, drop NA in precip
oah_precip <- df %>%
  filter(!is.na(.data[[precip_col]]), !is.na(.data[[oah_col]])) %>%
  count(precip = .data[[precip_col]], oah = .data[[oah_col]], name = "n") %>%
  group_by(precip) %>%
  mutate(prop = n / sum(n)) %>%
  ungroup()

#  Extract leading numbers to identify bins; 
oah_precip <- oah_precip %>%
  mutate(bin_num = suppressWarnings(as.numeric(str_extract(as.character(precip), "^-?\\d+")))) %>%
  filter(!is.na(bin_num), bin_num <= 10)

# Order precipitation bins numerically
lvl_order <- oah_precip %>%
  distinct(precip, bin_num) %>%
  arrange(bin_num, precip) %>%
  pull(precip) %>%
  unique()

oah_precip <- oah_precip %>%
  mutate(precip = factor(precip, levels = lvl_order, ordered = TRUE))


```

```{r}
#| echo: FALSE

# cumulative prop for highest hazard level
heavy_snow <- oah_precip %>%
  filter(bin_num == 10)

cumulative_high <- heavy_snow %>%
  filter(oah %in% c("Considerable -", "Considerable +", "High")) %>%
  summarise(cum_prop = sum(prop)) %>%
  pull(cum_prop)
```

The distribution of *OAH* levels varies clearly across different precipitation types (@fig-precip-bar). "Low" hazard predominates under conditions of none (0) or trace (2) precipitation, whereas higher hazard categories become increasingly prevalent as snowfall intensity increases. Heavy snow (level 10) is associated with a substantially larger share of “Considerable” and “High” hazard ratings (`r round(cumulative_high*100, 2)`%). This aligns with existing knowledge of the Scottish climate, where rapid weather shifts and frequent rain or snow-on-snow events contribute to unstable snowpack conditions (Diggins, 2009; Pozdnoukhov et al., 2008; Purves et al., 2003; Ward, 1980).

```{r}
#| echo: FALSE
#| label: fig-precip-bar
#| fig-cap: "Precipitation type vs observed avalanche hazard (OAH)."

#  Plot
ggplot(oah_precip, aes(x = precip, y = prop, fill = oah)) +
  geom_col(position = "fill", width = 0.9) +
  geom_text(aes(label = percent(prop, accuracy = 1)),
            position = position_fill(vjust = 0.5),
            size = 3, colour = "white", fontface = "bold") +
  scale_y_continuous(labels = percent_format(accuracy = 1)) +
  scale_fill_manual(values = hazard_cols, drop = FALSE) +
  labs(
    x = "Precipitation type/level",
    y = "Proportion within precipitation type",
    fill = "OAH"
  ) +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 30, hjust = 1))
```

### c. Seasonality

```{r}

#| echo: FALSE

library(dplyr)
library(ggplot2)
library(patchwork)


hazard_cols <- c(
  "Low"            = "#d4f0f0",
  "Moderate"       = "#91bfdb",
  "Considerable -" = "#fc8d59",
  "Considerable +" = "#d73027",
  "High"           = "#7f0000"
)

# Ensure OAH ordered Low -> High
data <- data %>%
  mutate(
    OAH = factor(OAH,
                 levels = c("Low","Moderate","Considerable -","Considerable +","High"))
  )

# Define season year: 

data <- data %>%
  mutate(
    month_num = lubridate::month(Date),
    season_year = if_else(month_num >= 11, lubridate::year(Date), lubridate::year(Date) - 1),
    month_in_season = factor(
      month_num,
      levels = c(11, 12, 1, 2, 3, 4, 5),
      labels = c("Nov","Dec","Jan","Feb","Mar","Apr","May")
    )
  )

# Aggregate counts and proportions per month (across all years)
oah_monthly <- data %>%
  filter(!is.na(OAH), month_in_season %in% levels(month_in_season)) %>%
  count(month_in_season, OAH, name = "n") %>%
  group_by(month_in_season) %>%
  mutate(month_total = sum(n),
         prop = n / month_total) %>%
  ungroup()

# A helper frame for the counts-only series
month_totals <- oah_monthly %>%
  distinct(month_in_season, month_total)


# Top: proportions
p_prop <- ggplot(oah_monthly, aes(x = month_in_season, y = prop, fill = OAH)) +
  geom_col(position = "fill", width = 0.85) +
  scale_fill_manual(values = hazard_cols, drop = FALSE) +
  labs(x = NULL, y = "Proportion of forecasts", fill = "OAH") +
  theme_minimal(base_size = 12) +
  theme(
    legend.position = "right",
    plot.margin = margin(t = 5, r = 10, b = 20, l = 10)   # extra space below
  )

# Bottom: counts 
max_n <- max(month_totals$month_total, na.rm = TRUE)
p_counts <- ggplot(month_totals, aes(x = month_in_season, y = month_total)) +
  geom_col(width = 0.85, fill = "grey70") +
  geom_text(aes(label = month_total), vjust = -0.25, size = 3.5) +
  scale_y_continuous(limits = c(0, max_n * 1.18), expand = expansion(mult = c(0, 0.02))) +
  labs(x = "Month (Nov–May)", y = "Number of forecasts") +
  theme_minimal(base_size = 12) +
  theme(
    plot.margin = margin(t = 22, r = 10, b = 5, l = 10)    # extra space above
  )


```

Monthly patterns of *OAH* across the extended avalanche season (November to May) reveal clear seasonal trends (@fig-season-bar-hist). Early (November-December) and late (April-May) months are dominated by "Low" and "Moderate" hazards, while mid-season months (Jan-Mar) contain a higher proportion of considerable and "High" hazards. Most forecasts occur between December and March, highlighting the period of greatest avalanche risk (SAIS, 2024). These patterns reflect the cyclical nature of snowpack development, with mid-winter weather instability driving higher-risk avalanches (Diggins, 2009; Podzdnoukhov et al., 2008; Purves et al., 2003). For modelling, this seasonality indicates that hazard levels are unevenly distributed, however, NNs can accommodate complex, changing relationships between weather conditions and other important variables.

```{r}
#| echo: FALSE
#| label: fig-season-bar-hist
#| fig-cap: "Seasonality of observed avalanche hazard (OAH): Nov–May."

# Combine with a title 
(p_prop / p_counts) +
  plot_layout(heights = c(1, 1)) +
  plot_annotation(
    theme = theme(
      plot.title = element_text(size = 15, face = "bold"),
      plot.subtitle = element_text(size = 11)
    )
  )
```

### d. FAH vs OAH

```{r OAH-FAH-plot-code}
#| echo: FALSE

library(dplyr)
library(tidyr)
library(ggplot2)
library(scales)
library(stringr)

# helpers 
find_col <- function(df, patterns) {
  nm <- names(df)
  hits <- sapply(patterns, function(p) nm[grepl(p, nm, ignore.case = TRUE)], simplify = FALSE)
  hit <- unique(unlist(hits))
  if (length(hit)) hit[1] else NULL
}

parse_datetime_safe <- function(x) {
  if (inherits(x, "POSIXct") || inherits(x, "Date")) return(as.POSIXct(x))
  if (!is.character(x)) x <- as.character(x)
  as.POSIXct(x, tz = "UTC",
             tryFormats = c("%Y-%m-%d %H:%M:%S",
                            "%Y-%m-%d %H:%M",
                            "%Y-%m-%d",
                            "%d/%m/%Y %H:%M:%S",
                            "%d/%m/%Y %H:%M",
                            "%d/%m/%Y",
                            "%m/%d/%Y %H:%M:%S",
                            "%m/%d/%Y %H:%M",
                            "%m/%d/%Y"))
}

#  hazard levels + normaliser
hazard_levels <- c("Low","Moderate","Considerable -","Considerable +","High")

norm_hazard <- function(x){
  x0 <- str_squish(tolower(as.character(x)))
  case_when(
    x0 == "low" ~ "Low",
    x0 == "moderate" ~ "Moderate",
    x0 %in% c("considerable -","considerable−","considerable–","considerable minus") ~ "Considerable -",
    x0 %in% c("considerable +","considerable plus")                                   ~ "Considerable +",
    x0 == "high" ~ "High",
    TRUE ~ NA_character_
  )
}

# ensure FAH/OAH columns exist 
if (!"fah" %in% names(dat)) {
  FAH_col <- find_col(dat, c("^FAH$", "forecast.*hazard"))
  if (!is.null(FAH_col)) dat <- mutate(dat, fah = .data[[FAH_col]])
}
if (!"oah" %in% names(dat)) {
  OAH_col <- find_col(dat, c("^OAH$", "observed.*hazard"))
  if (!is.null(OAH_col)) dat <- mutate(dat, oah = .data[[OAH_col]])
}

#  find Date and Area 
date_col <- find_col(dat, c("^date$", "forecast.*date", "datetime", "time"))
area_col <- find_col(dat, c("^area$", "region", "zone"))

if (is.null(date_col)) stop("No date-like column found. Expected something like 'Date', 'date', or 'datetime'.")
dat <- dat %>% mutate(.Date = parse_datetime_safe(.data[[date_col]]))
if (all(is.na(dat$.Date))) stop("Could not parse dates in the detected date column.")

#  next-day alignment: -
if (!is.null(area_col)) {
  dat_shifted <- dat %>%
    mutate(.Area = .data[[area_col]]) %>%
    arrange(.Area, .Date) %>%
    group_by(.Area) %>%
    mutate(oah_nextday_raw = lead(oah, 1)) %>%
    ungroup()
} else {
  dat_shifted <- dat %>%
    arrange(.Date) %>%
    mutate(oah_nextday_raw = lead(oah, 1))
}

# tidy and normalise labels 
df <- dat_shifted %>%
  transmute(
    FAH = factor(norm_hazard(fah), levels = hazard_levels),
    OAH_next = factor(norm_hazard(oah_nextday_raw), levels = hazard_levels)
  ) %>%
  drop_na(FAH, OAH_next)

# contingency + proportions (each FAH column = 100%) 
tab_counts <- with(df, table(OAH_next, FAH))
tab_prop   <- prop.table(tab_counts, 2)


# Figure 6 
df_prop <- as.data.frame(tab_prop) %>%
  rename(p = Freq, OAH = OAH_next) %>%
  mutate(on_diag = as.character(OAH) == as.character(FAH))

```

Lastly, @fig-FAH-OAH shows the relationship between *forecast avalanche hazard (FAH)* on day *t* and *OAH* on day *t+1*. Each column sums to 100%, showing how outcomes were distributed given a specific forecast level. Forecasts of "Low" hazard were highly accurate, with `r round(tab_prop[1,1]*100, 2)`% of cases confirmed the following day. This indicates that forecasters were most successful in predicting stable snowpack conditions and low hazard scenarios.

For intermediate levels such as “Moderate” and “Considerable –” the agreement weakens. Only about half of “Moderate” forecasts matched the following day’s observations, with a substantial spillover into “Low” and “Considerable –”. “Considerable –” forecasts split almost evenly between being observed as “Moderate,” “Considerable –,” and “Considerable +.” This pattern shows that there is a challenge in differentiating between neighbouring hazard categories, which may highlight the subjectivity and sensitivity of these intermediate hazard thresholds.

Forecasts of “High” hazard showed weaker reliability. Fewer than one-quarter of these forecasts corresponded to an observed “High” hazard the next day. Most “High” forecasts aligned with “Considerable –” or “Considerable +” outcomes. This shows that there is a tendency towards over-prediction of extreme conditions, or difficulty in anticipating when weather and snowpack instabilities will escalate into truly high-risk scenarios.

```{r OAH-FAH-plot}
#| echo: FALSE
#| label: fig-FAH-OAH
#| fig-cap: "FAH (day t) → OAH (day t+1) proportions."


fah_oah_corrplot <- ggplot(df_prop, aes(x = FAH, y = OAH, fill = p)) +
  geom_tile(color = "white") +
  geom_tile(data = dplyr::filter(df_prop, on_diag),
            color = "black", linewidth = 0.9, fill = NA) +
  geom_text(aes(label = percent(p, accuracy = 1)), size = 3.2) +
  scale_fill_gradient(low = "white", high = "#238b45", labels = percent) +
  labs(
    x = "Forecast Avalanche Hazard (FAH, day t)",
    y = "Observed Avalanche Hazard (OAH, day t+1)",
    fill = "Proportion"
  ) +
  theme_minimal(base_size = 12) +
  theme(axis.text.x = element_text(angle = 40, hjust = 1),
        plot.title = element_text(face = "bold", size = 14))

final_fah_oah_corrplot <- annotate_figure(fah_oah_corrplot,
                bottom = text_grob("Note: Each FAH column sums to 100%; next-day alignment performed within Area (if available).",
                                   hjust = 0, x = 0,
                                   size = 7, color = "black"))
final_fah_oah_corrplot
```

The plot shows that the forecast (*FAH*) is highest in stable, low-risk conditions and lowest at the extremes, with the middle categories marked by uncertainty. This implies that avalanche hazard ratings are best treated as ordinal outcomes for modelling purposes, where the cost of misclassification is not uniform and depends on how far apart the categories are. NNs can capture these patterns well when framed as ordinal classifiers. They can learn that misclassifications most often occur between neighbouring hazard levels, instead of treating all errors equally. Class imbalances will also need to be addressed during model training to ensure the network does not simply default to predicting the most common outcomes.

## Data Cleaning and Feature Engineering

Feature engineering focused on creating temporal variables to capture trends not natively handled by a feedforward neural network (FFNN). Given that EDA highlighted that *OAH* varies according to the season and time of year, several variables were derived.

Before removing observations with extensive missing values, we extracted the time of day, day of the week, month, and year from each date. Next, a categorical variable for each avalanche season (December-April, 2009-2025) was created to account for the observed year-on-year increase in risk.

```{r read_data}


data = read.csv("scotland_avalanche_forecasts_2009_2025.csv")



remove <- which(data$OAH == "")
data$OAH[remove] <- NA

data <- data %>% 
  mutate(across(where(is.character), as.factor))



c <- colnames(data)
r <- data.frame(var = character(),
                no = numeric())

for (i in c){
  
  if (is.factor(data[, i])){
  
  x <- length(unique(data[,i]))
  
  r <- rbind(r, data.frame(var = i,
                           no = x))
    
  }
  
  
}





```

```{r missing_val}

#Getting table of missing values


col.n <- colnames(data)
n <- nrow(data)

missing_df <- data.frame(Variable = character(),
                         "Missing (%)" = numeric(),
                         check.names = F)


for (i in col.n){
  
  x <- sum(is.na(data[,i]))
  
  perc <- (x/n) * 100
  
  missing_df <- rbind(missing_df,
                      data.frame(
                        Variable = i,
                        `Missing (%)` = perc
                      , check.names = F)
                      )
  
}
 
missing_df <- missing_df %>% 
  filter(`Missing (%)` > 0) %>% 
  arrange(desc(`Missing (%)`),)

#sum(!complete.cases(data))

x1 <- unique(data$AV.Cat)

```

Missing values were also a challenge. @tbl-missing-val presents the percentage of missing values for features with incomplete data. *Av.Cat* was removed because its meaning was unclear and it contained implausible values (e.g., `r x1`). @fig-ski-summit-wind shows no clear relationship between *ski penetration* and *OAH*, except for "High" risk avalanches; we therefore excluded it since snowfall effects are likely better represented by other variables. *Summit wind direction* (`r round(missing_df[missing_df$Variable == "Summit.Wind.Dir", "Missing (%)"], 2)`% missing) was also dropped for lack of a discernible trend.

```{r}
#| echo: FALSE
#| label: tbl-missing-val
#| tbl-cap: "Missing value exploration."


kable(missing_df,
      digits = 4) %>%
  kable_styling(
    bootstrap_options = c("hover"),
    full_width = FALSE
  ) %>%
  footnote(general = "All values rounded to 4 decimal places.")

```

<br>

```{r}
#| echo: FALSE
#| label: fig-ski-summit-wind
#| fig-cap: "Boxplots of ski penetration and summit wind direction by avalanche hazard category."

# 2) Plot (note the exact column name)
px <- ggplot(data, aes(x = OAH, y = Ski.Pen)) +
  geom_boxplot(na.rm = TRUE)+
  ylim(0, 20)

px1 <- ggplot(data, aes(x = OAH, y = Summit.Wind.Dir) )+
  geom_boxplot(na.rm = TRUE)+
  ylim(0,500)

p_all <- ggarrange(px, px1,
                   ncol = 2,
                   nrow = 1,
                   labels = c("A", "B"))

ski_summit_final_plot <- annotate_figure(p_all,
                bottom = text_grob("Note: Outliers have been excluded. Y-axis scale from 0 to 20 on the left, and 0 to 500 on the right.",
                                   hjust = 0, x = 0,
                                   size = 7, color = "black"))

ski_summit_final_plot

```

<br>

Lastly, we dropped high-cardinality categorical variables, including *osgrid* and *location. osgrid* appeared to be an ID variable with \>3000 unique values, which would make one-hot encoding overly complex and was unlikely to indicate risk. *Location* was also removed, since *latitude*, *longitude*, and *area* already captured spatial information more effectively.

```{r}

data_edit <- data %>% 
  select(-c(AV.Cat, Summit.Wind.Dir, Ski.Pen))


comp <- round(sum(!complete.cases(data_edit))/sum(complete.cases(data_edit)) * 100)

```

Having dropped these variables, we were left with roughly `r comp`% incomplete samples. We dropped the remaining samples that contained missing values to maintain complete cases for the models. While retaining more observations would be ideal, the remaining \~7000 were sufficient for representative results.

```{r date_time}



#adding date variable
data$Date <- as.POSIXct(data$Date, format="%Y-%m-%d %H:%M:%S", tz="UTC")


data$month     <- lubridate::month(data$Date)
data$hour      <- lubridate::hour(data$Date)
data$year      <- lubridate::year(data$Date)

#make a day of the season variable

#season begins from December to april



data <- data %>% 
  mutate(season_year = if_else(month >= 12, year, year -1),
         season_id = dense_rank(season_year),
         season = paste0("season", season_id),
         season = as.factor(season)) %>% 
  group_by(season, Area) %>% 
  mutate(dayofseason = row_number()) %>% 
  arrange(Date, Area)





```

```{r drop variables}

#Drop the top 3 variables wtih the most missing values
data <- data %>% 
  select(- c(AV.Cat, Ski.Pen, Summit.Wind.Dir, Obs, Location, OSgrid))


data_imp <- data[-remove,] #data for imputation later
#length(levels(data_imp$OAH))

data <- na.omit(data) #remove remaining missing



```

```{r split_data}

set.seed(1)
train.idx <- sample(1:nrow(data), 
                    round(nrow(data)*0.8))

train <- data[train.idx,] #Training data
test <- data[-train.idx,] #Testing Data

#levels(train$OAH)
#levels(test$OAH)
```

## Modeling Methodology

There are three main models used in this project to exploit different structural properties of the data: a convolutional neural network (CNN), a recurrent neural network (RNN), and a feedforward multilayer perceptron neural network network (FFNN-MLP). The FFNN-MLP was expected to handle categorical predictors (e.g., forecasted avalanche risk) effectively, while the CNNs and RNNs were targeted at uncovering temporal or sequential patterns.

CNNs process data by applying convolutional filters (small weight matrices) that slide across the input space. Each filter computes dot products with local patches of the data, producing feature maps that emphasise spatial or temporal patterns based on the type of filters used. Non-linear activations are then applied, followed by pooling layers that down sample the feature maps to reduce dimensionality while preserving the most important information. One can stack numerous layers of convolutions to learn deeper trends in the data, although we found that 2 layers were a good balance between the model’s complexity and ability to generalise.

```{r}
#| echo: false
#| results: asis
#| label: fig-animation-cnn
#| fig-cap: "Interactive visualisation of how a CNN operates."


knitr::include_url("cnn_filter_animation_quarto_ready_html.html")
```

::: {style="font-size: 0.6em; color: black; text-align: left; font-weight: bold"}
Credit: OpenAI (2025a and 2025b)
:::

RNNs are designed to capture sequential dependencies by incorporating feedback connections. At each time step, the hidden state depends not only on the current input but also on the state from the previous step. This architecture makes RNNs well suited for modeling temporal dynamics in sequential data.

A FFNN-MLP consists of an input layer, one or more hidden layers of perceptron nodes, and an output layer. Each hidden node computes a weighted sum of its inputs, passes it through a non-linear activation function, and forwards the result to the next layer. The weights, which determine the influence of each input, are initialised randomly and updated iteratively through backpropagation using an optimiser such as gradient descent. The activation functions (e.g., ReLU, sigmoid) introduce non-linearity, enabling the model to capture complex relationships between predictors and the target.

### Data Preprocessing of Neural Networks models

To prepare the data for CNN and RNN, we constructed a 3-dimensional array (number of samples × the time window length × the number of features). Each training sample was created by sliding a fixed window across the data. Within the array, columns represent time steps, and depth represent data features. Since we predict one day ahead, each sample represents the observations within the window, for the next day prediction. Data were standardised to avoid distortion from differing feature scales, and one-hot encoded so each categorical variable is split into multiple binary variable demarcating each category.

To preserve temporal trends for the CNN and RNN, the train and test split was not randomly shuffled. Instead, the first 80% of records were used for training, while the remainder was used for testing, similar to the approach of Fromm and Schonberger (2022).

### Convolutional Neural Network

CNN and RNN hyperparameter selection involved two stages. Firstly, several candidate datasets were created with time windows of 3 days, 7 days, 30 days and 120 days. A narrow search was used, training many models to identify the most appropriate window based on validation accuracy, estimated using cross-validation with a 20% holdout set. All models were trained for 60 epochs, except the FNN.

In the second stage, a broader grid search was conducted on the chosen window, selecting the model with the highest validation accuracy. In all our tests presented for the CNN, we assessed architectures with two convolutional layers using the ReLU function in those latent layers, and the softmax function in our input layer to create the class predictions. More complex architectures did not perform better, or meaningfully better. Measures to control for class imbalance were taken for both the CNN and RNN by calculating weights, inversely proportional to the frequency of observations of that class in the training set.

```{r Keras_data_preprocessing_function}

# library(keras3)
# library(tensorflow)
# library(reticulate)

# reticulate::install_miniconda()   # sets up a private Python just for R
# library(keras3)
# install_tensorflow()


#for this to work I need to convert the data into an array 
# form is (N (length), L (Time frame), F (Features))

#I promise that this code isnt made by Chatgpt I am just using a lot of comments otherwise I dont know what the #$!* is going on :)


#Function to let me test for numerous window sizes

#length(unique(data$OAH))
 pre_data <- function(w = 3, h = 1){

#--------Data Preparation-------------------------------------

T <- length(data$Date) #time period length

data_nn <- data %>%
  select(-c(Date)) #removing date column

L <- w #Rolling window length = 3. So what I am decided is that It can use the previus 3 days on record to train itself
H <- h # predict 1 day ahead


#number of samples we can have, so number of time slices, each one iterated one step ahead by H (which is just 1 day in our case)
N_samp <- T - L - H + 1



#----------------One hot encoding--------

#This part is quite a process. So big picture, the numeric data needs to be standardised, but when we do one-hot-encoding, it becomes quite messy to avoid standardising the dummy variables. SO we are going to make indices for the numeric columns so we can standardise only them later

#Taking our predictors and covariates into one df, and our labels into another

X.variables <- data_nn %>%
  select(-OAH)   #x_variables = everything in data other than OAH column

X.variables <- model.matrix(~., data = X.variables)[,-1] # One Hot encoding

y.variable <- data_nn[["OAH"]]  #y_variable = only our OAH column

numeric.variables <- names(data_nn)[sapply(data_nn, is.numeric)]
numeric.variables <- setdiff(numeric.variables, "OAH")

numeric.indexes <- which(colnames(X.variables) %in% numeric.variables) #okay so the colnames are the same so we can find the indexes in x variables which are numeric with this code <--

#dummy variable indexes
dummy.indexes <- setdiff(1:(ncol(X.variables)), numeric.indexes)


#Number features

F_ <- ncol(X.variables)

#--------------Making Array---------------------#


#Arrays - this is the confusing part. Keras needs an array for this to work. In our case we can think of this as turning our data into a stack of data sheets. each sheet is for a particular feature. THen there are N_sampl columns, and each is shifted by L

X <- array( NA_real_, #just an empty array, which will subsequently be filled
           dim = c(N_samp,L, F_  ),
           )

y <- integer(N_samp)




for (i in 1:N_samp){ #here we are filling in the array with our data

  index.window <- i:(i + L - 1) #index wind = ith value in N_samp to I + window minus 1

  X[i, , ] <- X.variables[index.window, , drop = F] #filling in the predictors with the corresponding indexes into that window
  y[i] <- y.variable[i + L + H -1] #we predict the ith + L + H y values.

}

y <- y - 1

length(unique(y))
unique(y)
#------------Training and testing split-----------------------------

#We want to preserve time series patterns, so no shuffling

#old version where everything was in order
idx <- 1:N_samp
number.training <- floor(N_samp * 0.8) #train = first 80% of features

tr <- idx[1:number.training] #Training indices
te <- idx[(number.training + 1):N_samp] #Testing indices

#making 1 = 0 to see if that helps



n.classes <- length(unique(y))

#test for randomness being better (didnt work)
# idx <- 1:N_samp
# set.seed(1)
# tr <- sample(idx, floor(0.8*N_samp))
# te <- idx[-tr]



X.train <- X[tr, , , drop = F] #training predictors data array
X.test <- X[te, , , drop = F ] #testing predictors array
y.train <- y[tr] #training target
y.test <- y[te]


#Class weights
class_counts <- table(y.train)
total <- sum(class_counts)
n_classes <- length(class_counts)


class_weights <- total / (n_classes * class_counts)

# keras expects a named list: names = class index as character
class_weights <- as.list(class_weights)
names(class_weights) <- as.character(0:(n_classes-1))



#----------------Scaling--------------------

#Remeber that we made indexes for our numeric data, which is the data we want to scale.

#We also need to scale the training data, and scale the testing data according to the training data mu and sd


if (length(numeric.indexes) > 0){

  mu.numeric <- apply(X.train[, , numeric.indexes, drop = F], 3, mean, na.rm = T)

  sd.numeric <- apply(X.train[ , , numeric.indexes, drop = F], 3, sd, na.rm = T)

   sd.numeric[sd.numeric == 0 | is.na(sd.numeric)] <- 1 #make sure SD == 1

  for (j in seq_along(numeric.indexes)){

    k <- numeric.indexes[j]
    X.train[, , k] <- (X.train[ , , k] - mu.numeric[j])/sd.numeric[j]
    X.test[, , k] <- (X.test[ , , k] - mu.numeric[j])/sd.numeric[j]
  }


}


round(apply(X.train[ , , numeric.indexes, drop = FALSE], 3, mean), 3)
round(apply(X.train[ , , numeric.indexes, drop = FALSE], 3, sd),   3)



#Getting there !

#Since we are doing a multiclass prediction we need to use softmax
#This means using the tocategorical() function on y

y.test <- to_categorical(y.test, num_classes = n.classes)


y.train <- to_categorical(y.train, num_classes = n.classes)

list(y.test, y.train, X.train, X.test, class_weights, F_)





}



```

```{r keras_grid_search}
#testing c-d-c

#vary window size hasnt made significant changes
#Adding weights to control for imbalance didnt help

keras_search <- function(window){

W_ <- pre_data(w = window, h = 1)

y.test <- W_[[1]]
y.train <- W_[[2]]
X.train <- W_[[3]]
X.test <- W_[[4]]
cl_weights <- W_[[5]]
F_ <- W_[[6]]
# ---------------- Grid search for existing architecture ----------------


set.seed(1)
tf$keras$utils$set_random_seed(123L)

# Define the grid (adjust values if you want a bigger/smaller search)
grid <- expand.grid(
  filters1   = c( 32, 64),
  filters2   = c( 32, 64),
  kernel_sz  = c(2, 5, 15),
  drop1      = c(0.3, 0.5),
  drop2      = c(0.3, 0.5),
  stringsAsFactors = FALSE
)

results <- data.frame(
  filters1 = integer(), filters2 = integer(),
  kernel_sz = integer(), drop1 = double(), drop2 = double(),
  val_accuracy = double(), test_loss = double(), test_accuracy = double(),
  stringsAsFactors = FALSE
)

histories <- vector("list", nrow(grid))  # optional: keep training histories

for (i in seq_len(nrow(grid))) {
  cat(sprintf("\n[%d/%d] f1=%d, f2=%d, k=%d, d1=%.2f, d2=%.2f\n",
              i, nrow(grid),
              grid$filters1[i], grid$filters2[i],
              grid$kernel_sz[i], grid$drop1[i], grid$drop2[i]))

  # Clear TF/Keras state between runs to avoid graph/memory accumulation
  tensorflow::tf$keras$backend$clear_session()

  
  set.seed(1)
  tf$keras$utils$set_random_seed(123L)

 
  input <- layer_input(shape = c(dim(X.train)[2], F_)) # (L, F)

  output <- input %>%
    layer_conv_1d(filters = grid$filters1[i],
                  kernel_size = grid$kernel_sz[i],
                  padding = "same", activation = "relu") %>%
    layer_dropout(rate = grid$drop1[i]) %>%
    layer_conv_1d(filters = grid$filters2[i],
                  kernel_size = grid$kernel_sz[i],
                  padding = "same", activation = "relu") %>%
    layer_global_average_pooling_1d() %>%
    layer_dropout(rate = grid$drop2[i]) %>%
    layer_dense(units = 5, activation = "softmax")

  model <- keras_model(inputs = input, outputs = output)

  model %>% compile(
    optimizer = "adam",
    loss = "categorical_crossentropy",
    metrics = "accuracy"
  )

  # ----- Train -----
  history <- model %>% fit(
    X.train, y.train,
    epochs = 60,
    batch_size = 64,
    verbose = 0,
    validation_split = 0.2,
    shuffle = FALSE
    # = cl_weights
  )

  # collect validation accuracy from the last epoch
  val_acc <- tail(history$metrics$val_accuracy, 1)
  if (length(val_acc) == 0) {
   
    val_acc <- tail(history$metrics$val_acc, 1)
  }

  # Evaluate on test set 
  test_metrics <- model %>% evaluate(X.test, y.test, verbose = 0)

  # test_metrics is usually a named numeric vector: c(loss=..., accuracy=...)
  test_loss <- as.numeric(test_metrics[[1]])
  test_acc  <- if (!is.null(names(test_metrics)) && "accuracy" %in% names(test_metrics)) {
    as.numeric(test_metrics[["accuracy"]])
  } else {
    as.numeric(test_metrics[[2]])
  }

  # store results
  results[i, ] <- list(
    grid$filters1[i], grid$filters2[i],
    grid$kernel_sz[i], grid$drop1[i], grid$drop2[i],
    as.numeric(val_acc), test_loss, test_acc
  )
  histories[[i]] <- history
}

# sort by best validation accuracy 
results <- results[order(-results$val_accuracy), ]

print(results)

# The best 
best <- results[1, ]

rm(W_) #For saving memory
gc()

return(results)

}



```

```{r keras_search}
#Window = 3 
# 
# res_window_3 <- keras_search(window = 3)
# save(res_window_3, file =  "res_window_3")
# 
# # #Window  = 7
# res_window_7 <- keras_search(window = 7)
# save(res_window_7, file =  "res_window_7")
# 
# #Window = 30
# res_window_30 <- keras_search(window = 30)
# 
# #Window = 120
# res_window_120 <- keras_search(window = 120)
# save(res_window_120, file =  "res_window_120")

load("res_window_3")
load("res_window_7")
load("res_window_30")
load("res_window_120")


res_window_3 <- res_window_3 %>% 
  mutate(model_no = row_number(),
         window  = 3)

res_window_7 <- res_window_7 %>% 
  mutate(model_no = row_number(),
         window  = 7)

res_window_30 <- res_window_30 %>% 
  mutate(model_no = row_number(),
         window  = 30)

res_window_120 <- res_window_120%>% 
  mutate(model_no = row_number(),
         window  = 120)


results_cnn_search <- rbind(res_window_3, res_window_30, res_window_7,  res_window_120)


```

```{r cnn_window_vis}
#| echo: FALSE
#| label: fig-cnn-window
#| fig-cap: "CNN prediction window search."

#graph of validation accuracy
ggplot(data = results_cnn_search, aes( x  = model_no, y  = val_accuracy, colour = as.factor(window)))+
  geom_point()+
  labs(x = "Model Number (arranged by validation accuracy)") + 
  labs(y = "Validation Accuracy",
       colour = "Window Length")


```

Considering @fig-cnn-window above, we chose a 7 day prediction window. Overall, most models performed best with 3-7 days, while 30- and 120-day windows were meaningfully worse. Validation accuracy was competitive between 3 and 7 days. However, 7 days appeared more robust, as some 3-day models showed much lower accuracy at the extremes.

```{r  expanded_search}
# #we will go with a window of roughly  7  days

# rm(res_window_120,  res_window_3, res_window_30)
# 
# W_ <- pre_data(w = 7, h = 1)
# 
# y.test <- W_[[1]]
# y.train <- W_[[2]]
# X.train <- W_[[3]]
# X.test <- W_[[4]]
# cl_weights <- W_[[5]]
# F_ <- W_[[6]]

# 
# # Repro seeds
# set.seed(1)
# tf$keras$utils$set_random_seed(123L)
# 
# # Define the grid (
grid <- expand.grid(
  filters1   = c( 16, 32, 64),
  filters2   = c( 16, 32, 64),
  kernel_sz  = c(5,15),
  drop1      = c(0.5,0.7),
  drop2      = c(0.5, 0.7),
  stringsAsFactors = FALSE
)
# 
# results <- data.frame(
#   filters1 = integer(), filters2 = integer(),
#   kernel_sz = integer(), drop1 = double(), drop2 = double(),
#   val_accuracy = double(), test_loss = double(), test_accuracy = double(),
#   stringsAsFactors = FALSE
# )
# 
# histories <- vector("list", nrow(grid))  # optional: keep training histories
# 
# for (i in seq_len(nrow(grid))) {
#   cat(sprintf("\n[%d/%d] f1=%d, f2=%d, k=%d, d1=%.2f, d2=%.2f\n",
#               i, nrow(grid),
#               grid$filters1[i], grid$filters2[i],
#               grid$kernel_sz[i], grid$drop1[i], grid$drop2[i]))
# 
#   # Clear TF/Keras state between runs to avoid graph/memory accumulation
#   tensorflow::tf$keras$backend$clear_session()
# 
# 
#   set.seed(1)
#   tf$keras$utils$set_random_seed(123L)
# 
# 
#   input <- layer_input(shape = c(dim(X.train)[2], F_)) # (L, F)
# 
#   output <- input %>%
#     layer_conv_1d(filters = grid$filters1[i],
#                   kernel_size = grid$kernel_sz[i],
#                   padding = "same", activation = "relu") %>%
#     layer_dropout(rate = grid$drop1[i]) %>%
#     layer_conv_1d(filters = grid$filters2[i],
#                   kernel_size = grid$kernel_sz[i],
#                   padding = "same", activation = "relu") %>%
#     layer_global_average_pooling_1d() %>%
#     layer_dropout(rate = grid$drop2[i]) %>%
#     layer_dense(units = 5, activation = "softmax")
# 
#   model <- keras_model(inputs = input, outputs = output)
# 
#   model %>% compile(
#     optimizer = "adam",
#     loss = "categorical_crossentropy",
#     metrics = "accuracy"
#   )
# 
#   # ----- Train -----
#   history <- model %>% fit(
#     X.train, y.train,
#     epochs = 60,
#     batch_size = 64,
#     verbose = 0,
#     validation_split = 0.2,
#     shuffle = FALSE
#     # = cl_weights
#   )
# 
#   # Collect validation accuracy from the last epoch
#   val_acc <- tail(history$metrics$val_accuracy, 1)
#   if (length(val_acc) == 0) {
#     # Fallback name in case of different metric key
#     val_acc <- tail(history$metrics$val_acc, 1)
#   }
# 
#   # Evaluate on test set
#   test_metrics <- model %>% evaluate(X.test, y.test, verbose = 0)
# 
#   # test_metrics is usually a named numeric vector:
#   test_loss <- as.numeric(test_metrics[[1]])
#   test_acc  <- if (!is.null(names(test_metrics)) && "accuracy" %in% names(test_metrics)) {
#     as.numeric(test_metrics[["accuracy"]])
#   } else {
#     as.numeric(test_metrics[[2]])
#   }
# 
#   # Store results
#   results[i, ] <- list(
#     grid$filters1[i], grid$filters2[i],
#     grid$kernel_sz[i], grid$drop1[i], grid$drop2[i],
#     as.numeric(val_acc), test_loss, test_acc
#   )
#   histories[[i]] <- history
# }
# 
# # Sort by best validation accuracy
# results <- results[order(-results$val_accuracy), ]
# 
# print(results)
# 
# final_cnn_search  <-  results
# save(final_cnn_search, file =  "final_cnn_serach")


load("final_cnn_serach")

# The best 
best <- final_cnn_search[1, ]

#best
# 
# gc()
```

```{r}
#| echo: FALSE
#| label: tbl-expan-grid
#| tbl-cap: "Expanded grid search hyperparameters and values."


grid <- data.frame(
  hyperparameter = c("filters1", "filters2", "kernel_sz", "drop1", "drop2"),
  values = I(list(
    c(16, 32, 64),
    c(16, 32, 64),
    c(5, 15),
    c(0.5, 0.7),
    c(0.5, 0.7)
  ))
)



kable(grid, format = "html",
      digits = 4) %>%
  kable_styling(
    bootstrap_options = c("hover"),
    full_width = FALSE
  ) 


```

```{r  final_cnn_search_viz}
#| echo: FALSE
#| label: fig-cnn-search-viz
#| fig-cap: "Top 30 configurations by validation accuracy for CNN."

res <- final_cnn_search %>%
  mutate(
    run_id = row_number(),
    label  = sprintf("f1=%d f2=%d k=%d d1=%.1f d2=%.1f",
                     filters1, filters2, kernel_sz, drop1, drop2)
  ) %>%
  arrange(desc(val_accuracy)) %>%
  mutate(rank = row_number(),
         top5 = rank <= 5)

library(scales)


top_n <- 30
ggplot(res %>% slice(1:top_n),
       aes(x = reorder(label, val_accuracy), y = val_accuracy, fill = top5)) +
  geom_col() +
  coord_flip() +
  geom_text(aes(label = percent(val_accuracy, accuracy = 0.1)),
            hjust = -0.1, size = 3) +
  scale_y_continuous(labels = percent_format(accuracy = 1), expand = expansion(mult = c(0, .1))) +
  scale_fill_manual(values = c("lightblue", "#1f78b4"), guide = "none") +
  labs(x = "Config", y = "Val accuracy")

x1 <- round(max(results_cnn_search$val_accuracy)*100)

```

After completing the initial window search, an expanded grid search using the hyperparameters presented in @tbl-expan-grid was conducted. The validation accuracy for the top 30 models is presented in @fig-cnn-search-viz .The best performing model during the initial prediction window search (the first stage of the search) achieved a validation accuracy of `r x1`, while the top model using the chosen window in the second stage of the search reached `r round(max(res$val_accuracy) * 100, 1)`%. We stuck with this model, as validation accuracy had not improved meaningfully over the course of the expanded search. The final selected model involved 32 filters in the first layer, followed by a dropout of 70%, then 16 layers in the second layers, followed by dropout of 70% and global average pooling into the final output layer.

```{r fiiting_final_cnn}

library(keras3)

W_ <- pre_data(w = 7, h = 1)

y.test <- W_[[1]]
y.train <- W_[[2]]
X.train <- W_[[3]]
X.test <- W_[[4]]
cl_weights <- W_[[5]]
F_ <- W_[[6]]
# 
# dim(X.test)
# dim(y.test)
# dim(X.train)
# dim(y.train)
# 
# input <- layer_input(shape = c(dim(X.train)[2], F_)) # (L, F)
# 
#   output <- input %>%
#     layer_conv_1d(filters = 32, kernel_size = 15, padding = "same", activation = "relu") %>%
#     layer_dropout(rate = 0.7) %>%
#     layer_conv_1d(filters = 16, kernel_size = 15, padding = "same", activation = "relu") %>%
#     layer_global_average_pooling_1d() %>%
#     layer_dropout(rate = 0.5) %>%
#     layer_dense(units = 5, activation = "softmax")
# 
#   model <- keras_model(inputs = input, outputs = output)
# 
#   model %>% compile(
#     optimizer = "adam",
#     loss = "categorical_crossentropy",
#     metrics = "accuracy"
#   )
# 
#     history <- model %>% fit(
#     X.train, y.train,
#     epochs = 60,
#     batch_size = 64,
#     verbose = 0,
#     validation_split = 0,
#     shuffle = FALSE,
#      class_weight = cl_weights
#   )
# 
# final_cnn_model  <- model
# 
# 
# 
# 
# 
# test_metrics <- model %>% evaluate(X.test, y.test, verbose = 0)
# 
# test_metrics$accuracy
# 
# 
# 
# pred_cnn  <- predict(final_cnn_model, X.test)
# 
# pred_cnn <- max.col(pred_cnn)
# save(pred_cnn, file = "pred_cnn")
##Temporal labels for cnn
# 
# temporal_labels_cnn <- max.col(y.test)
# save(temporal_labels_cnn, file = "temporal_labels_rnn")
# 
# 
# load("temporal_labels_rnn")
#save(pred_cnn, file   = "pred_cnn")

load("pred_cnn")

```

### Recurrent Neural Network

The approach for the RNN was the same as the CNN discussed above. Firstly, a search over numerous candidate window sizes were completed, and a subsequent expanded search was done as well. In @fig-rnn-window below, the validation accuracy of the models test is presented. We used a combination of two Long Short-Term memory layers (LSTM), with dropout and varied the LSTM units and dropout values in the grid search.

```{r  RNN_search_function}

#sam as above but for RNN model



 keras_search_rnn <- function(window){

  W_ <- pre_data(w = window, h = 1)

  y.test <- W_[[1]]
  y.train <- W_[[2]]
  X.train <- W_[[3]]
  X.test <- W_[[4]]
  cl_weights <- W_[[5]]
  F_ <- W_[[6]]

  set.seed(1)
  tf$keras$utils$set_random_seed(123L)


  grid <- expand.grid(
    units1 = c(32, 64),   # LSTM units in layer 1
    units2 = c(32, 64),   # LSTM units in layer 2
    drop1  = c(0.3, 0.5),
    drop2  = c(0.3, 0.5),
    stringsAsFactors = FALSE
  )

  results <- data.frame(
    units1 = integer(), units2 = integer(),
    drop1 = double(), drop2 = double(),
    val_accuracy = double(),
    test_loss = double(), test_accuracy = double(),
    stringsAsFactors = FALSE
  )

  histories <- vector("list", nrow(grid))

  for (i in seq_len(nrow(grid))) {
    cat(sprintf("\n[%d/%d] u1=%d, u2=%d, d1=%.2f, d2=%.2f\n",
                i, nrow(grid),
                grid$units1[i], grid$units2[i],
                grid$drop1[i], grid$drop2[i]))

    tensorflow::tf$keras$backend$clear_session()
    set.seed(1)
    tf$keras$utils$set_random_seed(123L)

    input <- layer_input(shape = c(dim(X.train)[2], F_)) # (timesteps, features)

    output <- input %>%
      layer_lstm(units = grid$units1[i], return_sequences = TRUE) %>%
      layer_dropout(rate = grid$drop1[i]) %>%
      layer_lstm(units = grid$units2[i], return_sequences = FALSE) %>%
      layer_dropout(rate = grid$drop2[i]) %>%
      layer_dense(units = 5, activation = "softmax")

    model <- keras_model(inputs = input, outputs = output)

    model %>% compile(
      optimizer = "adam",
      loss = "categorical_crossentropy",
      metrics = "accuracy"
    )

    history <- model %>% fit(
      X.train, y.train,
      epochs = 60,
      batch_size = 64,
      verbose = 0,
      validation_split = 0.2,
      shuffle = FALSE
    )

    # val acc from last epoch
    val_acc <- tail(history$metrics$val_accuracy, 1)
    if (length(val_acc) == 0) val_acc <- tail(history$metrics$val_acc, 1)

    test_metrics <- model %>% evaluate(X.test, y.test, verbose = 0)
    test_loss <- as.numeric(test_metrics[[1]])
    test_acc  <- if (!is.null(names(test_metrics)) && "accuracy" %in% names(test_metrics)) {
      as.numeric(test_metrics[["accuracy"]])
    } else {
      as.numeric(test_metrics[[2]])
    }

    results[i, ] <- list(
      grid$units1[i], grid$units2[i],
      grid$drop1[i], grid$drop2[i],
      as.numeric(val_acc), test_loss, test_acc
    )
    histories[[i]] <- history
  }

  results <- results[order(-results$val_accuracy), ]
  print(results)

  rm(W_)
  gc()
  return(results)
}



```

```{r RNN_window_search}
#| echo: FALSE
#| label: fig-rnn-window
#| fig-cap: "RNN prediction window search."



#
# #Window = 3 
# 
# rnn_window_3 <- keras_search_rnn(window = 3)
# save(rnn_window_3, file =  "rnn_window_3")
# #Window  = 7
# rnn_window_7 <- keras_search_rnn(window = 7)
# save(rnn_window_7, file =  "rnn_window_7")
# #Window = 30
# rnn_window_30 <- keras_search_rnn(window = 30)
# save(rnn_window_30, file =  "rnn_window_30")
# #Window = 120
# rnn_window_120 <- keras_search_rnn(window = 120)
# save(rnn_window_120, file =  "rnn_window_120")


load("rnn_window_3")
load( "rnn_window_7")
load("rnn_window_30")
load("rnn_window_120")

rnn_window_3 <- rnn_window_3 %>% 
  mutate(model_no = row_number(),
         window  = 3)

rnn_window_7 <- rnn_window_7 %>% 
  mutate(model_no = row_number(),
         window  = 7)

rnn_window_30 <- rnn_window_30 %>% 
  mutate(model_no = row_number(),
         window  = 30)

rnn_window_120 <- rnn_window_120%>% 
  mutate(model_no = row_number(),
         window  = 120)

results_rnn_search <- rbind(rnn_window_3, rnn_window_30, rnn_window_7,  rnn_window_120)
ggplot(data = results_rnn_search, aes( x  = model_no, y  = val_accuracy, colour = as.factor(window)))+
  geom_point()+
  labs(x = "Model Number (arranged by  validation accuracy)")
#max(results_rnn_search$test_accuracy)

```

The RNN window search showed more competitive performance. In this case, a window of 30 days appears more robust overall, containing fewer worse performing models towards the extreme end. We therefore conducted an expanded grid search using this window, the results of which are presented in @fig-rnn-configs below. The grid search hyperparameters are listed in @tbl-rnn-gs-param.

```{r}
#| echo: FALSE
#| label: tbl-rnn-gs-param
#| tbl-cap: "Expanded grid search hyperparameters and values."


grid_summary <- data.frame(
  Hyperparameter = c("units1", "units2", "drop1", "drop2"),
  Values = c(
    paste(c(16, 32, 64), collapse = ", "),
    paste(c(16, 32, 64), collapse = ", "),
    paste(c(0.3, 0.5, 0.7), collapse = ", "),
    paste(c(0.3, 0.5, 0.7), collapse = ", ")
  ),
  stringsAsFactors = FALSE
)



kable(grid_summary, format = "html",
      digits = 4) %>%
  kable_styling(
    bootstrap_options = c("hover"),
    full_width = FALSE
  ) 


```

```{r rnn_fitting}
# 
# library(keras3)
# 
# W_ <- pre_data(w = 30, h = 1) #using 30 days
# 
# y.test <- W_[[1]]
# y.train <- W_[[2]]
# X.train <- W_[[3]]
# X.test <- W_[[4]]
# cl_weights <- W_[[5]]
# F_ <- W_[[6]]

# 
# set.seed(1)
# tf$keras$utils$set_random_seed(123L)


# grid <- expand.grid(
#   units1 = c(16,32, 64),   # LSTM units in layer 1
#   units2 = c(16, 32, 64),   # LSTM units in layer 2
#   drop1  = c(0.3, 0.5, 0.7),
#   drop2  = c(0.3, 0.5, 0.7),
#   stringsAsFactors = FALSE
# )
# 
# results <- data.frame(
#   units1 = integer(), units2 = integer(),
#   drop1 = double(), drop2 = double(),
#   val_accuracy = double(),
#   test_loss = double(), test_accuracy = double(),
#   stringsAsFactors = FALSE
# )
# 
# histories <- vector("list", nrow(grid))
# 
# for (i in seq_len(nrow(grid))) {
#   cat(sprintf("\n[%d/%d] u1=%d, u2=%d, d1=%.2f, d2=%.2f\n",
#               i, nrow(grid),
#               grid$units1[i], grid$units2[i],
#               grid$drop1[i], grid$drop2[i]))
# 
#   tensorflow::tf$keras$backend$clear_session()
#   set.seed(1)
#   tf$keras$utils$set_random_seed(123L)
# 
#   input <- layer_input(shape = c(dim(X.train)[2], F_)) # (timesteps, features)
# 
#   output <- input %>%
#     layer_lstm(units = grid$units1[i], return_sequences = TRUE) %>%
#     layer_dropout(rate = grid$drop1[i]) %>%
#     layer_lstm(units = grid$units2[i], return_sequences = FALSE) %>%
#     layer_dropout(rate = grid$drop2[i]) %>%
#     layer_dense(units = 6, activation = "softmax")
# 
#   model <- keras_model(inputs = input, outputs = output)
# 
#   model %>% compile(
#     optimizer = "adam",
#     loss = "categorical_crossentropy",
#     metrics = "accuracy"
#   )
# 
#   history <- model %>% fit(
#     X.train, y.train,
#     epochs = 60,
#     batch_size = 64,
#     verbose = 0,
#     validation_split = 0.2,
#     shuffle = FALSE
#   )
# 
#   # val acc from last epoch
#   val_acc <- tail(history$metrics$val_accuracy, 1)
#   if (length(val_acc) == 0) val_acc <- tail(history$metrics$val_acc, 1)
# 
#   test_metrics <- model %>% evaluate(X.test, y.test, verbose = 0)
#   test_loss <- as.numeric(test_metrics[[1]])
#   test_acc  <- if (!is.null(names(test_metrics)) && "accuracy" %in% names(test_metrics)) {
#     as.numeric(test_metrics[["accuracy"]])
#   } else {
#     as.numeric(test_metrics[[2]])
#   }
# 
#   results[i, ] <- list(
#     grid$units1[i], grid$units2[i],
#     grid$drop1[i], grid$drop2[i],
#     as.numeric(val_acc), test_loss, test_acc
#   )
#   histories[[i]] <- history
# }
# 
# results <- results[order(-results$val_accuracy), ]
# 
# rnn_final_search <- results
# 
# save(rnn_final_search, file = "rnn_final_search")
  
load("rnn_final_search")

```

```{r}
#| echo: FALSE
#| label: fig-rnn-configs
#| fig-cap: "Top 30 configurations by validation accuracy for RNN."

res <- rnn_final_search %>%
  mutate(
    run_id = row_number(),
    label  = sprintf("u1=%d u2=%d d1=%.1f d2=%.1f",
                     units1, units2, drop1, drop2)
  ) %>%
  arrange(desc(val_accuracy)) %>%
  mutate(rank = row_number(),
         top5 = rank <= 5)

library(scales)


top_n <- 30
ggplot(res %>% slice(1:top_n),
       aes(x = reorder(label, val_accuracy), y = val_accuracy, fill = top5)) +
  geom_col() +
  coord_flip() +
  geom_text(aes(label = percent(val_accuracy, accuracy = 0.1)),
            hjust = -0.1, size = 3) +
  scale_y_continuous(labels = percent_format(accuracy = 1), expand = expansion(mult = c(0, .1))) +
  scale_fill_manual(values = c("lightblue", "#1f78b4"), guide = "none") +
  labs(x = "Config", y = "Val accuracy")

x1 <- round(max(results_rnn_search$val_accuracy)*100)

```

From the expanded grid search, the best performing model achieved a validation accuracy of `r round(max(res$val_accuracy) * 100, 1)`%. We fitted this model to the full training data and used that to predict onto the test set. Overall, we were satisfied with this search as the validation accuracy had not improved drastically compared to the initial window search; for example, the best model in the first stage reached roughly `r x1`%.

```{r final_rnn_search}


#rnn_final_search[1,]
#fitting final rnn mdoel = 64, 16 0.7.  0.5



# W_ <- pre_data(w = 30, h = 1) #using 30 days
# 
# y.test <- W_[[1]]
# y.train <- W_[[2]]
# X.train <- W_[[3]]
# X.test <- W_[[4]]
# cl_weights <- W_[[5]]
# F_ <- W_[[6]]
# 
# 
# 
#     input <- layer_input(shape = c(dim(X.train)[2], F_)) # (timesteps, features)
# 
#     output <- input %>%
#       layer_lstm(units = 64, return_sequences = TRUE) %>%
#       layer_dropout(rate = 0.7) %>%
#       layer_lstm(units = 16, return_sequences = FALSE) %>%
#       layer_dropout(rate = 0.5) %>%
#       layer_dense(units = 5, activation = "softmax")
# 
#     final_rnn_model <- keras_model(inputs = input, outputs = output)
# 
#     final_rnn_model %>% compile(
#       optimizer = "adam",
#       loss = "categorical_crossentropy",
#       metrics = "accuracy"
#     )
# 
# 
# 
#   history <- final_rnn_model %>% fit(
#     X.train, y.train,
#     epochs = 60,
#     batch_size = 64,
#     verbose = 0,
#     validation_split = 0.2,
#     shuffle = FALSE,
#     class_weight = cl_weights
#   )
# 
# true_labels <- apply(y.test, 1, max)
# pred_rnn  <- predict(final_rnn_model, X.test)
# 
# dim(X.test)
# 
# pred_rnn <- max.col(pred_rnn)
# 
# 
# 
# save(pred_rnn,  file = "pred_rnn")
# 
# ##labels for test  set later
# 
# temporal_labels_rnn <- max.col(y.test)
# save(temporal_labels_rnn, file = "temporal_labels_rnn")


load("temporal_labels_rnn")
load("pred_rnn")

#pred_rnn
```

### Feedforward Multilayer Perceptron Neural Network

```{r libs-and-seed}
# Load libraries for data wrangling, visualization, preprocessing, and modeling
library(dplyr)
library(tidyverse)
library(ggplot2)
library(janitor)
library(caTools)
library(caret)
library(e1071)
library(readr)
library(tidyverse)
library(janitor)
library(lubridate)
library(skimr)
library(naniar)
library(GGally)
library(corrplot)
library(factoextra)
library(cluster)
library(rsample)
library(recipes)
library(scales)
library(cowplot)
library(pROC)
library(reshape2)

# Load the required libraries so their functions are available for use.
library(xgboost)   # Core API for training and using XGBoost models.
library(caret)     # High-level interface for model training, tuning, and evaluation.
library(pROC)      # Provides functions for ROC curve plotting and AUC computation.
library(pdp)       # Used to generate Partial Dependence Plots for model interpretation.

# Set the random seed for reproducibility.
# Ensures that random processes like resampling and model training are consistent across runs.
set.seed(123)
```

The FFNN-MLP was developed for multi-class classification using the H2O deep learning framework. The objective was to predict a five level categorical outcome based on a high-dimensional feature set.

The FFNN-MLP architecture was systematically optimised through a randomised discrete grid search to identify the most effective combination of hyperparameters. All candidate models were structured with a consistent input and output layer, while the hidden layer configuration was varied. The input layer comprised 61 nodes, corresponding to the full set of hot-one encoded engineered predictor variables, plus the bias.

The grid search explored architectures containing between two and four hidden layers. Neuron configurations tested included dense layers such as \[256, 128, 64\], \[512, 256, 128\], and \[128, 64\]. The RectifierWithDropout activation function was initially considered to promote non-linearity and mitigate overfitting. The output layer contained five neurons and employed a softmax activation function to generate a probability distribution over the classes.

To counteract the effects of an imbalanced class distribution in the training data, a class-balancing strategy was implemented. Each training observation was assigned a weight inversely proportional to its class frequency, thereby increasing the influence of minority classes during the optimisation of the loss function.

```{r}
n1 <- 4*3*3*2*3
```

The randomised grid search evaluated a maximum of `r n1` candidate models, exploring a predefined hyperparameter space. Key hyperparameters and their search ranges are detailed in @tbl-ffnn-param below.

```{r}
#| echo: FALSE
#| label: tbl-ffnn-param
#| tbl-cap: "FFNN-MLP Hyperparameter Grid."

summary_df <- data.frame(
  Hyperparameter = c("hidden",
                     "activation",
                     "l1",
                     "l2",
                     "rate",
                     "input_dropout_ratio"),
  Values = c("(128,128); (128,64); (64,32,16); (64,32)",
             "RectifierWithDropout",
             "0, 1e-4, 1e-5",
             "0, 1e-4, 1e-5",
             "0.01, 0.001",
             "0.1, 0.3, 0.5"),
  stringsAsFactors = FALSE
)






tbl_df <- summary_df %>%
  mutate(Values = gsub("; ", "<br>", Values))

kable(tbl_df, format = "html", escape = FALSE) %>%
  kable_styling(
    bootstrap_options = c( "hover", "condensed"),
    full_width = FALSE, position = "left", font_size = 13
  ) %>%
  row_spec(0, color = "white") %>%   # header style
  column_spec(1, bold = TRUE, width = "22%") %>%             # Hyperparameter column
  column_spec(2, width = "60%", monospace = TRUE)

```

Each model was trained for a maximum of 200 epochs. An early stopping mechanism was employed to prevent overfitting, which terminated training if the validation set logloss did not improve by a tolerance of at least 1e-4 over 10 consecutive scoring intervals.

```{r h2o-initialise}
## ---------------------------
## H2O Deep Learning (Neural Network) workflow
## Built from existing objects: `train`, `test`, `predictor_cols`, `target`, `levels_y`, `num_class`
## This script uses the data preparation already done above and trains an H2O neural net,
## evaluates on the test set and shows how to tune hyperparameters with H2O Grid Search.
## ---------------------------


target <- "OAH"
predictor_cols <- setdiff(names(data), target) #Matt edit, replaced df with data, to be consistent as above


# Load h2o and initialize --------------------------------------------------
# Note: if h2o isn't installed, run: install.packages("h2o")
library(h2o)
# Start (or connect to) an H2O cluster. This creates an in-memory instance used for training.
# nthreads = -1 will use all available cores.
#h2o.init(nthreads = -1)

```

```{r turn-to-h2o-format}
# Convert existing R data.frames to H2O frames --------------------------------
# We use the `train` and `test` data.frames already prepared earlier in the pipeline.
# H2O works with its own frame objects; as.h2o() converts R data.frames into H2O frames.
# train_h2o <- as.h2o(train) #Matt Comment: now this uses my test and train made before modeling
# test_h2o  <- as.h2o(test)
```

```{r feature-modification-h2o}
# Define features (x) and response (y) -------------------------------------
# Using the same predictor list used for XGBoost so we reuse feature engineering already performed.
# x_vars <- predictor_cols  # character vector of predictor column names
# y_var  <- target          # the response column name ("oah")
# 
# 
# # Ensure response is treated as a categorical (factor) in H2O for classification
# train_h2o[[y_var]] <- as.factor(train_h2o[[y_var]])
# test_h2o[[y_var]]  <- as.factor(test_h2o[[y_var]])
```

```{r neural-net-weights}

#Matt Edit: This is causing NA's when I run it. I dont necessarily think we have to weight rare classes more, but weight things equally, so for now I am setting balance classes = T in H2o.

# # ---------------------------
# # H2O neural network with class weighting + randomized grid search
# # ---------------------------
# # This block expects the following objects already exist:
# #  - train (R data.frame), test (R data.frame)
# #  - predictor_cols (character vector), target (string), levels_y (factor levels), num_class (int)
# #  - h2o cluster started (h2o.init() called earlier)
# # If train_h2o/test_h2o are not present, the script will create them from train/test.
# 
# 
# # 1) Create a class-weight column (inverse frequency) to help under-represented classes -
# #    weight_i = N / (K * count(class_i))  -> standard balanced weight
# freq_table <- as.data.frame(table(train[[target]]))
# names(freq_table) <- c("class", "count")
# N_total <- nrow(train)
# K <- num_class
# 
# # map class -> weight
# class_counts <- setNames(freq_table$count, freq_table$class)
# class_weights_map <- N_total / (K * class_counts)   # higher weight for rarer classes
# 
# # create a vector of weights for each row in train, matching the order of 'train'
# weights_vector <- as.numeric(class_weights_map[as.character(train[[target]])])
# 
# # attach weights column into train H2O frame (column name: "class_weights")
# train_h2o[,"class_weights"] <- as.h2o(weights_vector)
# 
# # For reproducibility, add the weights to the R 'train' too (keeps them in sync)
# train$class_weights <- weights_vector
```

```{r neural-net-weighted-data-train-test-split}
# Split training H2O frame into train/validation for early stopping ---------
# We'll keep a validation frame to enable early stopping during H2O training.
# Use an 80/20 split of the existing training data.
#    We keep a validation frame so early stopping works.
# set.seed(123)
# 
# splits <- h2o.splitFrame(train_h2o, ratios = 0.8, seed = 123)
# 
# train_h2o_split <- splits[[1]]
# valid_h2o_split <- splits[[2]]
```

```{r neural-net-hyper-params}
# Define hyperparameter search space (RandomDiscrete)
#    Note: large search space with many models will run long; adjust max_models for runtime.
hyper_params <- list(
  hidden = list(
    c(128,128),
    c(128,64),
    c(64,32,16),
    c(64,32)),
  activation =  c("RectifierWithDropout"),
  l1 = c(0, 1e-4, 1e-5),
  l2 = c(0, 1e-4,1e-5),
  rate = c(0.01, 0.001),                # learning rates (smaller = slower but better generalization)
  input_dropout_ratio = c(0.1, 0.3, 0.5))     # apply some input dropout regularization
```

```{r neural-net-search-criteria}
# Because hidden_dropout_ratios must match hidden layer length, we will set a default dropout
# and not include it in the hyperparameter space (keeps models from failing due to mismatched lengths)

search_criteria <- list(
  strategy = "RandomDiscrete",
  max_models = 30,    # change to 40-60 if you want a longer run (4-5 hours) — careful with RAM
  seed = 123,
  max_runtime_secs = 60*60*4)  # optional: cap runtime to ~4 hours (comment out if you prefer model count)

```

```{r h2o-grid-search}
# Launch grid search using class weights (weights_column) ---------------------
#    Note: epochs increased to allow deeper training; early stopping will limit wasted epochs.


# grid_id <- "h2o_nn_balanced_grid"
# 
# grid <- h2o.grid(algorithm = "deeplearning",
#                  grid_id = grid_id,
#                  x = predictor_cols,
#                  y = target,
#                  training_frame = train_h2o_split,
#                  validation_frame = valid_h2o_split,
#                  # Use the weights column computed earlier:
#                  # weights_column = "class_weights",
#                  # # Make H2O use classes weighting; setting balance_classes=FALSE because we supply weights explicitly
#                  balance_classes = T,
#                  # Training control
#                  epochs = 200,                       # allow many epochs (early stopping will truncate)
#                  stopping_metric = "logloss",
#                  stopping_tolerance = 1e-4,
#                  stopping_rounds = 10,
#                  # optimization / regularization defaults; some are overridden by hyperparams
#                  distribution = "multinomial",
#                  #activation = "RectifierWithDropout",
#                  adaptive_rate = TRUE,
#                  # small-ish default l1/l2 (grid will explore others)
#                  l1 = 0, l2 = 0,
#                  # Use relatively small mini-batch default (H2O autodetects)
#                  reproducible = T,   # reproducible=TRUE may slow things on multi-core machines
#                  seed = 123,
#                  # hyperparameter space and search criteria:
#                  hyper_params = hyper_params,
#                  search_criteria = search_criteria)
# 
# # Review the grid search summary (sorted by validation logloss) ---------------
# grid_perf <- h2o.getGrid(grid_id = grid_id, sort_by = "logloss", decreasing = FALSE)
# save(grid_perf, file = "h2o_grid_perf")
# 
# 
# 
# 
# # Best model ID (first row in sorted grid)
# best_model_id <- as.character(grid_perf@model_ids[[1]])
# best_model    <- h2o.getModel(best_model_id)
# 
# 
# 
# dir.create("models", showWarnings = FALSE)
# 
# # Save the model into that folder
# saved_path <- h2o.saveModel(
#   object = best_model,
#   path   = "models",   # relative path, works on every machine
#   force  = TRUE
# )
# 
# cat("Model saved at:", saved_path, "\n")
# 
# 
# 
# # Find the first saved model folder inside "models/"
# model_dirs <- list.dirs("models", recursive = FALSE)



#best_model <-  h2o.loadModel("models/h2o_nn_balanced_grid_model_3") #loading h2o model
# load("h2o_grid_perf")
```

```{r get-best-model}
# # Pull the best model (lowest validation logloss) ----------------------------
# best_model_id <- grid_perf@model_ids[[1]]
# best_model <- h2o.getModel(best_model_id)
# 
# cat("Best model id:", best_model_id, "\n")
# print(best_model)
# 
# h2o.performance(best_model)
# 
# save(best_model, file = "FFN_final_model")

load("FFN_final_model")



#plot of performance

#perf.df <- as.data.frame(grid_perf@summary_table) 



# ggplot(data = perf.df, aes(x = hidden, y = logloss, colour = as.factor(input_dropout_ratio))) + 
#   geom_point(size = 2)+
#   labs(y = "logloss")
# 
# ggplot(data = perf.df, aes(x = hidden, y = logloss, colour = as.factor(l1))) + 
#   geom_point(size = 2)+
#   labs(y = "logloss")
# 
# ggplot(data = perf.df, aes(x = hidden, y = logloss, colour = as.factor(l2))) + 
#   geom_point(size = 2)+
#   labs(y = "logloss")


#h2o.performance(best_model)



# 
# ggplot(perf.df, aes(x = rate, y = logloss, color = factor(input_dropout_ratio))) +
#   geom_point() +
#   facet_wrap(~ hidden) +
#   theme_minimal() +
#   labs(title = "Learning Rate vs Logloss by Architecture & Dropout")


```

```{r best-model-eval}

# levels_y <- levels(train$OAH)
# # Evaluate the best model on the held-out test set ---------------------------
# 
# best_pred_h2o <- h2o.predict(best_model, test_h2o)   # returns 'predict' + per-class probabilities
# best_pred_df <- as.data.frame(best_pred_h2o)

# # predicted class labels (as factor)
# pred_labels_best <- factor(best_pred_df$predict)
# true_labels_best <- factor(as.vector(test_h2o[[target]]))
# 
# # confusion matrix & caret metrics
# cm_best <- confusionMatrix(pred_labels_best, true_labels_best)  #adding the  levels
# print(cm_best)
# cat("Test Accuracy (best H2O NN):", round(cm_best$overall['Accuracy'],4), "\n")
# 
# cm_best$overall
# mean(pred_labels_best==true_labels_best)
```

```{r per-class-auc}
# NEEDS DEBUGGIN!


# Compute per-class AUC (one-vs-all) using predicted probabilities ------------
# probability columns in best_pred_df must match levels_y order; H2O uses level names as columns
# prob_matrix <- as.matrix(best_pred_df[, levels_y])
# 
# auc_list <- sapply(seq_along(levels_y), function(i) {
#   cls <- levels_y[i]
#   bin_true <- as.numeric(true_labels_best == cls)
#   scores <- prob_matrix[, cls]
#   if (length(unique(bin_true)) < 2) return(NA)   # if a class not present in test
#   auc_val <- as.numeric(roc(bin_true, scores)$auc)
#   return(round(auc_val,4))
# })
# 
# names(auc_list) <- levels_y
# 
# print(auc_list)
```

```{r top-three-predicted-classes}
# NEEDS DEBUGGIN!

# # Top-K (e.g. Top-3) hit ratio - useful in practice -------------------------
# # calculate if true label is in top-3 predicted probabilities
# topk <- function(prob_row, k = 3) {
#   ord <- order(prob_row, decreasing = TRUE)
#   return(names(prob_row)[ord[1:k]])
# }
# 
# probs_df <- as.data.frame(prob_matrix)
# 
# in_topk <- mapply(function(i, true_lbl) {
#   topk_preds <- topk(as.numeric(probs_df[i, ]), k = 3)
#   true_lbl %in% topk_preds
# }, i = seq_len(nrow(probs_df)), true_lbl = as.character(true_labels_best))
# 
# 
# top3_hit_ratio <- mean(in_topk, na.rm = TRUE)
# 
# cat("Top-3 hit ratio (best model):", round(top3_hit_ratio,4), "\n")
```

```{r save-best-model}
# Save the best model to disk  -----------------------------------
# model_path <- h2o.saveModel(best_model, path = getwd(), force = TRUE)
# cat("Saved best model to:", model_path, "\n")

#------------------------------------ Notes  -----------------------------------------------------------
# - If you encounter OOM / memory errors: reduce `max_models` or reduce hidden layer sizes
#   or set max_runtime_secs in search_criteria to limit the run-time.
# - Because we used per-row weights, the model actively penalizes mistakes on rare classes.
# - If you want even stronger balancing, you can set balance_classes = TRUE as well,
#   or apply targeted over-sampling (SMOTE) on the R side before converting to H2O.
# - For an ensemble: keep a few of the top grid models and use h2o.stackedEnsemble() to combine them.
```

```{r best-model-metrics}
# ---------- Robust metrics, top-k, AUCs, calibration ----------

#Get predicted DF and true labels (from best_pred_df / test_h2o)
# If we don't have best_pred_df, compute it:
# best_pred_h2o <- h2o.predict(best_model, test_h2o)
# best_pred_df <- as.data.frame(best_pred_h2o)
# 
# pred_df <- best_pred_df   # data.frame containing "predict" and per-class probability columns named by levels_y
# true_labels <- factor(as.vector(test_h2o[[target]]))
# 
# 
# library(caret)
# cm_best <- confusionMatrix(table(pred_labels_best, true_labels_best))


# # Extract only probability columns (exclude "predict")
# prob_cols <- pred_df[, setdiff(colnames(pred_df), "predict")]
# 
# # Confusion matrix (caret)
# pred_factor <- factor(pred_df$predict)
# cm <- confusionMatrix(pred_factor, true_labels)
# print(cm)   # this is safe
```

```{r neural-net-per-class-metrices}
# # Per-class precision, recall, F1, macro-F1 (safe numeric conversions)
# byClass <- as.data.frame(cm$byClass)
# 
# # caret's byClass could have columns like 'Sensitivity','Specificity','Pos Pred Value' etc.
# # Ensure numeric columns are numeric:
# num_cols <- sapply(byClass, is.numeric)
# if (!all(num_cols)) {
#   # coerce columns that look like numbers but are factors/characters
#   for (cname in names(byClass)) {
#     byClass[[cname]] <- suppressWarnings(as.numeric(as.character(byClass[[cname]])))
#   }
# }
# 
# precision <- byClass$`Pos Pred Value`       # precision
# recall    <- byClass$Sensitivity            # recall
# f1 <- 2 * (precision * recall) / (precision + recall)
# f1[is.na(f1)] <- 0
# metrics_tbl <- data.frame(
#   class = rownames(byClass),
#   precision = round(precision,4),
#   recall = round(recall,4),
#   F1 = round(f1,4))
# 
# print(metrics_tbl)
# cat("Macro F1:", round(mean(f1, na.rm = TRUE),4), "\n")
```

```{r top-hits}
# # Top-K hit ratios (Top-1, Top-2, Top-3)
# topk_hit <- function(probs_df, true_labels, k=3) {
#   n <- nrow(probs_df)
#   hits <- logical(n)
#   for (i in seq_len(n)) {
#     row_probs <- as.numeric(probs_df[i, ])
#     names(row_probs) <- colnames(probs_df)
#     topk_names <- names(sort(row_probs, decreasing = TRUE))[1:k]
#     hits[i] <- as.character(true_labels[i]) %in% topk_names
#   }
#   mean(hits, na.rm = TRUE)
# }
# 
# cat("Top-1 hit:", round(topk_hit(prob_cols, true_labels, 1),4), "\n")
# cat("Top-2 hit:", round(topk_hit(prob_cols, true_labels, 2),4), "\n")
# cat("Top-3 hit:", round(topk_hit(prob_cols, true_labels, 3),4), "\n")
```

```{r calibration-plot-perclass}
# Calibration plot per class (reliability diagram) - example for one class
# calibration_plot_one <- function(class_label, prob_df, true_labels, n_bins = 10) {
#   scores <- prob_df[[class_label]]
#   true_bin <- as.numeric(true_labels == class_label)
#   df_cal <- data.frame(scores = scores, true = true_bin)
#   df_cal$bin <- cut(df_cal$scores, breaks = seq(0,1,length.out = n_bins+1), include.lowest = TRUE)
#   agg <- aggregate(cbind(mean_prob = scores, obs_rate = true) ~ bin, data = df_cal, FUN = mean)
#   agg$bin_mid <- (seq(0,1,length.out = n_bins+1)[-1] + seq(0,1,length.out = n_bins+1)[- (n_bins+1)])/2
#   ggplot(agg, aes(x = mean_prob, y = obs_rate)) +
#     geom_point() + geom_line() + geom_abline(intercept = 0, slope = 1, linetype = "dashed", color = "gray") +
#     labs(title = paste("Calibration for", class_label), x = "Mean predicted probability", y = "Observed frequency")
# }
# 
# # Example: plot calibration for 'Moderate' (change to any class)
# if ("Moderate" %in% colnames(prob_cols)) {
#   print(calibration_plot_one("Moderate", prob_cols, true_labels, n_bins = 10))
# }
```

```{r neural-net-save-metrics}
# Save metrics to file 
#write.csv(metrics_tbl, "h2o_nn_per_class_metrics.csv", row.names = FALSE)
```


```{r}
#| echo: FALSE

# Note from Sanana: stole this chunk and placed it in the results section

# #| label: fig-var-imp-ffnn
# #| tbl-cap: "Variable Importance for FFN-MLP."
# 
# # imp_data <- as.data.frame(h2o.varimp(best_model)) %>%
# #    mutate(variable = reorder(variable, relative_importance),
# #           rank = row_number()) %>%
# #   filter(rank <= 30)
# # 
# # save(imp_data, file = "imp_data")
# 
# 
# load("imp_data")
# #h2o.varimp_plot(best_model, num_of_features = 50)
# 
# ggplot(data = imp_data, aes(x = relative_importance, y = variable, fill = relative_importance))+
#   geom_col()+
#   labs(
#        fill = "Relative Importance",
#        x = "Relative Importance")
```

## Results and Discussion

### Feature Importance

In @fig-var-imp-ffnn, we have the feature importance plot. We used the importance plot derived from the FFNN since this was our most balanced model (to be discussed in more detail in the subsections to follow). We believe it best captures the multi-factorial nature of this problem and gives more reliable insights into which features are most influential in avalanche forecasting.

The FAH variable clearly dominates, as FAH.Moderate, FAH.Low, FAH.Considerable-, and FAH.Considerable+ are the most influential predictors with the highest relative importance values. This shows that avalanche hazard categories strongly drive the model’s predictions. Altitude (Alt) also plays a major role, ranking just below the FAH features, which highlights elevation as a critical factor in avalanche outcomes.

Snow index comes in at a moderate level of importance. It has predictive power, but not as strong as FAH. Certain seasons like season12, season5, and season3 also show moderate importance. This suggests seasonal trends do matter, but no single season dominates the prediction. If avalanches were restricted to one specific time/season window, we would have seen that season stand out much more.

For the features with lower but still potentially useful importance, we have geographic area, specifically Area.Torridon and Area.Glencoe. Here, they contribute only a little. This suggests that spatial differences matter less, especially compared to the strong influence of FAH. Lastly, precipitation ranks the lowest. A detailed breakdown of precipitation types adds little predictive power beyond snow index, which is clearly more influential.

```{r}
#| echo: FALSE
#| label: fig-var-imp-ffnn
#| tbl-cap: "Variable Importance for FFN-MLP."

# imp_data <- as.data.frame(h2o.varimp(best_model)) %>%
#    mutate(variable = reorder(variable, relative_importance),
#           rank = row_number()) %>%
#   filter(rank <= 30)
# 
# save(imp_data, file = "imp_data")


load("imp_data")
#h2o.varimp_plot(best_model, num_of_features = 50)

ggplot(data = imp_data, aes(x = relative_importance, y = variable, fill = relative_importance))+
  geom_col()+
  labs(
       fill = "Relative Importance",
       x = "Relative Importance")
```

Overall, the FAH values are by far the most important predictors, followed by altitude and snow index. Seasonal and spatial attributes matter too but they are less powerful in comparison, while precipitation types add little value to predictive performance gains.


```{r sanana_work}

#Hi Sanana ! :)

#Here  are all the  model  and prediction objects


#Below are some workings on the prediction objects. Skip past to the below banner that says HERE



#pred_ffn_labels <- best_pred_df[,"predict"]
# pred_cnn
# pred_rnn

 true_labels <- test$OAH
# pred_ffn_labels <- factor(pred_ffn_labels, levels = levels(true_labels))
# save(pred_ffn_labels, file = "pred_ffn_labels")
# save(true_labels, file = "true_labels")

load("pred_ffn_labels")
load("true_labels")


# Map integers back to labels

load("pred_cnn")
load("pred_rnn")

pred_cnn_labels <- factor(levels(true_labels)[pred_cnn],
                   levels = levels(true_labels))



pred_rnn_labels <- factor(levels(true_labels)[pred_rnn],
                   levels = levels(true_labels))



#Labels for CNNN and RNN

#Test set changes for each one, because different windows were used


load("temporal_labels_cnn")
load("temporal_labels_rnn")

temporal_labels_rnn <- factor(levels(true_labels)[temporal_labels_rnn],
                   levels = levels(true_labels))

temporal_labels_cnn <- factor(levels(true_labels)[temporal_labels_cnn],
                   levels = levels(true_labels))

# head(temporal_labels_cnn)
# head(pred_cnn_labels)
# 
# save(temporal_labels_rnn, file = "temporal_labels_rnn")
# save(temporal_labels_cnn, file = "temporal_labels_cnn")


#---------------------------------------------------HERE-------------------------------------------------------

#pred_rnn_labels <---- predictions for RNN loaded above
#pred_cnn_labels <---- predictions for cnn loaded above
#pred_ffn_labels <---- predictions for ffn loaded above


# mean(pred_rnn_labels == temporal_labels_rnn)
# mean(pred_cnn_labels == temporal_labels_cnn)

#temporal_labels <----- test set labels for cnn and rnn
#true_labels <----------- test set for ffn (randomly shuffled)




```

### Confusion Matrices

```{r, echo = FALSE}
# Getting FFN in order
# pred_ffn_labels <- best_pred_df[,"predict"]

true_labels <- test$OAH
pred_ffn_labels <- factor(pred_ffn_labels, levels = levels(true_labels))
ffn_cm <- confusionMatrix(pred_ffn_labels, true_labels)

# ----------------------------------------------------------
# Getting CNN and RNN in order
# pred_cnn_labels
# pred_rnn_labels

# Test set labels for cnn and rnn


# Checking
# class(pred_cnn_labels)
# class(pred_rnn_labels)
# #class(temporal_labels)
# 
# length(pred_cnn_labels) # Why are you longer :/
# length(pred_rnn_labels)
#length(temporal_labels)

# "Fixing" CNN preds
#pred_cnn_labels <- pred_cnn_labels[1:length(temporal_labels)]

# Create the confusion matrices
cnn_cm <- confusionMatrix(pred_cnn_labels, temporal_labels_cnn)
rnn_cm <- confusionMatrix(pred_rnn_labels, temporal_labels_rnn)


acc_rnn <- round(mean(pred_rnn_labels == temporal_labels_rnn) * 100)
acc_ffn <- round(mean(pred_ffn_labels == true_labels) *100)
```

@fig-compare_cms displays the confusion matrices for our 3 models. Our FFNN-MLP has a clearer diagonal, which is indicative of its stronger performance. The confusion matrix for the RNN is very diffused with widespread misclassifications. @tbl-overall_stats confirms this observation is indeed the case, as the RNN had the lowest accuracy (`r acc_rnn`%) and the FFNN-MLP had the highest (`r acc_ffn`%). One limitation of the approach for evaluating the CNN and RNN is that, since the training and tests sets are not shuffled, and "High" risk avalanches are rare, there are no "High" risk avalanches in the test set. Therefore, the models ability to distinguish "High" risk cases is not measured well beyond.

The diagonal of the RNN is the weakest of them all. Its correct predictions are concentrated in "Low" (752) with spillover to "Moderate" (165), and its misclassifications are concentrated in "Low" (197) and "Moderate" (299) predictions which were actually "Moderate" instances. This tells us the model often confused "Moderate" cases as either "Low" (underestimated hazard) or "Moderate" (overestimated hazard).

In the case of the CNN, almost all predictions go to "Low". There is a noticeable concentration on the diagonal for "Low" classifications (735), and a considerable concentration of "Moderate" classifications (270). On the off-diagonal, the model misclassified 205 "Low" instances as "Moderate", and 148 "Moderate" instances as "Low". Clearly, the CNN collapsed into predicting these two classes in many cases and often confused the two classes. The rows for these respective classes are all zeros. This shows that this model fell for imbalance bias, favouring the majority class "Low" and second most abundance "Moderate".

The FFNN-MLP had some noticeable misclassifications between "Considerable-" and "Moderate". We see that for 103 cases, the model predicted "Considerable-" instead of "Moderate", which shows that it confuses those two classes a lot. Similarly, 112 Low cases were predicted as "Moderate", another pair of classes the model struggles with.

```{r fig-compare_cms, echo = FALSE}
#| fig-cap: "Confusion Matrices Comparing the Final CNN, RNN, and Feed-Forward Models"

library(gridExtra)

plot_cm <- function(cm, title) {
  cm_table <- as.data.frame(cm$table)
  ggplot(cm_table, aes(x = Reference, y = Prediction, fill = Freq)) +
    geom_tile() +
    geom_text(aes(label = Freq), color = "coral") +
    scale_fill_gradient(low = "white", high = "steelblue") +
    labs(title = title, x = "Actual", y = "Predicted") +
    theme_minimal()
}


p1 <- plot_cm(cnn_cm, "CNN Confusion Matrix")
p2 <- plot_cm(rnn_cm, "RNN Confusion Matrix")
p3 <- plot_cm(ffn_cm, "Feed-Forward NN Confusion Matrix")

grid.arrange(p1, p2, p3, nrow = 3)
```

```{r tbl-accuracy_comparison, eval = FALSE, echo = FALSE}
# # Accuracy table summary
# accuracy_tbl <- data.frame(
#   Model = c("FFNN", "CNN", "RNN"),
#   Accuracy = paste0(
#     round(c(
#       ffn_cm$overall["Accuracy"],
#       cnn_cm$overall["Accuracy"],
#       rnn_cm$overall["Accuracy"]
#     ) * 100, 1), "%")
# )
# 
# kable(accuracy_tbl, caption = "Overall Accuracy by Model") %>%
#   kable_styling(full_width = FALSE)
```

### Overall Model Performance

In @tbl-overall_stats below, we have an overview of each model's performance. The overall key insights that can be derived are:

-   The FFNN-MLP clearly dominates with the highest accuracy, recall, specificity, and precision scores. It would be the best model for both detecting and correctly rejecting classes.
-   The CNN serves as a middle ground, with acceptable accuracy but low recall. This means this model misses many real cases of considerable avalanche hazards.
-   The RNN fails overall. Its accuracy, recall and precision metrics are all low, making it unreliable for classification.

```{r tbl-overall_stats, echo = FALSE}
get_overall_stats <- function(cm_obj) {
  data.frame(
    Accuracy     = round(cm_obj$overall["Accuracy"], 3),
    Kappa        = round(cm_obj$overall["Kappa"], 3),
    Recall  = round(mean(cm_obj$byClass[,"Sensitivity"], na.rm = TRUE), 3),
    Specificity  = round(mean(cm_obj$byClass[,"Specificity"], na.rm = TRUE), 3),
    Precision    = round(mean(cm_obj$byClass[,"Pos Pred Value"], na.rm = TRUE), 3)
    )
}

# Build comparison table
model_comparison <- rbind(
  FFNN = get_overall_stats(ffn_cm),
  CNN = get_overall_stats(cnn_cm),
  RNN = get_overall_stats(rnn_cm)
)

kable(as.data.frame(model_comparison), caption = "Overall Model performance Statistics") %>% kable_styling()
```

The FFNN-MLP's specificity was 0.934, the highest value observed. This tells us that this model is very good at ruling out cases that do not belong to a class. Its recall was 0.618, which means the model correctly identifies \~62% of true cases or in other words, it catches more actual danger levels compared to CNN and RNN. The CNN and RNN specificity scores were 0.880 and 0.849 respectively, which indicates they have fair ability to avoid false alarms of avalanche hazards, but not up to par with the FFNN-MLP's ability. Notably, the RNN's precision was 0.311, which means when it predicts, its only correct about 31% of the time. This means the RNN is wrong more often than its not. Contrast that with the CNN's 0.439 precision (correct a little under half the time) and the FFNN-MLP's 0.633 (correct around 63% of the time).

### Model Statistics by Class

Looking at performance statistics by class can help us gain useful insight into each models strengths and weaknesses, providing a more detailed overview of model behaviour. @tbl-ffn_stats_by_class, @tbl-cnn_stats_by_class and @tbl-rnn_stats_by_class summarise our model statistics by class.

```{r tbl-cnn_stats_by_class, echo = FALSE}

kable(as.data.frame(cnn_cm$byClass), caption = "CNN Statistics by Class") %>% kable_styling()
```

```{r tbl-rnn_stats_by_class, echo = FALSE}

kable(as.data.frame(rnn_cm$byClass), caption = "RNN Statistics by Class") %>% kable_styling()
```

```{r tbl-ffn_stats_by_class, echo = FALSE}

kable(as.data.frame(ffn_cm$byClass), caption = "FFNN Statistics by Class") %>% kable_styling()
```

The CNN statistics detailed in @tbl-cnn_stats_by_class show that this model had mixed results. It was able to capture 82% of "Low" danger cases, which is the majority class, but its performance dropped sharply for the minority cases. Additionally, it only detected 3.5% of "Considerable -" cases.

Even on a class-by-class basis, the RNN performed poorly overall. It was able to catch about 79% of "Low" hazard cases. Beyond that, its performance collapsed. It missed all "Considerable -" cases (0 sensitivity overall). It only managed to detect 35% of "Moderate" cases. Moreover, its specificity for the "Low" class was just 52%, meaning it frequently mislabelled other classes as "Low".

However, the FFNN-MLP (@tbl-ffn_stats_by_class) was especially good at recognising the common classes such as "Low" and "Moderate" danger, correctly identifying around 82% of "Low" cases and 75% of "Moderate" cases. This means that it both caught most of the true cases and rarely mislabelled other classes as these. However, the FFNN-MLP struggled with the rarer categories. For example, it only detected 18% of "Considerable +" cases. This suggests some model bias towards classes that were better represented.

### Summary of Findings

In this study, we aimed to compare the ability of CNN, RNN, and FFNN-MLP models in predicting avalanches hazard levels. This is a multi-factorial investigation, as many different variables contribute to avalanche outcomes. Because the nature of this problem is sequential and we were able to preprocess that data to fit a time series context, we expected the CNN and RNN to outperform the FFNN-MLP since they have the added time series advantage.

The FFNN-MLP turned out to be the best-performing model overall. It showed the most balanced detection and was the only one that managed to give acceptable performance across hazard levels. The CNN struggled with the minority classes and mainly acted as a majority-class ("Low" hazard) detector despite the controls put in place to re-weight the classes. This is unsatisfactory because in a real forecasting setting, it's of great importance that we can detect the higher-risk cases. The RNN underperformed across the board, missing nearly all the minority class instances ("Considerable +", "Considerable -"), and doing *fairly* well with the majority cases but still not as well as the other two models. This strongly disagrees with our initial hypothesis that sequence models would benefit from time series dependencies.

### Strenghts and Weaknesses

A clear strength across all models is that they handled the majority class fairly well. This suggests that with a more balanced dataset where all classes have enough representation, similar architectures could perform much better. On the other hand, all three models showed clear bias toward the most common categories. None of them did especially well with the higher-risk, minority classes. The highlights a weakness in applying these models directly without addressing the class imbalance and customising the class weights more in favour of the minority classes could help counteract this measure.

### Improvements & Future Contributions

Another limitation of the present study is that largely complete cases were used in the data set. However, in practice avalanche forecasters do not have such luxuries. Forecasts have to be made regardless of whether some data points are missing. At the beginning of the study, we considered imputation, although we agreed that this would be beyond the scope of the aims of evaluating neural network performance. Developing an imputation model for this dataset could be a way of meaningfully improving its practical use, robustness, and accuracy overall.

There are several improvements that can be made going forward:

1.  Directly addressing the class imbalance by further exploring data generation or sampling techniques to help the models learn more about the high-risk hazard levels that are rare in the current dataset.
2.  Literature shows that climate change is altering avalanche formation and risk. This means that future models need to take into account not just the historical data, but the shifting weather and snowpack patterns.
3.  Scotland has very localised weather conditions where patterns in the eastern region can differ greatly from the west. Building separate models for different regions or microclimates may improve local accuracy and forecast reliability.
4.  Exploring alternative model types could strengthen predictions. Ensemble techniques such as Gradient Boosting or hybrid deep learning ensembles could potentially handle the categorical nature of this data better than the implemented architectures.

## Conclusion

Our objective requires reliable detection across all avalanche danger levels, not just the majority ones. In this study, the FFNN-MLP came closest to meeting this requirement, but even it struggled with the minority, high-risk cases. Future work should focus on reducing class imbalance, exploring region-specific models, and experimenting with ensemble approaches to ensure that high-risk avalanche conditions are consistently and accurately detected.

## References

1.  Bain, D. 2024. *Avalanche awareness in Scotland*. Sports Scotland Glenmore Lodge. Available: <https://www.glenmorelodge.org.uk/avalanche-awareness-in-scotland/> \[2025, September 11\].

2.  Beattie, B. 1976. The densification of a seasonal snowpack in the Cairngorms with relation to avalanches. B.Sc. (professional) dissertation. University of Lancaster.

3.  Blagovechshenskiy, V., Medeu, A., Gulyayeva, T., Zhdanov, V., Ranova, S., Kamalbekova, A. & Aldabergen, U. 2023. Application of artificial intelligence in the assessment and forecast of avalanche danger in the Ile Alatau Ridge. *Water*. 15(7). DOI: 10.3390/w15071438

4.  Britannica. 2026. *Ben Nevis*. Available: <https://www.britannica.com/place/Ben-Nevis> \[2025, September 10\].

5.  Choubin, B., Borji, M., Mosavi, A., Sajedi-Hosseini, F., Singh, V.P. & Shamshirband, S. 2019. Snow avalanche hazard prediction using machine learning methods. *Journal of Hydrology*. 577. DOI: 10.1016/j.jhydrol.2019.123929

6.  Diggins, M. 2009. The challenges for Scottish avalanche forecasters observing a maritime snowpack. *Proceedings of the International Snow Science Workshop*. 22-24. Available: <https://arc.lib.montana.edu/snow-science/objects/issw-2009-0024-0026.pdf> \[2025, September 10\].

7.  Fromm, R. & Schonberger, C. 2022. Estimating the danger of snow avalanches with machine learning approach using a comprehensive snow cover model. *Machine Learning with Applications*. 10. DOI: 10.1016/j.mlwa.2022.100405

8.  Gauthier, F., Laliberte, J. & Meloche, F. 2025. Assessing the predictive capability of several machine learning algorithms to forecast snow avalanches using numerical weather prediction model in eastern Canada. *EGUsphere*. DOI: 10.5194/egusphere-2025-1572

9.  Heierli, J., Purves, R.S., Felber, A. & Kowalski, J. 2004. Verification of nearest-neighbours interpretations in avalanche forecasting. *Annals of Glaciology*. 38:84-88. DOI: 10.3189/172756404781815095

10. Helbig, N., van Herwijnen, A. & Jonas, T. 2015. Forecasting wet-snow avalanche probability in mountainous terrain. *Cold Regions Science and Technology*. 120:219-226. DOI: 10.1016/j.coldregions.2015.07.001

11. Hendrick, M., Techel, F., Volpi, M., Olvski, T., Perez-Guillen, van Herwijnen, A. & Schweizer, J. 2023. Automated prediction of wet-snow avalanche activity in the Swiss Alps. *Journal of Glaciology*. 69(277):1365-1378. DOI: 10.1017/jog.2023.24

12. Kala, M., Jain, S., Singh, A. & Krishnan, N.C. 2025. Addressing class imbalance in avalanche forecasting. *Cold Regions Science and Technology*. 231. DOI: 10.1016/j.coldregions.2024.104411

13. Langmuir, E. 1970. Snow profiles in Scotland. *Weather*. 25(5):205-209. DOI: 10.1002/j.1477-8696.1970.tb03262.x

14. MetOffice. 2010. *UK regional climates*. Available: <https://www.metoffice.gov.uk/research/climate/maps-and-data/regional-climates/index> \[2025, September 11\].

15. OpenAI. (2025a). ChatGPT (Sept 23 version) \[Large language model\]. https://chatgpt.com/share/68d59a8c-efa4-8004-a295-616e83f37afb

16. OpenAI. (2025b). ChatGPT (Sept 23 version) \[Large language model\]. https://chatgpt.com/share/68d59ad3-d50c-8004-b862-18966277b186

17. Pozdnoukhov, A., Purves, R.S. & Kanevski, M. 2008. Applying machine learning methods to avalanche forecasting. *Annals of Glaciology*. 49:107-113. DOI: 10.3189/172756408787814870

18. Pozdnoukhov, A., Matasci, G., Kanevski, M. & Purves, R.S. 2011. Spatio-temporal avalanche forecasting with Support Vector Machines. *Nat. Hazards Earth Syst. Sci.* 11:367-382. DOI: 10.5194/nhess-11-367-2011

19. Purves, R.S., Morrison, K.W., Moss, G. & Wright, D.S.B. Nearest neighbours for avalanche forecasting in Scotland - Development, verification and optimisation of a model. *Cold Regions Science and Technology*. 37(3):343-355. DOI: 10.1016/S0165-232X(03)00075-2

20. Rahmati, O., Ghorbanzadeh, O., Teimurian, T., Mohammadi, F., Tiefenbacher, J.P., Falah, F., Pirasteh, S., Ngo, P.T.T. & Bui, D.T. 2019. Spatial modeling of snow avalanche using machine learning models and geo-environmental factors: Comparison of effectiveness in two mountain regions. *Remote Sensing*. 11(24). DOI: 10.3390/rs11242995

21. Schirmer, M., Lehning, M. & Schweizer, J. 2009. Statistical forecasting of regional avalanche danger using simulated snow-cover data. *Journal of Glaciology*. 55(193):761-768. DOI: 10.3189/002214309790152429

22. Scottish Avalanche Information Service. 2024. *Reviews of the winter season*. Available: <https://www.sais.gov.uk/sais-annual-reports/> \[2025, September 13\].

23. Scottish Avalanche Information Service. 2025. *Avalanche information for the Scottish mountains*. Available: <https://www.sais.gov.uk/> \[2025, September 5\].

24. Scottish Government. 2011. *Scotland's marine atlas: Information for the National Marine Plan*. Available: <https://www.gov.scot/publications/scotlands-marine-atlas-information-national-marine-plan/pages/7/> \[2025, September, 10\].

25. Sharma, V., Kumar, S. & Sushil, R. 2023. A neural network model for automated prediction of avalanche danger level. *Nat. Hazards Earth Syst. Sci.* 23:2523-2530. DOI: 10.5194/nhess-23-2523-2023

26. Singh, A. & Ganju, A. 2008. Artificial neural networks for snow avalanche forecasting in Indian Himalaya. *Proceedings of the 12th International Conference of International Association for Computer Methods and Advances in Geomechanics (IACMAG)*. 1-6 October 2008. Gao, India. 1664-1670. Available: <https://www.researchgate.net/publication/263651736_Artificial_Neural_Networks_for_Snow_Avalanche_Forecasting_in_Indian_Himalaya> \[2025, September 13\].

27. Spink, P.C. 1970. Scottish snowbeds in summer 1969. *Weather*. 25(5): 201-204. DOI: 10.1002/j.1477-8696.1970.tb03261.x

28. Tiwari, A., Arun, G. & Vishwakarma, B.D. 2021. Parameter importance assessment improves efficacy of machine learning methods for predicting snow avalanche sites in Leh-Manali Highway, India. *Science of The Total Environment*. 794. DOI: 10.1016/j.scitotenv.2021.148738

29. Tu, J.V. 1996. Advantages and disadvantages of using artificial neural network versus logistic regression for predicting medical outcomes. *Journal of Clinical Epidemiology*. 49(11):1225-1231. DOI: 10.1016/S0895-4356(96)00002-9

30. Ward, R.G.W. 1980. Avalanche hazard in the Cairngorm Mountains, Scotland. *Journal of Glaciology*. 26(94):31-41. DOI: 10.3189/S0022143000201020

31. Ward, R.G.W. 1984a. Avalanche prediction in Scotland: I. A survey of avalanche activity. *Applied Geography*. 4(2):91-108. DOI: 10.1016/0143-6228(84)90016-X

32. Ward, R.G.W. 1984b. Avalanche prediction in Scotland: II. Development of a predictive model. *Applied Geography*. 4(2):109-133. DOI: 10.1016/0143-6228(84)90017-1

33. Ward, R.G.W., Langmuir, E.D.G. & Beattie, B. 1985. Snow profiles and avalanche activity in the Cairngorm Mountains, Scotland. *Journal of Glaciology*. 31(107):18-27. DOI: 10.3189/S0022143000004949

34. Webster, H. 2020. *Avalanches in Scotland - interview with Mark Diggins*. Walkhighlands. Available: <https://www.walkhighlands.co.uk/news/avalanches-in-scotland-interview-with-mark-diggins/> \[2025, September 10\].

35. Wen, H., Wu, X., Liao, X., Wang, D., Huang, K. & Wunnemann, B. 2022. Application of machine learning methods for snow avalanche susceptibility mapping in the Parlung Tsangpo catchement, southeastern Qinghai-Tibet Plateau. *Cold Regions Science and Technology*. 198. DOI: 10.1016/j.coldregions.2022.103535

36. Werritty, A. & Sugden, D. 2012. Climate change and Scotland: Recent trends and impacts. *Earth and Environmental Science Transactions of The Royal Society of Edinburgh*. 103(2):133-147. DOI: 10.1017/S1755691013000030

37. WSL Institute for Snow and Avalanche Research SLF. 2016. *Scotland - Avalanche warning experts exchange*. Available: <https://www.slf.ch/en/news/scotland-avalanche-warning-experts-exchange/?utm_source=chatgpt.com> \[2025, September 12\].
