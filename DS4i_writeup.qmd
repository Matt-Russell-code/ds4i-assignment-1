---
title: "DS4I - Project Writeup"
format: html
execute:
  echo: false
  warning: false
  message: false
  results: hide
editor: visual
---

```{r setup}


#libraries 

library(kableExtra)
library(tidyverse)
library(ggplot2)
library(ggpubr)

#theme for ggplot2, let me know if you do not like this theme and we can change it :)

#Theme for ggplot2 
theme_set(
  theme_bw(base_size = 9) +
    theme(
      plot.title   = element_text(hjust = 0.5, size = 9),
      panel.border = element_rect(colour = "black", fill = NA, linewidth = 0.5),
      # keep grids but make them subtle
      panel.grid.major = element_line(colour = "grey85", linewidth = 0.3),
      panel.grid.minor = element_line(colour = "grey92", linewidth = 0.2),
      # base-R style text/ticks
      axis.text  = element_text(colour = "black"),
      axis.title = element_text(colour = "black"),
      axis.ticks = element_line(colour = "black", linewidth = 0.3)
    )
)



#making geompoint have hollow circles
update_geom_defaults(
  "point",
  list(size = 1, alpha = 0.9, shape = 21, colour = "black")
)


```

```{r}
#| echo: false
knitr::include_graphics("av.jpg")

```

[Photo by Nicolas Cool on Unsplash]{style="font-size:12px"}

## Introduction

## Literature Review

## Exploratory Data Analysis

```{r}



```

## Feature Engineering

```{r read_data}

library(lubridate)
data = read.csv("scotland_avalanche_forecasts_2009_2025.csv")


data <- data %>% 
  mutate(across(where(is.character), as.factor))

```

```{r missing_val}

#Getting table of missing values


col.n <- colnames(data)
n <- nrow(data)

missing_df <- data.frame(Variable = character(),
                         "Missing (%)" = numeric(),
                         check.names = F)


for (i in col.n){
  
  x <- sum(is.na(data[,i]))
  
  perc <- (x/n) * 100
  
  missing_df <- rbind(missing_df,
                      data.frame(
                        Variable = i,
                        `Missing (%)` = perc
                      , check.names = F)
                      )
  
}
 
missing_df <- missing_df %>% 
  filter(`Missing (%)` > 0) %>% 
  arrange(desc(`Missing (%)`),)


```

```{r tablex}



kable(missing_df, 
      format = "html",
      digits = 2,
      caption = "Missing Values") %>%
  kable_styling(
    bootstrap_options = c("hover"),
    full_width = FALSE
  ) %>%
  footnote(general = "")


```

```{r check_missing_fnx}

# a few functions to help me evaluate the extent of missing values

#Find percentage of missing values per Column
find_missing <- function(a){
  
  
col.n <- colnames(a)
n <- nrow(a)
missing_df <- data.frame(Variable = character(),
                         "Missing (%)" = numeric(),
                         check.names = F)


for (i in col.n){
  
  x <- sum(is.na(a[,i]))
  
  perc <- (x/n) * 100
  
  missing_df <- rbind(missing_df,
                      data.frame(
                        Variable = i,
                        `Missing (%)` = perc
                      , check.names = F)
                      )
  
}
 
missing_df <- missing_df %>% 
  filter(`Missing (%)` > 0) %>% 
  arrange(desc(`Missing (%)`),)

return(missing_df)

}

#Find total % missing values
find_total_missing <- function(b){
  
  
  count_na <- 0
for (i in 1:nrow(b)){
  if (any(is.na(data[i, ]))){count_na <- count_na +1}
}


count_na/nrow(b)
  
  return(count_na*100/nrow(b) )
}

```

```{r imputation}
#Come back to this later

# 
# #I am not grouping by Date, OsGrid, Obs or OAH (As thats our target variable)
# groupings <- groupings[-c(1, 3, 5, 7)]
# 
# #Cell mean imputation
# 
# #Imputing Training data first
# train.imp <- train %>%
#   group_by(Area, FAH) %>% #cant group by precip code, or location, too many missing
#   mutate(
#     across(where(is.integer),
#            ~ replace_na(., as.integer(round(mean(., na.rm = TRUE))))),
#     across(where(is.double),
#            ~ replace_na(., mean(., na.rm = TRUE)))
#   ) %>% #after this, some variables 
#   ungroup() %>% 
#   mutate(Aspect = round(mean(Aspect, na.rm = T),), #remaining, just taking the mean
#          Max.Temp.Grad = round(mean(Max.Temp.Grad, na.rm = T)),
#          Max.Hardness.Grad = round(mean(Max.Hardness.Grad, na.rm  = T))) 
# 
# train.imp <- train.imp[-which(is.na(train.imp$OAH)),] #removing remaining missing
#   
# 
# #Imputing Test data in the same manner
# test.imp <- test %>%
#   group_by(Area, FAH) %>% #cant group by precip code, or location, too many missing
#   mutate(
#     across(where(is.integer),
#            ~ replace_na(., as.integer(round(mean(., na.rm = TRUE))))),
#     across(where(is.double),
#            ~ replace_na(., mean(., na.rm = TRUE)))
#   ) %>% #after this, some variables 
#   ungroup() %>% 
#   mutate(Aspect = round(mean(Aspect, na.rm = T),), #remaining, just taking the mean
#          Max.Temp.Grad = round(mean(Max.Temp.Grad, na.rm = T)),
#          Max.Hardness.Grad = round(mean(Max.Hardness.Grad, na.rm  = T))) 
# 
# test.imp <- train.imp[-which(is.na(test.imp$OAH)),] #removing remaining missing


```

```{r date_time}

#adding date variable
data$Date <- as.POSIXct(data$Date, format="%Y-%m-%d %H:%M:%S", tz="UTC")


data$month     <- lubridate::month(data$Date)
data$hour      <- lubridate::hour(data$Date)
data$year      <- lubridate::year(data$Date)

#make a day of the season variable

#season begins from December to april



data <- data %>% 
  mutate(season_year = if_else(month >= 12, year, year -1),
         season_id = dense_rank(season_year),
         season = paste0("season", season_id),
         season = as.factor(season)) %>% 
  group_by(season, Area) %>% 
  mutate(dayofseason = row_number()) %>% 
  arrange(Date, Area)



```

```{r drop variables}

#Drop the top 3 variables wtih the most missing values
data <- data %>% 
  select(- c(AV.Cat, Ski.Pen, Summit.Wind.Dir, Obs, Location))

data <- na.omit(data) #remove remaining missing

#data <- data[,!(names(data)) == "Obs"]



```

```{r split_data}

set.seed(1)
train.idx <- sample(1:nrow(data), 
                    round(nrow(data)*0.8))

train <- data[train.idx,] #Training data
test <- data[-train.idx,] #Testing Data


```

```{r dimension_reduction}





```

```{r keras_matthew_feedforward}



```

```{r Keras_Matthew}
#considerations for future

# justifying our dropping of Missing values ie columns and others
# Justifying our window length
# Looking at factor columns with a huge amount of categories

library("keras3")
library(tensorflow)

#for this to work I need to convert the data into an array 
# form is (N (length), L (Time frame), F (Features))

#I promise that this code isnt made by Chatgpt I am just using a lot of comments otherwise I dont know what the #$!* is going on :)


#--------Data Preparation-------------------------------------

T <- length(data$Date) #time period length

data_nn <- data %>% 
  select(-c(Date)) #removing date column, OSgrid, location


L <- 3 #Rolling window length = 3. So what I am decided is that It can use the previus 3 days on record to train itself
H <- 1 # predict 1 day ahead


#number of samples we can have, so number of time slices, each one iterated one step ahead by H (which is just 1 day in our case)
N_samp <- T - L - H + 1

#BRain is broken, but this is what needs to happen, remove OAH and put into vector, one hot enocode x variables and fix the rest wrt Y = OAH and so on

#----------------One hot encoding--------

#This part is quite a process. So big picture, the numeric data needs to be standardised, but when we do one-hot-encoding, it becomes quite messy to avoid standardising the dummy variables. SO we are going to make indices for the numeric columns so we can standardise only them later

#Taking our predictors and covariates into one df, and our labels into another

X.variables <- data_nn %>% 
  select(-OAH)   #x_variables = everything in data other than OAH column
  
X.variables <- model.matrix(~., data = X.variables)[,-1] # One Hot encoding

y.variable <- data_nn[["OAH"]]  #y_variable = only our OAH column

numeric.variables <- names(data_nn)[sapply(data_nn, is.numeric)]
numeric.variables <- setdiff(numeric.variables, "OAH")

numeric.indexes <- which(colnames(X.variables) %in% numeric.variables) #okay so the colnames are the same so we can find the indexes in x variables which are numeric with this code <--

#dummy variable indexes
dummy.indexes <- setdiff(1:(ncol(X.variables)), numeric.indexes)


#Number features

F_ <- ncol(X.variables)

#--------------Making Array---------------------#


#Arrays - this is the confusing part. Keras needs an array for this to work. In our case we can think of this as turning our data into a stack of data sheets. each sheet is for a particular feature. THen there are N_sampl columns, and each is shifted by L

X <- array( NA_real_, #just an empty array, which will subsequently be filled
           dim = c(N_samp,L, F_  ),
           )

y <- integer(N_samp)






for (i in 1:N_samp){ #here we are filling in the array with our data
  
  index.window <- i:(i + L - 1) #index wind = ith value in N_samp to I + window minus 1
  
  X[i, , ] <- X.variables[index.window, , drop = F] #filling in the predictors with the corresponding indexes into that window
  y[i] <- y.variable[i + L + H -1] #we predict the ith + L + H y values.
  
}



#------------Training and testing split-----------------------------

#We want to preserve time series patterns, so no shuffling 
idx <- 1:N_samp
number.training <- floor(N_samp * 0.8) #train = first 80% of features

tr <- idx[1:number.training] #Training indices
te <- idx[(number.training + 1):N_samp] #Testing indices

X.train <- X[tr, , , drop = F] #training predictors data array
X.test <- X[te, , , drop = F ] #testing predictors array
y.train <- y[tr] #training target
y.test <- y[te]




#----------------Scaling--------------------

#Remeber that we made indexes for our numeric data, which is the data we want to scale. 

#We also need to scale the training data, and scale the testing data according to the training data mu and sd


if (length(numeric.indexes) > 0){
  
  mu.numeric <- apply(X.train[, , numeric.indexes, drop = F], 3, mean, na.rm = T)
  
  sd.numeric <- apply(X.train[ , , numeric.indexes, drop = F], 3, sd, na.rm = T)
  
   sd.numeric[sd.numeric == 0 | is.na(sd.numeric)] <- 1 #make sure SD == 1
  
  for (j in seq_along(numeric.indexes)){
    
    k <- numeric.indexes[j]
    X.train[, , k] <- (X.train[ , , k] - mu.numeric[j])/sd.numeric[j]
    X.test[, , k] <- (X.test[ , , k] - mu.numeric[j])/sd.numeric[j]
  }
  
  
}

# Numeric cols should ne ~N(0,1)
round(apply(X.train[ , , numeric.indexes, drop = FALSE], 3, mean), 3)
round(apply(X.train[ , , numeric.indexes, drop = FALSE], 3, sd),   3)



#Getting there !

#Since we are doing a multiclass prediction we need to use softmax
#This means using the tocategorical() function on y

# y.test <- to_categorical(y.test)
# save(y.test, file = "y_cat")
# 
# y.train <- to_categorical(y.train)
# save(y.train, file = "y_train" )
# 

load("y_train")
load("y_cat")
```

```{r}



#Function to let me test for numerous window sizes

# 
# pre_data <- function(w = 3, h = 1){
# 
# #--------Data Preparation-------------------------------------
# 
# T <- length(data$Date) #time period length
# 
# data_nn <- data %>%
#   select(-c(Date)) #removing date column, OSgrid, location
# 
# 
# L <- w #Rolling window length = 3. So what I am decided is that It can use the previus 3 days on record to train itself
# H <- h # predict 1 day ahead
# 
# 
# #number of samples we can have, so number of time slices, each one iterated one step ahead by H (which is just 1 day in our case)
# N_samp <- T - L - H + 1
# 
# #BRain is broken, but this is what needs to happen, remove OAH and put into vector, one hot enocode x variables and fix the rest wrt Y = OAH and so on
# 
# #----------------One hot encoding--------
# 
# #This part is quite a process. So big picture, the numeric data needs to be standardised, but when we do one-hot-encoding, it becomes quite messy to avoid standardising the dummy variables. SO we are going to make indices for the numeric columns so we can standardise only them later
# 
# #Taking our predictors and covariates into one df, and our labels into another
# 
# X.variables <- data_nn %>%
#   select(-OAH)   #x_variables = everything in data other than OAH column
# 
# X.variables <- model.matrix(~., data = X.variables)[,-1] # One Hot encoding
# 
# y.variable <- data_nn[["OAH"]]  #y_variable = only our OAH column
# 
# numeric.variables <- names(data_nn)[sapply(data_nn, is.numeric)]
# numeric.variables <- setdiff(numeric.variables, "OAH")
# 
# numeric.indexes <- which(colnames(X.variables) %in% numeric.variables) #okay so the colnames are the same so we can find the indexes in x variables which are numeric with this code <--
# 
# #dummy variable indexes
# dummy.indexes <- setdiff(1:(ncol(X.variables)), numeric.indexes)
# 
# 
# #Number features
# 
# F_ <- ncol(X.variables)
# 
# #--------------Making Array---------------------#
# 
# 
# #Arrays - this is the confusing part. Keras needs an array for this to work. In our case we can think of this as turning our data into a stack of data sheets. each sheet is for a particular feature. THen there are N_sampl columns, and each is shifted by L
# 
# X <- array( NA_real_, #just an empty array, which will subsequently be filled
#            dim = c(N_samp,L, F_  ),
#            )
# 
# y <- integer(N_samp)
# 
# 
# 
# 
# 
# 
# for (i in 1:N_samp){ #here we are filling in the array with our data
# 
#   index.window <- i:(i + L - 1) #index wind = ith value in N_samp to I + window minus 1
# 
#   X[i, , ] <- X.variables[index.window, , drop = F] #filling in the predictors with the corresponding indexes into that window
#   y[i] <- y.variable[i + L + H -1] #we predict the ith + L + H y values.
# 
# }
# 
# 
# 
# #------------Training and testing split-----------------------------
# 
# #We want to preserve time series patterns, so no shuffling
# 
# #old version where everything was in order
# idx <- 1:N_samp
# number.training <- floor(N_samp * 0.8) #train = first 80% of features
# 
# tr <- idx[1:number.training] #Training indices
# te <- idx[(number.training + 1):N_samp] #Testing indices
# 
# #making 1 = 0 to see if that helps
# 
# y <- y - 1 # Now categories are {0,1,2,3,4,5,}
# 
# #test for randomness being better (didnt work)
# # idx <- 1:N_samp
# # set.seed(1)
# # tr <- sample(idx, floor(0.8*N_samp))
# # te <- idx[-tr]
# 
# 
# 
# X.train <- X[tr, , , drop = F] #training predictors data array
# X.test <- X[te, , , drop = F ] #testing predictors array
# y.train <- y[tr] #training target
# y.test <- y[te]
# 
# 
# #Class weights
# class_counts <- table(y.train)
# total <- sum(class_counts)
# n_classes <- length(class_counts)
# 
# 
# class_weights <- total / (n_classes * class_counts)
# 
# # keras expects a named list: names = class index as character
# class_weights <- as.list(class_weights)
# names(class_weights) <- as.character(0:(n_classes-1))
# 
# 
# 
# #----------------Scaling--------------------
# 
# #Remeber that we made indexes for our numeric data, which is the data we want to scale.
# 
# #We also need to scale the training data, and scale the testing data according to the training data mu and sd
# 
# 
# if (length(numeric.indexes) > 0){
# 
#   mu.numeric <- apply(X.train[, , numeric.indexes, drop = F], 3, mean, na.rm = T)
# 
#   sd.numeric <- apply(X.train[ , , numeric.indexes, drop = F], 3, sd, na.rm = T)
# 
#    sd.numeric[sd.numeric == 0 | is.na(sd.numeric)] <- 1 #make sure SD == 1
# 
#   for (j in seq_along(numeric.indexes)){
# 
#     k <- numeric.indexes[j]
#     X.train[, , k] <- (X.train[ , , k] - mu.numeric[j])/sd.numeric[j]
#     X.test[, , k] <- (X.test[ , , k] - mu.numeric[j])/sd.numeric[j]
#   }
# 
# 
# }
# 
# # Numeric cols should ne ~N(0,1)
# round(apply(X.train[ , , numeric.indexes, drop = FALSE], 3, mean), 3)
# round(apply(X.train[ , , numeric.indexes, drop = FALSE], 3, sd),   3)
# 
# 
# 
# #Getting there !
# 
# #Since we are doing a multiclass prediction we need to use softmax
# #This means using the tocategorical() function on y
# 
# y.test <- to_categorical(y.test, num_classes = 6)
# 
# 
# y.train <- to_categorical(y.train, num_classes = 6)
# 
# list(y.test, y.train, X.train, X.test, class_weights, F_)
# 
# 
# 
# 
# 
# }

```

```{r Keras_Matthew_modelling}

w_ <- pre_data(w = 3, h = 1)

y.test <- w_[[1]]
y.train <- w_[[2]]
X.train <- w_[[3]]
X.test <- w_[[4]]
cl_weights <- w_[[5]]
F_ <- w_[[6]]



#------------Finally : the modeling part--------------------

set.seed(1)
tf$keras$utils$set_random_seed(123L)

input <- layer_input(shape = c(dim(X.train)[2], F_)) # Shape = (L, F)

output <- input %>%
  layer_conv_1d(filters = 16, kernel_size = 5, padding = "same", activation = "relu") %>%
  layer_dropout(rate = 0.5) %>%
  layer_conv_1d(filters = 32, kernel_size = 5, padding = "same", activation = "relu") %>% 
  layer_global_average_pooling_1d()%>%
  layer_dropout(rate = 0.5) %>%
  layer_dense(units = 6, activation = "softmax")
  

model <- keras_model(inputs = input, outputs = output)

model %>%  compile(
  optimizer = "adam",
  loss = "categorical_crossentropy",
  metrics = "accuracy"
)



#Training
#200 epochs better
history <- model %>% 
  fit(X.train, y.train,
      epochs = 200,
      batch_size = 64,
      verbose = 0,
      validation_split = 0.2,
      shuffle = F)
      #class_weight = cl_weights)


metrics <- model %>% evaluate(X.test, y.test)

metrics

plot(history)


```

```{r}



# 
# #Function to let me test for numerous window sizes
# 
# 
# pre_data <- function(w, h = 1){
# 
# #--------Data Preparation-------------------------------------
# 
# T <- length(data$Date) #time period length
# 
# data_nn <- data %>% 
#   select(-c(Date, OSgrid, Location)) #removing date column, OSgrid, location
# 
# 
# L <- w #Rolling window length = 3. So what I am decided is that It can use the previus 3 days on record to train itself
# H <- h # predict 1 day ahead
# 
# 
# #number of samples we can have, so number of time slices, each one iterated one step ahead by H (which is just 1 day in our case)
# N_samp <- T - L - H + 1
# 
# #BRain is broken, but this is what needs to happen, remove OAH and put into vector, one hot enocode x variables and fix the rest wrt Y = OAH and so on
# 
# #----------------One hot encoding--------
# 
# #This part is quite a process. So big picture, the numeric data needs to be standardised, but when we do one-hot-encoding, it becomes quite messy to avoid standardising the dummy variables. SO we are going to make indices for the numeric columns so we can standardise only them later
# 
# #Taking our predictors and covariates into one df, and our labels into another
# 
# X.variables <- data_nn %>% 
#   select(-OAH)   #x_variables = everything in data other than OAH column
#   
# X.variables <- model.matrix(~., data = X.variables)[,-1] # One Hot encoding
# 
# y.variable <- data_nn[["OAH"]]  #y_variable = only our OAH column
# 
# numeric.variables <- names(data_nn)[sapply(data_nn, is.numeric)]
# numeric.variables <- setdiff(numeric.variables, "OAH")
# 
# numeric.indexes <- which(colnames(X.variables) %in% numeric.variables) #okay so the colnames are the same so we can find the indexes in x variables which are numeric with this code <--
# 
# #dummy variable indexes
# dummy.indexes <- setdiff(1:(ncol(X.variables)), numeric.indexes)
# 
# 
# #Number features
# 
# F_ <- ncol(X.variables)
# 
# #--------------Making Array---------------------#
# 
# 
# #Arrays - this is the confusing part. Keras needs an array for this to work. In our case we can think of this as turning our data into a stack of data sheets. each sheet is for a particular feature. THen there are N_sampl columns, and each is shifted by L
# 
# X <- array( NA_real_, #just an empty array, which will subsequently be filled
#            dim = c(N_samp,L, F_  ),
#            )
# 
# y <- integer(N_samp)
# 
# 
# 
# 
# 
# 
# for (i in 1:N_samp){ #here we are filling in the array with our data
#   
#   index.window <- i:(i + L - 1) #index wind = ith value in N_samp to I + window minus 1
#   
#   X[i, , ] <- X.variables[index.window, , drop = F] #filling in the predictors with the corresponding indexes into that window
#   y[i] <- y.variable[i + L + H -1] #we predict the ith + L + H y values.
#   
# }
# 
# 
# 
# #------------Training and testing split-----------------------------
# 
# #We want to preserve time series patterns, so no shuffling 
# 
# #old version where everything was in order
# idx <- 1:N_samp
# number.training <- floor(N_samp * 0.8) #train = first 80% of features
# 
# tr <- idx[1:number.training] #Training indices
# te <- idx[(number.training + 1):N_samp] #Testing indices
# 
# #making 1 = 0 to see if that helps
# 
# y <- y - 1 # Now categories are {0,1,2,3,4,5,}
# 
# #test for randomness being better (didnt work)
# # idx <- 1:N_samp
# # set.seed(1)
# # tr <- sample(idx, floor(0.8*N_samp))
# # te <- idx[-tr]
# 
# 
# 
# X.train <- X[tr, , , drop = F] #training predictors data array
# X.test <- X[te, , , drop = F ] #testing predictors array
# y.train <- y[tr] #training target
# y.test <- y[te]
# 
# 
# #Class weights
# class_counts <- table(y.train)
# total <- sum(class_counts)
# n_classes <- length(class_counts)
# 
# 
# class_weights <- total / (n_classes * class_counts)
# 
# # keras expects a named list: names = class index as character
# class_weights <- as.list(class_weights)
# names(class_weights) <- as.character(0:(n_classes-1))
# 
# 
# 
# #----------------Scaling--------------------
# 
# #Remeber that we made indexes for our numeric data, which is the data we want to scale. 
# 
# #We also need to scale the training data, and scale the testing data according to the training data mu and sd
# 
# 
# if (length(numeric.indexes) > 0){
#   
#   mu.numeric <- apply(X.train[, , numeric.indexes, drop = F], 3, mean, na.rm = T)
#   
#   sd.numeric <- apply(X.train[ , , numeric.indexes, drop = F], 3, sd, na.rm = T)
#   
#    sd.numeric[sd.numeric == 0 | is.na(sd.numeric)] <- 1 #make sure SD == 1
#   
#   for (j in seq_along(numeric.indexes)){
#     
#     k <- numeric.indexes[j]
#     X.train[, , k] <- (X.train[ , , k] - mu.numeric[j])/sd.numeric[j]
#     X.test[, , k] <- (X.test[ , , k] - mu.numeric[j])/sd.numeric[j]
#   }
#   
#   
# }
# 
# # Numeric cols should ne ~N(0,1)
# round(apply(X.train[ , , numeric.indexes, drop = FALSE], 3, mean), 3)
# round(apply(X.train[ , , numeric.indexes, drop = FALSE], 3, sd),   3)
# 
# 
# 
# #Getting there !
# 
# #Since we are doing a multiclass prediction we need to use softmax
# #This means using the tocategorical() function on y
# 
# y.test <- to_categorical(y.test, num_classes = 6)
# 
# 
# y.train <- to_categorical(y.train, num_classes = 6)
# 
# list(y.test, y.train, X.train, X.test, class_weights, F_)
# 
# 
# 
# 
# 
# }


```

```{r}
#testing c-d-c

#vary window size hasnt made significant changes
#Adding weights to control for imbalance didnt help

# w_7 <- pre_data(w = 3, h = 1)
# 
# y.test <- w_7[[1]]
# y.train <- w_7[[2]]
# X.train <- w_7[[3]]
# X.test <- w_7[[4]]
# cl_weights <- w_7[[5]]
# F_ <- w_7[[6]]
# # ---------------- Grid search for your existing architecture ----------------
# 
# # Repro seeds (same as you set)
# set.seed(1)
# tf$keras$utils$set_random_seed(123L)
# 
# # Define the grid (adjust values if you want a bigger/smaller search)
# grid <- expand.grid(
#   filters1   = c( 32, 64),
#   filters2   = c( 32, 64),
#   kernel_sz  = c( 9, 16),
#   drop1      = c(0.3, 0.5),
#   drop2      = c(0.3, 0.5),
#   stringsAsFactors = FALSE
# )
# 
# results <- data.frame(
#   filters1 = integer(), filters2 = integer(),
#   kernel_sz = integer(), drop1 = double(), drop2 = double(),
#   val_accuracy = double(), test_loss = double(), test_accuracy = double(),
#   stringsAsFactors = FALSE
# )
# 
# histories <- vector("list", nrow(grid))  # optional: keep training histories
# 
# for (i in seq_len(nrow(grid))) {
#   cat(sprintf("\n[%d/%d] f1=%d, f2=%d, k=%d, d1=%.2f, d2=%.2f\n",
#               i, nrow(grid),
#               grid$filters1[i], grid$filters2[i],
#               grid$kernel_sz[i], grid$drop1[i], grid$drop2[i]))
# 
#   # Clear TF/Keras state between runs to avoid graph/memory accumulation
#   tensorflow::tf$keras$backend$clear_session()
# 
#   # Re-seed before each run to keep trials comparable
#   set.seed(1)
#   tf$keras$utils$set_random_seed(123L)
# 
#   # ----- Build your SAME model, with grid parameters plugged in -----
#   input <- layer_input(shape = c(dim(X.train)[2], F_)) # (L, F)
# 
#   output <- input %>%
#     layer_conv_1d(filters = grid$filters1[i],
#                   kernel_size = grid$kernel_sz[i],
#                   padding = "same", activation = "relu") %>%
#     layer_dropout(rate = grid$drop1[i]) %>%
#     layer_conv_1d(filters = grid$filters2[i],
#                   kernel_size = grid$kernel_sz[i],
#                   padding = "same", activation = "relu") %>%
#     layer_global_average_pooling_1d() %>%
#     layer_dropout(rate = grid$drop2[i]) %>%
#     layer_dense(units = 6, activation = "softmax")
# 
#   model <- keras_model(inputs = input, outputs = output)
# 
#   model %>% compile(
#     optimizer = "adam",
#     loss = "categorical_crossentropy",
#     metrics = "accuracy"
#   )
# 
#   # ----- Train (identical settings you had) -----
#   history <- model %>% fit(
#     X.train, y.train,
#     epochs = 60,
#     batch_size = 64,
#     verbose = 0,
#     validation_split = 0.2,
#     shuffle = FALSE
#     # = cl_weights
#   )
# 
#   # Collect validation accuracy from the last epoch
#   val_acc <- tail(history$metrics$val_accuracy, 1)
#   if (length(val_acc) == 0) {
#     # Fallback name in case of different metric key
#     val_acc <- tail(history$metrics$val_acc, 1)
#   }
# 
#   # Evaluate on test set (same as your code)
#   test_metrics <- model %>% evaluate(X.test, y.test, verbose = 0)
# 
#   # test_metrics is usually a named numeric vector: c(loss=..., accuracy=...)
#   test_loss <- as.numeric(test_metrics[[1]])
#   test_acc  <- if (!is.null(names(test_metrics)) && "accuracy" %in% names(test_metrics)) {
#     as.numeric(test_metrics[["accuracy"]])
#   } else {
#     as.numeric(test_metrics[[2]])
#   }
# 
#   # Store results
#   results[i, ] <- list(
#     grid$filters1[i], grid$filters2[i],
#     grid$kernel_sz[i], grid$drop1[i], grid$drop2[i],
#     as.numeric(val_acc), test_loss, test_acc
#   )
#   histories[[i]] <- history
# }
# 
# # Sort by best validation accuracy (descending)
# results <- results[order(-results$val_accuracy), ]
# 
# print(results)
# 
# # The best config (row 1). If you want to refit using it, use results[1, ] values.
# best <- results[1, ]
# cat("\nBest by val_accuracy:\n")
# print(best)
# 
# #save(results, file = "huge_search1_c-d-c-d")
search1 <- load("huge_search1_c-d-c-d")

```

```{r ridiculous_keras_search}

# w_7 <- pre_data(w = 3, h = 1)
# 
# y.test <- w_7[[1]]
# y.train <- w_7[[2]]
# X.train <- w_7[[3]]
# X.test <- w_7[[4]]
# cl_weights <- w_7[[5]]
# F_ <- w_7[[6]]
# 
# # ---------------- Grid search (extended architectures) ----------------
# 
# # Repro seeds (same as you set)
# set.seed(1)
# tf$keras$utils$set_random_seed(123L)
# 
# # New: a compact grid that expands architectures by # of conv blocks and simple growth
# grid <- expand.grid(
#   n_blocks    = c(1, 2, 3),          # number of Conv1D blocks
#   filters_base= c(32, 64),           # base filters for the first block
#   growth      = c(1, 2),             # filters multiply by growth^(block-1)
#   kernel_sz   = c(9, 16),            # kernel size
#   drop_rate   = c(0.3, 0.5),         # same dropout used per block
#   batch_norm  = c(FALSE, TRUE),      # optional batch norm after each conv
#   pooling     = c("gap","gmp"),      # global average vs global max pooling
#   dense_units = c(0, 64),            # optional dense head before softmax
#   activation  = c("relu","elu"),     # activation for conv/dense layers
#   lr          = c(1e-3, 5e-4),       # Adam learning rates
#   stringsAsFactors = FALSE
# )
# 
# # Cap to at most 1000 models (random sample to keep coverage)
# if (nrow(grid) > 1000) {
#   set.seed(1)
#   grid <- grid[sample(seq_len(nrow(grid)), 1000), , drop = FALSE]
# }
# cat(sprintf("Total models to try: %d\n", nrow(grid)))
# 
# results <- data.frame(
#   n_blocks = integer(),
#   filters_base = integer(),
#   growth = integer(),
#   kernel_sz = integer(),
#   drop_rate = double(),
#   batch_norm = logical(),
#   pooling = character(),
#   dense_units = integer(),
#   activation = character(),
#   lr = double(),
#   val_accuracy = double(),
#   test_loss = double(),
#   test_accuracy = double(),
#   stringsAsFactors = FALSE
# )
# 
# histories <- vector("list", nrow(grid))  # optional: keep training histories
# 
# for (i in seq_len(nrow(grid))) {
#   cat(sprintf(
#     "\n[%d/%d] blocks=%d, f0=%d, g=%d, k=%d, drop=%.2f, bn=%s, pool=%s, dense=%d, act=%s, lr=%g\n",
#     i, nrow(grid),
#     grid$n_blocks[i], grid$filters_base[i], grid$growth[i],
#     grid$kernel_sz[i], grid$drop_rate[i],
#     ifelse(grid$batch_norm[i], "T", "F"),
#     grid$pooling[i], grid$dense_units[i], grid$activation[i], grid$lr[i]
#   ))
# 
#   # Clear TF/Keras state between runs to avoid graph/memory accumulation
#   tensorflow::tf$keras$backend$clear_session()
# 
#   # Re-seed before each run to keep trials comparable
#   set.seed(1)
#   tf$keras$utils$set_random_seed(123L)
# 
#   # ----- Build model dynamically based on grid row -----
#   input <- layer_input(shape = c(dim(X.train)[2], F_)) # (L, F)
#   x <- input
# 
#   # Conv blocks
#   for (b in seq_len(grid$n_blocks[i])) {
#     n_filters <- as.integer(round(grid$filters_base[i] * (grid$growth[i]^(b - 1))))
#     x <- x %>%
#       layer_conv_1d(filters = n_filters,
#                     kernel_size = grid$kernel_sz[i],
#                     padding = "same",
#                     activation = grid$activation[i])
#     if (isTRUE(grid$batch_norm[i])) {
#       x <- x %>% layer_batch_normalization()
#     }
#     x <- x %>% layer_dropout(rate = grid$drop_rate[i])
#   }
# 
#   # Global pooling choice
#   if (grid$pooling[i] == "gap") {
#     x <- x %>% layer_global_average_pooling_1d()
#   } else {
#     x <- x %>% layer_global_max_pooling_1d()
#   }
# 
#   # Optional dense head
#   if (grid$dense_units[i] > 0) {
#     x <- x %>% layer_dense(units = grid$dense_units[i], activation = grid$activation[i]) %>%
#       layer_dropout(rate = grid$drop_rate[i])
#   }
# 
#   output <- x %>% layer_dense(units = 6, activation = "softmax")
# 
#   model <- keras_model(inputs = input, outputs = output)
# 
#   model %>% compile(
#     optimizer = optimizer_adam(learning_rate = grid$lr[i]),
#     loss = "categorical_crossentropy",
#     metrics = "accuracy"
#   )
# 
#   # ----- Train (same core settings you had) -----
#   history <- model %>% fit(
#     X.train, y.train,
#     epochs = 60,
#     batch_size = 64,
#     verbose = 0,
#     validation_split = 0.2,
#     shuffle = FALSE
#     # class_weight = cl_weights  # uncomment if you want to use class weights
#   )
# 
#   # Collect validation accuracy from the last epoch
#   val_acc <- tail(history$metrics$val_accuracy, 1)
#   if (length(val_acc) == 0) val_acc <- tail(history$metrics$val_acc, 1)
# 
#   # Evaluate on test set
#   test_metrics <- model %>% evaluate(X.test, y.test, verbose = 0)
#   test_loss <- as.numeric(test_metrics[[1]])
#   test_acc  <- if (!is.null(names(test_metrics)) && "accuracy" %in% names(test_metrics)) {
#     as.numeric(test_metrics[["accuracy"]])
#   } else {
#     as.numeric(test_metrics[[2]])
#   }
# 
#   # Store results
#   results[i, ] <- list(
#     grid$n_blocks[i],
#     grid$filters_base[i],
#     grid$growth[i],
#     grid$kernel_sz[i],
#     grid$drop_rate[i],
#     grid$batch_norm[i],
#     grid$pooling[i],
#     grid$dense_units[i],
#     grid$activation[i],
#     grid$lr[i],
#     as.numeric(val_acc),
#     test_loss,
#     test_acc
#   )
#   histories[[i]] <- history
# }
# 
# # Sort by best validation accuracy (descending)
# results <- results[order(-results$val_accuracy), ]
# 
# print(results)
# 
# cat("\nBest by val_accuracy:\n")
# best <- results[1, ]
# print(best)
# max(results$test_accuracy)

#save(results, file = "cnn-finalsearch")


results1 <- load("cnn-finalsearch")

load("huge_search1_c-d-c-d")


```

## Modelling

## Discussion
