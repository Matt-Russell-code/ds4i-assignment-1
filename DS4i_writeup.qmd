---
title: "DS4I - Project Writeup"
format: html
execute:
  echo: false
  warning: false
  message: false
  results: hide
editor: visual
---

```{r setup}


#libraries 

library(kableExtra)
library(tidyverse)
library(ggplot2)
library(ggpubr)
library(lubridate)
library(caret)

#theme for ggplot2, let me know if you do not like this theme and we can change it :)

#Theme for ggplot2 
theme_set(
  theme_bw(base_size = 9) +
    theme(
      plot.title   = element_text(hjust = 0.5, size = 9),
      panel.border = element_rect(colour = "black", fill = NA, linewidth = 0.5),
      # keep grids but make them subtle
      panel.grid.major = element_line(colour = "grey85", linewidth = 0.3),
      panel.grid.minor = element_line(colour = "grey92", linewidth = 0.2),
      # base-R style text/ticks
      axis.text  = element_text(colour = "black"),
      axis.title = element_text(colour = "black"),
      axis.ticks = element_line(colour = "black", linewidth = 0.3)
    )
)



#making geompoint have hollow circles
update_geom_defaults(
  "point",
  list(size = 1, alpha = 0.9, shape = 21, colour = "black")
)


```

```{r}
#| echo: false
knitr::include_graphics("av.jpg")

```

[Photo by Nicolas Cool on Unsplash]{style="font-size:12px"}

## Introduction

## Literature Review

Snow avalanches are a recognised hazard in snow-prone mountain regions worldwide. Unlike in many alpine regions, avalanches in Scotland rarely impact large settlements and infrastructure, however, pose significant risk to recreational users in the Scottish Highlands (Diggins, 2009; Ward, 1980). Winter activities, such as climbing, walking, and skiing are increasingly popular, leading to greater human exposure to avalanche-prone terrain and resulting in numerous injuries and occasional fatalities each year (Scottish Avalanche Information Service \[SAIS\], 2024). This has prompted several studies to examine Scottish snowpack conditions and avalanche forecasting strategies, with the earliest studies conducted between 1970 and 1980 (Langmuir, 1970; Spink, 1970; Beattie, 1976; Ward, 1980; Ward & Beattie, 1985). However, these early studies were largely descriptive, relying on snow-pit measurements and expert judgements to identify snow properties and weather conditions correlated with avalanche risk. Building on this foundation, Ward (1984) applied one of the first predictive forecasting models for avalanches in Scotland.

Numerous studies emphasise the inherent complexity of avalanche modelling where predictions are based on multiple interacting factors (Choubin et al., 2019; Herwijen et al., 2016; Hendrick et al., 2023; Pozdnoukhov et al., 2008; Singh & Ganju, 2008). This complexity is only heightened in regions with highly variable weather patterns (Sharma & Ganju, 1999). In response, early statistical methods such as nearest neighbours became a popular technique to support (not replace) forecasters in predicting avalanche outcomes through relating current weather conditions to past avalanche events (Blagovechshenskiy et al., 2023; Kala et al., 2025; Pozdnoukhov et al., 2018; Singh & Ganju, 2008). However, this approach is susceptible to overfitting and struggles with complex and high-dimensional data (Kala et al., 2025; Pozdnoukhov et al., 2018; Singh & Ganju, 2018). Using a dataset from Lochaber, Scotland, Pozdnoukhov et al. (2008) reported that support vector machines (SVMs) showed a broadly similar performance to a baseline nearest neighbours method, however, had the added advantage of handling high dimensionality datasets and produced more descriptive forecasts. SVMs have been explored and shown success in aiding avalanche risk forecasting in other regions including Iran, India, Switzerland, and Tibet (Choubin et al., 2019; Rahmati et al., 2019; Schirmer et al., 2009; Tiwari et al., 2021; Wen et al., 2022). 

Over recent years, more complex machine learning (ML) algorithms such as regression trees, random forests, and neural networks have demonstrated strong predictive performance in avalanche forecasting (Blagovechshenskiy et al., 2023; Choubin et al., 2019; Gauthier et al., 2025; Hendrick et al., 2023; Rahmati et al., 2019; Singh & Ganju, 2018; Tiwari et al., 2021; Wen et al., 2022). Avalanche datasets are often high-dimensional, and using human forecaster judgement to select which features to include can be highly subjective and variable (Helbig et al., 2015; Pozdnoukhov et al., 2008). Hybrid approaches, which combine ML with expert-guided feature selection, have shown practical value (Gauthier et al., 2025). However, fully data-driven forecasting methods, particularly neural networks, offer important advantages.

Simulating the work of a human brain, neural networks are deep learning algorithms consisting of interconnected nodes/neurons that can model complex and nonlinear relationships between inputs and outputs (Blagovechshenskiy et al., 2023; Sharma et al., 2023; Tu, 1996). Their ability to process all input variables simultaneously allows them to implicitly learn interactions and patterns in the data without prior knowledge on which variables are most important (Blagovechshenskiy et al., 2023; Fromm & Schonberger, 2022; Sharma et al., 2023). Studies in avalanche forecasting from regions such as Switzerland, Kazakhstan, and  India have demonstrated that neural networks not only achieve strong predictive performance, but can also assess the relative importance of each variable to avalanche risk (Fromm & Schonberger; Sharma et al., 2023; Singh & Ganju, 2008). This ability makes them particularly well suited for high-dimensional datasets, such as those encountered in avalanche forecasting, as well as in contexts where prior domain knowledge is limited or when the most relevant predictors are unknown.

Despite advances in data-driven avalanche forecasting worldwide, research specific to Scotland remains limited. This scarcity lies in three domains: 

\(1\) The small number of published forecasting studies (only 9 relevant peer-reviewed papers exist on Scopus, with the most recent study conducted in 2011 (keywords: ( "avalanche forecasting" OR "avalanche prediction" ) AND Scotland)

\(2\) The restricted range of statistical approaches explored (only KNN and SVM) (Joachim et al., 2004; Pozdnoukhov et al., 2008; Pozdnoukhov et al., 2011; Purves et al., 2003)

\(3\) The focus on only a few regions in Scotland (primarily the Cairngorms and Lochaber regions)

This limited research interest likely stems from the highly localised nature of avalanche risk in Scotland, which generally occurs in remote areas and impacts only a small subset of individuals (Diggins, 2009; SAIS, 2024). Nevertheless, improved avalanche forecasting is argued to be increasingly important under changing climate conditions and global warming, which may alter snowpack properties and avalanche risk and frequency (Gauthier et al., 2025; Werritty & Sugden, 2013). A detailed discussion of climate change, however, is beyond the scope of this paper. As such, the present study aims to extend forecasting research in Scotland both spatially and methodologically by applying a neural network (or two or three?????) across the six distinct regions of Creah Meagaidh, Glencoe, Lochaber, Northern Cairngorms, Southern Cairngorms, and Torridon.

\<insert paragraph explaining modelling approaches of prev papers + this project’s modelling approaches. I.e. the neural net architecture\>

Hypothesis???? We can workshop together

## Context and Dataset

Scotland experiences a mild, wet, and temperate maritime climate due to the strong influence of the North Atlantic Ocean and persistent south-westerly winds (Pozdnoukhov et al., 2008; Scottish Government, 2011). These conditions result in rapid temperature changes, frequent heavy precipitation (falling as either snow or rain in the winter), and strong winds. With the highest peak, Ben Nevis in Lochaber (1345 m), mountains are low compared to alpine ranges (\~ 4000 m), resulting in a snowpack that is typically shallower, wetter, and more variable in comparison (Britannica, 2025; Pozdnoukhov et al., 2008; WSL Institute for Snow and Avalanche Research SLF, 2016). Aspects such as rapid freeze-thaw cycles, rain-on-snow, and wind redistribution further destabilise snow structure and stability, creating avalanche risks that can arise and disappear within a short time (Diggins, 2009; Pozdnoukhov et al., 2008; Purves et al., 2003; Scottish Government, 2011). Snowpack conditions also differ regionally; while the West is characterised as strongly maritime, with mild and wet winters, the North and East experience colder and drier conditions (MetOffice, 2010). As such, Scotland’s climate is highly variable both spatially and temporally, adding to the complexity of predicting avalanche risk.

This paper uses a 15-year archive of avalanche forecasts from Scotland across the six areas of Creah Meagaidh, Glencoe, Lochaber, Northern Cairngorms, Southern Cairngorms, and Torridon, produced by the Scottish Avalanche Information Service (see Figure ? below). 

```{r map}



library(dplyr)
library(ggplot2)
library(ggrepel)
library(maps)

# palette 
hazard_cols <- c(
  "Low"            = "#d4f0f0",
  "Moderate"       = "#91bfdb",
  "Considerable -" = "#fc8d59",
  "Considerable +" = "#d73027",
  "High"           = "#7f0000"
)
hazard_levels <- names(hazard_cols)
data <- data %>% mutate(OAH = factor(as.character(OAH), levels = hazard_levels, ordered = TRUE))

# label positions 
area_labels <- data %>%
  filter(!is.na(longitude), !is.na(latitude), !is.na(Area)) %>%
  group_by(Area) %>%
  summarise(lon = median(longitude, na.rm = TRUE),
            lat = median(latitude,  na.rm = TRUE),
            .groups = "drop")

# tight bounding box from points
d_pts <- data %>% filter(!is.na(longitude), !is.na(latitude), !is.na(OAH))
pad_x <- 0.25; pad_y <- 0.20
xlim_use <- range(d_pts$longitude, na.rm = TRUE) + c(-pad_x, pad_x)
ylim_use <- range(d_pts$latitude,  na.rm = TRUE) + c(-pad_y, pad_y)

# plot
ggplot() +
  borders("world", regions = "UK", fill = "grey92", colour = "grey70") +
  geom_point(
    data = d_pts,
    aes(x = longitude, y = latitude, color = OAH),
    size = 1.8, alpha = 0.85
  ) +
  ggrepel::geom_text_repel(
    data = area_labels,
    aes(x = lon, y = lat, label = Area),
    size = 3, seed = 941,
    box.padding = 0.15, point.padding = 0.1,
    min.segment.length = 0, max.overlaps = 20
  ) +
  scale_color_manual(values = hazard_cols, drop = FALSE, name = "OAH") +
  coord_quickmap(xlim = xlim_use, ylim = ylim_use, expand = FALSE) +
  labs(
    title = " Figure 1: Observed Avalanche Hazard (OAH) — Scotland",
    subtitle = "Point observations coloured by hazard level; SAIS areas labelled",
    x = NULL, y = NULL
  ) +
  theme_minimal(base_size = 12) +
  theme(
    panel.grid.major = element_line(color = "grey95"),
    legend.position  = "right",
    plot.title       = element_text(face = "bold")
  )


```

Data cover the Scottish winters (November to April) from the December 2009 to March 2025. Predictor variables are categorised into three main groups, including (1) position and topography, (2) weather, and (3) snowpack test, all collected at the forecast location. Each observation also includes the date and the forecast area. For a full overview of the predictors, see Table ?.

\*\*table of predictors

```{r pred_tab}

```

## Exploratory Data Analysis

### a. Feature Map Correlations

```{r feature-map-correlations}


library(dplyr)
library(ggplot2)

dat <- data
dat <- dplyr::ungroup(dat)  # in case it's a grouped tibble

#  Keep only numeric columns, drop seasonal values
num_df <- dat %>%
  select(where(is.numeric)) %>%
  select(
    -any_of(c("fah_i", "oah_i", "season_id")),   # <- explicitly drop season_id
    -matches("^(dayofseason|season(_year|_d)?|seasonality|month|year|hour)$", ignore.case = TRUE)
  )

#  must have at least 2 numeric vars
if (ncol(num_df) < 2L) stop("Not enough numeric columns for correlation analysis.")

#  Drop zero-variance columns
ok_cols <- vapply(num_df, function(x) sd(x, na.rm = TRUE) > 0, logical(1))
num_df  <- num_df[, ok_cols, drop = FALSE]

#  Correlation matrix (Pearson; pairwise complete)
corr_mat <- cor(num_df, use = "pairwise.complete.obs", method = "pearson")

# Long form 
corr_long <- as.data.frame(as.table(corr_mat)) %>%
  rename(var1 = Var1, var2 = Var2, r = Freq) %>%
  filter(as.character(var1) != as.character(var2)) %>%
  mutate(abs_r = abs(r))

# Heatmap — colours only
p_corr <- ggplot(as.data.frame(as.table(corr_mat)),
                 aes(Var1, Var2, fill = Freq)) +
  geom_tile() +
  scale_fill_gradient2(limits = c(-1, 1)) +
  labs(title = "Figure 2: Map of Feature Correlations",
       x = NULL, y = NULL, fill = "r") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1),
        panel.grid = element_blank())

p_corr

```

Figure 2 provide an overview of how the variables in the dataset relate to one another. The plot shows that many features are moderately to strongly correlated, while others appear largely independent. This pattern indicates that the dataset contains groups of variables that move together, suggesting potential redundancy. From a modelling perspective, these interdependencies are important because highly correlated features can increase complexity, making it harder for models to distinguish unique contributions. However, they can also capture the multifactorial nature of avalanche risk. In terms of modelling neural networks, the presence of correlated variables is not inherently problematic, as the model can learn non-linear interactions. However, strong correlations increase the risk of overfitting if the network simply memorises patterns rather than generalising.

```{r}

# Table for top pairs of highly correlated variables
library(dplyr)

top_pairs <- corr_long %>%
  rowwise() %>%
  mutate(pair = paste(sort(c(var1, var2)), collapse = "_")) %>%
  ungroup() %>%
  distinct(pair, .keep_all = TRUE) %>%
  arrange(desc(abs_r)) %>%
  slice(1:5) %>%
  select(var1, var2, r, abs_r)

if (knitr::is_html_output()) {
 
  DT::datatable(
    top_pairs,
    caption = htmltools::tags$caption(
      style = 'caption-side: top; text-align: left;',
      'Table 2: Most highly correlated variable pairs (numeric variables only)'
    ),
    options = list(pageLength = 10, order = list(list(4, 'desc'))),
    rownames = FALSE,
    class = 'compact stripe hover'
  ) |>
    DT::formatRound(c('r', 'abs_r'), 3)
} else {
  # Static table that shows NOW in the editor preview
  knitr::kable(
    top_pairs,
    caption = "Table 2: Most highly correlated variable pairs (numeric variables only)",
    digits = 3,
    align = c('l','l','r','r')
  )
}


```

Table 2 shows the strongest correlations among numeric variables. The highest is between summit air temperature and air temperature (r=0.846), which indicates strong redundancy. This is important as air temperature has consistently been identified in the literature as a key driver of avalanche conditions (Ward,1980; Pozdnoukhouv et al., 2018) , meaning it will likely be an important predictor in the final neural network model . Other correlations with absolute r values less than 0.80 are moderately strong but still useful for neural networks, which are well suited to handling multiple interdependent inputs. The pair between insolation and settlement (r= 0.647) is worth acknowledging, as the literature does acknowledge the role of human exposure and settlement patterns in avalanche risk, making this link notable though less central to forecasting accuracy. Only very high correlations (such as between summit and standard air temperature) may warrant consideration for reduction to avoid redundancy, while the others remain informative and manageable within a neural network framework.

### b. Precipitation Type

```{r precipitation-plot}


library(dplyr)
library(ggplot2)
library(scales)
library(stringr)
library(forcats)

df <- dplyr::ungroup(data)  

#  Columns
precip_col <- names(df)[str_detect(tolower(names(df)), "precip")] %>% .[1]
stopifnot(length(precip_col) == 1)
oah_col <- names(df)[tolower(names(df)) == "oah"] %>% .[1]
stopifnot(length(oah_col) == 1)

# Hazard levels + palette
hazard_levels <- c("Low","Moderate","Considerable -","Considerable +","High")
df[[oah_col]] <- factor(df[[oah_col]], levels = hazard_levels, ordered = TRUE)
hazard_cols <- c(
  "Low"            = "#d4f0f0",
  "Moderate"       = "#91bfdb",
  "Considerable -" = "#fc8d59",
  "Considerable +" = "#d73027",
  "High"           = "#7f0000"
)

#Summarise counts/proportions, drop NA in precip
oah_precip <- df %>%
  filter(!is.na(.data[[precip_col]]), !is.na(.data[[oah_col]])) %>%
  count(precip = .data[[precip_col]], oah = .data[[oah_col]], name = "n") %>%
  group_by(precip) %>%
  mutate(prop = n / sum(n)) %>%
  ungroup()

#  Extract leading numbers to identify bins; 
oah_precip <- oah_precip %>%
  mutate(bin_num = suppressWarnings(as.numeric(str_extract(as.character(precip), "^-?\\d+")))) %>%
  filter(!is.na(bin_num), bin_num <= 10)

# Order precipitation bins numerically
lvl_order <- oah_precip %>%
  distinct(precip, bin_num) %>%
  arrange(bin_num, precip) %>%
  pull(precip) %>%
  unique()

oah_precip <- oah_precip %>%
  mutate(precip = factor(precip, levels = lvl_order, ordered = TRUE))

#  Plot
ggplot(oah_precip, aes(x = precip, y = prop, fill = oah)) +
  geom_col(position = "fill", width = 0.9) +
  geom_text(aes(label = percent(prop, accuracy = 1)),
            position = position_fill(vjust = 0.5),
            size = 3, colour = "white", fontface = "bold") +
  scale_y_continuous(labels = percent_format(accuracy = 1)) +
  scale_fill_manual(values = hazard_cols, drop = FALSE) +
  labs(
    title = "Figure 3: Precipitation type vs Observed Avalanche Hazard (OAH)",
    x = "Precipitation type/level",
    y = "Proportion within precipitation type",
    fill = "OAH"
  ) +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 30, hjust = 1))

```

Figure 3 shows the distribution of observed avalanche hazard (OAH) levels across different precipitation types. The results reveal a clear trend as low hazard dominates under conditions of none (0) or trace (2) precipitation, whereas higher hazard categories become increasingly prevalent as snowfall intensity increases. Heavy snow is associated with a substantially larger share of “Considerable” and “High” hazard ratings (82%). This supports the findings of previous literature that shows the role of intense precipitation events in destabilising the snowpack (Diggins, 2009; Ward, 1980). This also aligns with existing knowledge of the Scottish climate, where rapid weather shifts and frequent rain or snow-on-snow events contribute to unstable snowpack conditions (Pozdnoukhov et al., 2008; Purves et al., 2003). The results show that the precipitation type is a key driver of avalanche hazards. Neural networks can further explore the relationship between precipitation type and specific snowpack features.

### c. Seasonality

```{r seasons-plot}

library(dplyr)
library(ggplot2)
library(patchwork)


hazard_cols <- c(
  "Low"            = "#d4f0f0",
  "Moderate"       = "#91bfdb",
  "Considerable -" = "#fc8d59",
  "Considerable +" = "#d73027",
  "High"           = "#7f0000"
)

# Ensure OAH ordered Low -> High
data <- data %>%
  mutate(
    OAH = factor(OAH,
                 levels = c("Low","Moderate","Considerable -","Considerable +","High"))
  )

# Define season year: 

data <- data %>%
  mutate(
    month_num = lubridate::month(Date),
    season_year = if_else(month_num >= 11, lubridate::year(Date), lubridate::year(Date) - 1),
    month_in_season = factor(
      month_num,
      levels = c(11, 12, 1, 2, 3, 4, 5),
      labels = c("Nov","Dec","Jan","Feb","Mar","Apr","May")
    )
  )

# Aggregate counts and proportions per month (across all years)
oah_monthly <- data %>%
  filter(!is.na(OAH), month_in_season %in% levels(month_in_season)) %>%
  count(month_in_season, OAH, name = "n") %>%
  group_by(month_in_season) %>%
  mutate(month_total = sum(n),
         prop = n / month_total) %>%
  ungroup()

# A helper frame for the counts-only series
month_totals <- oah_monthly %>%
  distinct(month_in_season, month_total)


# Top: proportions
p_prop <- ggplot(oah_monthly, aes(x = month_in_season, y = prop, fill = OAH)) +
  geom_col(position = "fill", width = 0.85) +
  scale_fill_manual(values = hazard_cols, drop = FALSE) +
  labs(x = NULL, y = "Proportion of forecasts", fill = "OAH") +
  theme_minimal(base_size = 12) +
  theme(
    legend.position = "right",
    plot.margin = margin(t = 5, r = 10, b = 20, l = 10)   # extra space below
  )

# Bottom: counts 
max_n <- max(month_totals$month_total, na.rm = TRUE)
p_counts <- ggplot(month_totals, aes(x = month_in_season, y = month_total)) +
  geom_col(width = 0.85, fill = "grey70") +
  geom_text(aes(label = month_total), vjust = -0.25, size = 3.5) +
  scale_y_continuous(limits = c(0, max_n * 1.18), expand = expansion(mult = c(0, 0.02))) +
  labs(x = "Month (Nov–May)", y = "Number of forecasts") +
  theme_minimal(base_size = 12) +
  theme(
    plot.margin = margin(t = 22, r = 10, b = 5, l = 10)    # extra space above
  )

# Combine with a title 
(p_prop / p_counts) +
  plot_layout(heights = c(1, 1)) +
  plot_annotation(
    title = "Figure 4. Seasonality of Observed Avalanche Hazard (OAH): Nov–May",
    theme = theme(
      plot.title = element_text(size = 15, face = "bold"),
      plot.subtitle = element_text(size = 11)
    )
  )



```

Figure 4 shows the monthly distribution of observed avalanche hazard (OAH) across the extended avalanche season from November to May. Early (Nov–Dec) and late (Apr–May) months show a greater proportion of low and moderate hazards, while the mid-season months (Jan–Mar) contain a higher share of considerable and high hazards. These seasonal patterns show the influence of Scotland's maritime climate where snowpack instability peaks in mid-winter (Diggins, 2009; Podzdnoukhov et al., 2008; Purves et al., 2003). The count data reveal that most forecasts are concentrated between December and March, reinforcing that these months the period of greatest avalanche risk (SAIS, 2024). In terms of implications for modelling, the seasonality shows that hazard levels are not evenly distributed throughout the year, but instead follow a cyclical pattern with concentrated periods of instability. Neural networks are well-suited to this dataset as they can capture complex, changing relationships between weather conditions and other important variables.

### d. Temperature

```{r temperature-plot}

# Plot 1: Boxplot of Air.Temp by OAH (Figure 5)
data %>%
  filter(!is.na(Air.Temp), !is.na(OAH)) %>%
  ggplot(aes(x = OAH, y = Air.Temp, fill = OAH)) +
  geom_boxplot(outlier.alpha = 0.3) +
  scale_fill_manual(values = hazard_cols, drop = FALSE, guide = "none") +
  labs(
    title = "Figure 5: Distribution of Air Temperature across Observed Avalanche Hazard (OAH)",
    x = "Observed Avalanche Hazard (OAH)",
    y = "Air Temperature (°C)"
  ) +
  theme(
    plot.title = element_text(size = 12, face = "bold") # smaller than before
  )


```

Figure 5 shows the distribution of air temperature across observed avalanche hazard (OAH) levels. Higher hazard categories are generally associated with lower air temperatures, with “Considerable -”, “Considerable +”, and “High” hazards centred below 0 °C. “Low” hazard days show medians above freezing and wider variability. The transition at “Moderate” hazard, where temperatures cluster around 0 °C reflects conditions that can either stabilise or destabilise the snowpack, supporting earlier findings that avalanche activity in Scotland is highly sensitive to small and rapid shifts in temperature due to its maritime climate (Ward, 1980; Diggins, 2009; Pozdnoukhov et al., 2008).

The figure highlights air temperature as a critical factor for avalanche hazard, reinforcing the need to include it in forecasting approaches. The observed relationship aligns with prior literature emphasising the destabilising effects of freeze–thaw variability and sub-zero conditions. A neural network model will be able to capture the interactions between temperature and other snowpack features.

### e. FAH vs OAH

```{r OAH-FAH-plot}
library(dplyr)
library(tidyr)
library(ggplot2)
library(scales)
library(stringr)

# helpers 
find_col <- function(df, patterns) {
  nm <- names(df)
  hits <- sapply(patterns, function(p) nm[grepl(p, nm, ignore.case = TRUE)], simplify = FALSE)
  hit <- unique(unlist(hits))
  if (length(hit)) hit[1] else NULL
}

parse_datetime_safe <- function(x) {
  if (inherits(x, "POSIXct") || inherits(x, "Date")) return(as.POSIXct(x))
  if (!is.character(x)) x <- as.character(x)
  as.POSIXct(x, tz = "UTC",
             tryFormats = c("%Y-%m-%d %H:%M:%S",
                            "%Y-%m-%d %H:%M",
                            "%Y-%m-%d",
                            "%d/%m/%Y %H:%M:%S",
                            "%d/%m/%Y %H:%M",
                            "%d/%m/%Y",
                            "%m/%d/%Y %H:%M:%S",
                            "%m/%d/%Y %H:%M",
                            "%m/%d/%Y"))
}

#  hazard levels + normaliser
hazard_levels <- c("Low","Moderate","Considerable -","Considerable +","High")

norm_hazard <- function(x){
  x0 <- str_squish(tolower(as.character(x)))
  case_when(
    x0 == "low" ~ "Low",
    x0 == "moderate" ~ "Moderate",
    x0 %in% c("considerable -","considerable−","considerable–","considerable minus") ~ "Considerable -",
    x0 %in% c("considerable +","considerable plus")                                   ~ "Considerable +",
    x0 == "high" ~ "High",
    TRUE ~ NA_character_
  )
}

# ensure FAH/OAH columns exist 
if (!"fah" %in% names(dat)) {
  FAH_col <- find_col(dat, c("^FAH$", "forecast.*hazard"))
  if (!is.null(FAH_col)) dat <- mutate(dat, fah = .data[[FAH_col]])
}
if (!"oah" %in% names(dat)) {
  OAH_col <- find_col(dat, c("^OAH$", "observed.*hazard"))
  if (!is.null(OAH_col)) dat <- mutate(dat, oah = .data[[OAH_col]])
}

#  find Date and Area 
date_col <- find_col(dat, c("^date$", "forecast.*date", "datetime", "time"))
area_col <- find_col(dat, c("^area$", "region", "zone"))

if (is.null(date_col)) stop("No date-like column found. Expected something like 'Date', 'date', or 'datetime'.")
dat <- dat %>% mutate(.Date = parse_datetime_safe(.data[[date_col]]))
if (all(is.na(dat$.Date))) stop("Could not parse dates in the detected date column.")

#  next-day alignment: -
if (!is.null(area_col)) {
  dat_shifted <- dat %>%
    mutate(.Area = .data[[area_col]]) %>%
    arrange(.Area, .Date) %>%
    group_by(.Area) %>%
    mutate(oah_nextday_raw = lead(oah, 1)) %>%
    ungroup()
} else {
  dat_shifted <- dat %>%
    arrange(.Date) %>%
    mutate(oah_nextday_raw = lead(oah, 1))
}

# tidy and normalise labels 
df <- dat_shifted %>%
  transmute(
    FAH = factor(norm_hazard(fah), levels = hazard_levels),
    OAH_next = factor(norm_hazard(oah_nextday_raw), levels = hazard_levels)
  ) %>%
  drop_na(FAH, OAH_next)

# contingency + proportions (each FAH column = 100%) 
tab_counts <- with(df, table(OAH_next, FAH))
tab_prop   <- prop.table(tab_counts, 2)


# Figure 6 
df_prop <- as.data.frame(tab_prop) %>%
  rename(p = Freq, OAH = OAH_next) %>%
  mutate(on_diag = as.character(OAH) == as.character(FAH))

ggplot(df_prop, aes(x = FAH, y = OAH, fill = p)) +
  geom_tile(color = "white") +
  geom_tile(data = dplyr::filter(df_prop, on_diag),
            color = "black", linewidth = 0.9, fill = NA) +
  geom_text(aes(label = percent(p, accuracy = 1)), size = 3.2) +
  scale_fill_gradient(low = "white", high = "#238b45", labels = percent) +
  labs(
    title = "Figure 6. FAH (day t) → OAH (day t+1) proportions",
    subtitle = "Each FAH column sums to 100%; next-day alignment performed within Area (if available)",
    x = "Forecast Avalanche Hazard (FAH, day t)",
    y = "Observed Avalanche Hazard (OAH, day t+1)",
    fill = "Proportion"
  ) +
  theme_minimal(base_size = 12) +
  theme(axis.text.x = element_text(angle = 40, hjust = 1),
        plot.title = element_text(face = "bold", size = 14))



```

Figure 6 shows the relationship between forecast avalanche hazard (FAH) on day *t* and observed avalanche hazard (OAH) on day *t+1*. Each column sums to 100%, showing how outcomes were distributed given a specific forecast level. There is a strong alignment at the lower hazard categories as forecasts (FAH) of “Low” hazards were accurate in the majority of cases, with 87% of observations (OAH) also recorded as “Low.” This indicates that forecasters were most successful in predicting stable snowpack conditions and low hazard scenarios.

For intermediate levels such as “Moderate” and “Considerable –” the agreement weakens. Only about half of “Moderate” forecasts matched the following day’s observations, with a substantial spillover into “Low” and “Considerable –”. “Considerable –” forecasts split almost evenly between being observed as “Moderate,” “Considerable –,” and “Considerable +.” This pattern shows that there is a challenge in differentiating between neighbouring hazard categories, which may highlight the subjectivity and sensitivity of these intermediate hazard thresholds.

Forecasts of “High” hazard showed weaker reliability. Fewer than one-quarter of these forecasts corresponded to an observed “High” hazard the next day. Most “High” forecasts aligned with “Considerable –” or “Considerable +” outcomes. This shows that there is a tendency towards over-prediction of extreme conditions, or difficulty in anticipating when weather and snowpack instabilities will escalate into truly high-risk scenarios.

The plot shows that the forecast (FAH) is highest in stable, low-risk conditions and lowest at the extremes, with the middle categories marked by uncertainty. This implies that avalanche hazard ratings are best treated as ordinal outcomes for modeeling purposes, where the cost of misclassification is not uniform and depends on how far apart the categories are. Neural networks can capture these patterns well when framed as ordinal classfiers. They can learn that misclassifications most often occur between neighbouring hazard levels, instead of treating all errors equally. Class imbalances will also need to be addressed during model training to ensure the network does not simply default to predicting the most common outcomes.

### f. Regional Dominance??

```{r}

```

### g. Missing Values

```{r}

```

## Feature Engineering

```{r read_data}


data = read.csv("scotland_avalanche_forecasts_2009_2025.csv")

# row_unique_counts <- apply(
#   df[ , sapply(df, is.character)],  
#   1,                                
#   function(row) length(unique(row)) 
# )


data <- data %>% 
  mutate(across(where(is.character), as.factor))

c <- colnames(data)
r <- data.frame(var = character(),
                no = numeric())

for (i in c){
  
  if (is.factor(data[, i])){
  
  x <- length(unique(data[,i]))
  
  r <- rbind(r, data.frame(var = i,
                           no = x))
    
  }
  
  
}





```

```{r missing_val}

#Getting table of missing values


col.n <- colnames(data)
n <- nrow(data)

missing_df <- data.frame(Variable = character(),
                         "Missing (%)" = numeric(),
                         check.names = F)


for (i in col.n){
  
  x <- sum(is.na(data[,i]))
  
  perc <- (x/n) * 100
  
  missing_df <- rbind(missing_df,
                      data.frame(
                        Variable = i,
                        `Missing (%)` = perc
                      , check.names = F)
                      )
  
}
 
missing_df <- missing_df %>% 
  filter(`Missing (%)` > 0) %>% 
  arrange(desc(`Missing (%)`),)


```

```{r tablex}



kable(missing_df, 
      format = "html",
      digits = 2,
      caption = "Missing Values") %>%
  kable_styling(
    bootstrap_options = c("hover"),
    full_width = FALSE
  ) %>%
  footnote(general = "")


```

```{r check_missing_fnx}

# a few functions to help me evaluate the extent of missing values

#Find percentage of missing values per Column
find_missing <- function(a){
  
  
col.n <- colnames(a)
n <- nrow(a)
missing_df <- data.frame(Variable = character(),
                         "Missing (%)" = numeric(),
                         check.names = F)


for (i in col.n){
  
  x <- sum(is.na(a[,i]))
  
  perc <- (x/n) * 100
  
  missing_df <- rbind(missing_df,
                      data.frame(
                        Variable = i,
                        `Missing (%)` = perc
                      , check.names = F)
                      )
  
}
 
missing_df <- missing_df %>% 
  filter(`Missing (%)` > 0) %>% 
  arrange(desc(`Missing (%)`),)

return(missing_df)

}

#Find total % missing values
find_total_missing <- function(b){
  
  
  count_na <- 0
for (i in 1:nrow(b)){
  if (any(is.na(data[i, ]))){count_na <- count_na +1}
}


count_na/nrow(b)
  
  return(count_na*100/nrow(b) )
}

```

```{r date_time}

#adding date variable
data$Date <- as.POSIXct(data$Date, format="%Y-%m-%d %H:%M:%S", tz="UTC")


data$month     <- lubridate::month(data$Date)
data$hour      <- lubridate::hour(data$Date)
data$year      <- lubridate::year(data$Date)

#make a day of the season variable

#season begins from December to april



data <- data %>% 
  mutate(season_year = if_else(month >= 12, year, year -1),
         season_id = dense_rank(season_year),
         season = paste0("season", season_id),
         season = as.factor(season)) %>% 
  group_by(season, Area) %>% 
  mutate(dayofseason = row_number()) %>% 
  arrange(Date, Area)

  

36 - 4 + 26

```

```{r drop variables}

#Drop the top 3 variables wtih the most missing values
data <- data %>% 
  select(- c(AV.Cat, Ski.Pen, Summit.Wind.Dir, Obs, Location, OSgrid))

data <- na.omit(data) #remove remaining missing

#any(is.na(data))



```

```{r split_data}

set.seed(1)
train.idx <- sample(1:nrow(data), 
                    round(nrow(data)*0.8))

train <- data[train.idx,] #Training data
test <- data[-train.idx,] #Testing Data



```

```{r Keras_data_preprocessing_function}

library(keras3)
library(tensorflow)
library(reticulate)

# reticulate::install_miniconda()   # sets up a private Python just for R
# library(keras3)
# install_tensorflow()


#for this to work I need to convert the data into an array 
# form is (N (length), L (Time frame), F (Features))

#I promise that this code isnt made by Chatgpt I am just using a lot of comments otherwise I dont know what the #$!* is going on :)


#Function to let me test for numerous window sizes


pre_data <- function(w = 3, h = 1){

#--------Data Preparation-------------------------------------

T <- length(data$Date) #time period length

data_nn <- data %>%
  select(-c(Date)) #removing date column

L <- w #Rolling window length = 3. So what I am decided is that It can use the previus 3 days on record to train itself
H <- h # predict 1 day ahead


#number of samples we can have, so number of time slices, each one iterated one step ahead by H (which is just 1 day in our case)
N_samp <- T - L - H + 1



#----------------One hot encoding--------

#This part is quite a process. So big picture, the numeric data needs to be standardised, but when we do one-hot-encoding, it becomes quite messy to avoid standardising the dummy variables. SO we are going to make indices for the numeric columns so we can standardise only them later

#Taking our predictors and covariates into one df, and our labels into another

X.variables <- data_nn %>%
  select(-OAH)   #x_variables = everything in data other than OAH column

X.variables <- model.matrix(~., data = X.variables)[,-1] # One Hot encoding

y.variable <- data_nn[["OAH"]]  #y_variable = only our OAH column

numeric.variables <- names(data_nn)[sapply(data_nn, is.numeric)]
numeric.variables <- setdiff(numeric.variables, "OAH")

numeric.indexes <- which(colnames(X.variables) %in% numeric.variables) #okay so the colnames are the same so we can find the indexes in x variables which are numeric with this code <--

#dummy variable indexes
dummy.indexes <- setdiff(1:(ncol(X.variables)), numeric.indexes)


#Number features

F_ <- ncol(X.variables)

#--------------Making Array---------------------#


#Arrays - this is the confusing part. Keras needs an array for this to work. In our case we can think of this as turning our data into a stack of data sheets. each sheet is for a particular feature. THen there are N_sampl columns, and each is shifted by L

X <- array( NA_real_, #just an empty array, which will subsequently be filled
           dim = c(N_samp,L, F_  ),
           )

y <- integer(N_samp)




for (i in 1:N_samp){ #here we are filling in the array with our data

  index.window <- i:(i + L - 1) #index wind = ith value in N_samp to I + window minus 1

  X[i, , ] <- X.variables[index.window, , drop = F] #filling in the predictors with the corresponding indexes into that window
  y[i] <- y.variable[i + L + H -1] #we predict the ith + L + H y values.

}



#------------Training and testing split-----------------------------

#We want to preserve time series patterns, so no shuffling

#old version where everything was in order
idx <- 1:N_samp
number.training <- floor(N_samp * 0.8) #train = first 80% of features

tr <- idx[1:number.training] #Training indices
te <- idx[(number.training + 1):N_samp] #Testing indices

#making 1 = 0 to see if that helps

y <- y - 1 # Now categories are {0,1,2,3,4,5,}

#test for randomness being better (didnt work)
# idx <- 1:N_samp
# set.seed(1)
# tr <- sample(idx, floor(0.8*N_samp))
# te <- idx[-tr]



X.train <- X[tr, , , drop = F] #training predictors data array
X.test <- X[te, , , drop = F ] #testing predictors array
y.train <- y[tr] #training target
y.test <- y[te]


#Class weights
class_counts <- table(y.train)
total <- sum(class_counts)
n_classes <- length(class_counts)


class_weights <- total / (n_classes * class_counts)

# keras expects a named list: names = class index as character
class_weights <- as.list(class_weights)
names(class_weights) <- as.character(0:(n_classes-1))



#----------------Scaling--------------------

#Remeber that we made indexes for our numeric data, which is the data we want to scale.

#We also need to scale the training data, and scale the testing data according to the training data mu and sd


if (length(numeric.indexes) > 0){

  mu.numeric <- apply(X.train[, , numeric.indexes, drop = F], 3, mean, na.rm = T)

  sd.numeric <- apply(X.train[ , , numeric.indexes, drop = F], 3, sd, na.rm = T)

   sd.numeric[sd.numeric == 0 | is.na(sd.numeric)] <- 1 #make sure SD == 1

  for (j in seq_along(numeric.indexes)){

    k <- numeric.indexes[j]
    X.train[, , k] <- (X.train[ , , k] - mu.numeric[j])/sd.numeric[j]
    X.test[, , k] <- (X.test[ , , k] - mu.numeric[j])/sd.numeric[j]
  }


}


round(apply(X.train[ , , numeric.indexes, drop = FALSE], 3, mean), 3)
round(apply(X.train[ , , numeric.indexes, drop = FALSE], 3, sd),   3)



#Getting there !

#Since we are doing a multiclass prediction we need to use softmax
#This means using the tocategorical() function on y

y.test <- to_categorical(y.test, num_classes = 6)


y.train <- to_categorical(y.train, num_classes = 6)

list(y.test, y.train, X.train, X.test, class_weights, F_)





}
```

```{r keras_grid_search}
#testing c-d-c

#vary window size hasnt made significant changes
#Adding weights to control for imbalance didnt help

keras_search <- function(window){

W_ <- pre_data(w = window, h = 1)

y.test <- W_[[1]]
y.train <- W_[[2]]
X.train <- W_[[3]]
X.test <- W_[[4]]
cl_weights <- W_[[5]]
F_ <- W_[[6]]
# ---------------- Grid search for existing architecture ----------------


set.seed(1)
tf$keras$utils$set_random_seed(123L)

# Define the grid (adjust values if you want a bigger/smaller search)
grid <- expand.grid(
  filters1   = c( 32, 64),
  filters2   = c( 32, 64),
  kernel_sz  = c(2, 5, 15),
  drop1      = c(0.3, 0.5),
  drop2      = c(0.3, 0.5),
  stringsAsFactors = FALSE
)

results <- data.frame(
  filters1 = integer(), filters2 = integer(),
  kernel_sz = integer(), drop1 = double(), drop2 = double(),
  val_accuracy = double(), test_loss = double(), test_accuracy = double(),
  stringsAsFactors = FALSE
)

histories <- vector("list", nrow(grid))  # optional: keep training histories

for (i in seq_len(nrow(grid))) {
  cat(sprintf("\n[%d/%d] f1=%d, f2=%d, k=%d, d1=%.2f, d2=%.2f\n",
              i, nrow(grid),
              grid$filters1[i], grid$filters2[i],
              grid$kernel_sz[i], grid$drop1[i], grid$drop2[i]))

  # Clear TF/Keras state between runs to avoid graph/memory accumulation
  tensorflow::tf$keras$backend$clear_session()

  
  set.seed(1)
  tf$keras$utils$set_random_seed(123L)

 
  input <- layer_input(shape = c(dim(X.train)[2], F_)) # (L, F)

  output <- input %>%
    layer_conv_1d(filters = grid$filters1[i],
                  kernel_size = grid$kernel_sz[i],
                  padding = "same", activation = "relu") %>%
    layer_dropout(rate = grid$drop1[i]) %>%
    layer_conv_1d(filters = grid$filters2[i],
                  kernel_size = grid$kernel_sz[i],
                  padding = "same", activation = "relu") %>%
    layer_global_average_pooling_1d() %>%
    layer_dropout(rate = grid$drop2[i]) %>%
    layer_dense(units = 6, activation = "softmax")

  model <- keras_model(inputs = input, outputs = output)

  model %>% compile(
    optimizer = "adam",
    loss = "categorical_crossentropy",
    metrics = "accuracy"
  )

  # ----- Train -----
  history <- model %>% fit(
    X.train, y.train,
    epochs = 60,
    batch_size = 64,
    verbose = 0,
    validation_split = 0.2,
    shuffle = FALSE
    # = cl_weights
  )

  # collect validation accuracy from the last epoch
  val_acc <- tail(history$metrics$val_accuracy, 1)
  if (length(val_acc) == 0) {
   
    val_acc <- tail(history$metrics$val_acc, 1)
  }

  # Evaluate on test set 
  test_metrics <- model %>% evaluate(X.test, y.test, verbose = 0)

  # test_metrics is usually a named numeric vector: c(loss=..., accuracy=...)
  test_loss <- as.numeric(test_metrics[[1]])
  test_acc  <- if (!is.null(names(test_metrics)) && "accuracy" %in% names(test_metrics)) {
    as.numeric(test_metrics[["accuracy"]])
  } else {
    as.numeric(test_metrics[[2]])
  }

  # store results
  results[i, ] <- list(
    grid$filters1[i], grid$filters2[i],
    grid$kernel_sz[i], grid$drop1[i], grid$drop2[i],
    as.numeric(val_acc), test_loss, test_acc
  )
  histories[[i]] <- history
}

# sort by best validation accuracy 
results <- results[order(-results$val_accuracy), ]

print(results)

# The best 
best <- results[1, ]

rm(W_) #For saving memory
gc()

return(results)

}



```

```{r keras_search}
#Window = 3 

#res_window_3 <- keras_search(window = 3)

#save(res_window_3, file =  "res_window_3")

#Window  = 7
#res_window_7 <- keras_search(window = 7)
#save(res_window_7, file =  "res_window_7")

#Window = 30
#res_window_30 <- keras_search(window = 30)
#save(res_window_30, file =  "res_window_30")
#window = 

#Window = 120
#res_window_120 <- keras_search(window = 120)
#save(res_window_120, file =  "res_window_120")

load("res_window_3")
load("res_window_7")
load("res_window_30")
load("res_window_120")

res_window_3 <- res_window_3 %>% 
  mutate(model_no = row_number(),
         window  = 3)

res_window_7 <- res_window_7 %>% 
  mutate(model_no = row_number(),
         window  = 7)

res_window_30 <- res_window_30 %>% 
  mutate(model_no = row_number(),
         window  = 30)

res_window_120 <- res_window_120%>% 
  mutate(model_no = row_number(),
         window  = 120)


results_cnn_search <- rbind(res_window_3, res_window_30, res_window_7,  res_window_120)
```

```{r cnn_window_vis}
#graph of validation accuracy
ggplot(data = results_cnn_search, aes( x  = model_no, y  = val_accuracy, colour = as.factor(window)))+
  geom_point()+
  labs(x = "Model Number (arranged by  validation accuracy)")



```

```{r}
#we will go with a window of roughly  7  days
rm(res_window_120,  res_window_3, res_window_30)


#I  am going to do one more expanded search, just with a window  of 7 days

# res_window_7[which.max(res_window_7$val_accuracy),]
# 
# #serach
# 
# 
# 
# W_ <- pre_data(w = 3, h = 1)
# 
# y.test <- W_[[1]]
# y.train <- W_[[2]]
# X.train <- W_[[3]]
# X.test <- W_[[4]]
# cl_weights <- W_[[5]]
# F_ <- W_[[6]]
# 
# 
# # Repro seeds 
# set.seed(1)
# tf$keras$utils$set_random_seed(123L)
# 
# # Define the grid (
# grid <- expand.grid(
#   filters1   = c( 16, 32, 64),
#   filters2   = c( 16, 32, 64),
#   kernel_sz  = c(5,15),
#   drop1      = c(0.5,0.7),
#   drop2      = c(0.5, 0.7),
#   stringsAsFactors = FALSE
# )
# 
# results <- data.frame(
#   filters1 = integer(), filters2 = integer(),
#   kernel_sz = integer(), drop1 = double(), drop2 = double(),
#   val_accuracy = double(), test_loss = double(), test_accuracy = double(),
#   stringsAsFactors = FALSE
# )
# 
# histories <- vector("list", nrow(grid))  # optional: keep training histories
# 
# for (i in seq_len(nrow(grid))) {
#   cat(sprintf("\n[%d/%d] f1=%d, f2=%d, k=%d, d1=%.2f, d2=%.2f\n",
#               i, nrow(grid),
#               grid$filters1[i], grid$filters2[i],
#               grid$kernel_sz[i], grid$drop1[i], grid$drop2[i]))
# 
#   # Clear TF/Keras state between runs to avoid graph/memory accumulation
#   tensorflow::tf$keras$backend$clear_session()
# 
#  
#   set.seed(1)
#   tf$keras$utils$set_random_seed(123L)
# 
#  
#   input <- layer_input(shape = c(dim(X.train)[2], F_)) # (L, F)
# 
#   output <- input %>%
#     layer_conv_1d(filters = grid$filters1[i],
#                   kernel_size = grid$kernel_sz[i],
#                   padding = "same", activation = "relu") %>%
#     layer_dropout(rate = grid$drop1[i]) %>%
#     layer_conv_1d(filters = grid$filters2[i],
#                   kernel_size = grid$kernel_sz[i],
#                   padding = "same", activation = "relu") %>%
#     layer_global_average_pooling_1d() %>%
#     layer_dropout(rate = grid$drop2[i]) %>%
#     layer_dense(units = 6, activation = "softmax")
# 
#   model <- keras_model(inputs = input, outputs = output)
# 
#   model %>% compile(
#     optimizer = "adam",
#     loss = "categorical_crossentropy",
#     metrics = "accuracy"
#   )
# 
#   # ----- Train -----
#   history <- model %>% fit(
#     X.train, y.train,
#     epochs = 60,
#     batch_size = 64,
#     verbose = 0,
#     validation_split = 0.2,
#     shuffle = FALSE
#     # = cl_weights
#   )
# 
#   # Collect validation accuracy from the last epoch
#   val_acc <- tail(history$metrics$val_accuracy, 1)
#   if (length(val_acc) == 0) {
#     # Fallback name in case of different metric key
#     val_acc <- tail(history$metrics$val_acc, 1)
#   }
# 
#   # Evaluate on test set 
#   test_metrics <- model %>% evaluate(X.test, y.test, verbose = 0)
# 
#   # test_metrics is usually a named numeric vector: 
#   test_loss <- as.numeric(test_metrics[[1]])
#   test_acc  <- if (!is.null(names(test_metrics)) && "accuracy" %in% names(test_metrics)) {
#     as.numeric(test_metrics[["accuracy"]])
#   } else {
#     as.numeric(test_metrics[[2]])
#   }
# 
#   # Store results
#   results[i, ] <- list(
#     grid$filters1[i], grid$filters2[i],
#     grid$kernel_sz[i], grid$drop1[i], grid$drop2[i],
#     as.numeric(val_acc), test_loss, test_acc
#   )
#   histories[[i]] <- history
# }
# 
# # Sort by best validation accuracy 
# results <- results[order(-results$val_accuracy), ]
# 
# print(results)
# 
# final_cnn_search  <-  results
#save(final_cnn_search, file =  "final_cnn_serach")


load("final_cnn_serach")

# The best 
best <- final_cnn_search[1, ]

best

gc()
```

```{r}


res <- final_cnn_search %>%
  mutate(
    run_id = row_number(),
    label  = sprintf("f1=%d f2=%d k=%d d1=%.1f d2=%.1f",
                     filters1, filters2, kernel_sz, drop1, drop2)
  ) %>%
  arrange(desc(val_accuracy)) %>%
  mutate(rank = row_number(),
         top5 = rank <= 5)

library(scales)


top_n <- 30 
ggplot(res %>% slice(1:top_n),
       aes(x = reorder(label, val_accuracy), y = val_accuracy, fill = top5)) +
  geom_col() +
  coord_flip() +
  geom_text(aes(label = percent(val_accuracy, accuracy = 0.1)),
            hjust = -0.1, size = 3) +
  scale_y_continuous(labels = percent_format(accuracy = 1), expand = expansion(mult = c(0, .1))) +
  scale_fill_manual(values = c("grey80", "grey40"), guide = "none") +
  labs(title = sprintf("Top %d configs by validation accuracy", top_n),
       x = "Config", y = "Val accuracy")

```

```{r}
#best model  had 16 filters with 2 layers,  and kernel size 15,  0.7 dropout between layers

#Fit  final model on full  training data

  input <- layer_input(shape = c(dim(X.train)[2], F_)) # (L, F)

  output <- input %>%
    layer_conv_1d(filters = 16, kernel_size = 15, padding = "same", activation = "relu") %>%
    layer_dropout(rate = 0.7) %>%
    layer_conv_1d(filters = 16, kernel_size = 15, padding = "same", activation = "relu") %>%
    layer_global_average_pooling_1d() %>%
    layer_dropout(rate = 0.7) %>%
    layer_dense(units = 6, activation = "softmax")

  model <- keras_model(inputs = input, outputs = output)

  model %>% compile(
    optimizer = "adam",
    loss = "categorical_crossentropy",
    metrics = "accuracy"
  )

    history <- model %>% fit(
    X.train, y.train,
    epochs = 60,
    batch_size = 64,
    verbose = 0,
    validation_split = 0,
    shuffle = FALSE
    # = cl_weights
  )

final_cnn_model  <- model    
save(final_cnn_model,  file = "final_cnn_model")
load("final_cnn_model")

test_metrics <- model %>% evaluate(X.test, y.test, verbose = 0)

test_metrics$accuracy



```

```{r  RNN}
#sam as above but for RNN model
keras_search_rnn <- function(window){

W_ <- pre_data(w = window, h = 1)

y.test <- W_[[1]]
y.train <- W_[[2]]
X.train <- W_[[3]]
X.test <- W_[[4]]
cl_weights <- W_[[5]]
F_ <- W_[[6]]
# ---------------- Grid search architecture ----------------

# 
set.seed(1)
tf$keras$utils$set_random_seed(123L)

# grid
grid <- expand.grid(
  filters1   = c( 32, 64),
  filters2   = c( 32, 64),
  kernel_sz  = c(5, 15),
  drop1      = c(0.3, 0.5),
  drop2      = c(0.3, 0.5),
  stringsAsFactors = FALSE
)

results <- data.frame(
  filters1 = integer(), filters2 = integer(),
  kernel_sz = integer(), drop1 = double(), drop2 = double(),
  val_accuracy = double(), test_loss = double(), test_accuracy = double(),
  stringsAsFactors = FALSE
)

histories <- vector("list", nrow(grid))  #  keep training histories

for (i in seq_len(nrow(grid))) {
  cat(sprintf("\n[%d/%d] f1=%d, f2=%d, k=%d, d1=%.2f, d2=%.2f\n",
              i, nrow(grid),
              grid$filters1[i], grid$filters2[i],
              grid$kernel_sz[i], grid$drop1[i], grid$drop2[i]))

  # Clear TF/Keras state between runs to avoid graph/memory accumulation
  tensorflow::tf$keras$backend$clear_session()

  # r-seed before each run to keep trials comparable
  set.seed(1)
  tf$keras$utils$set_random_seed(123L)

  # ----- Build model as an RNN (LSTM) using the same grid fields -----
  input <- layer_input(shape = c(dim(X.train)[2], F_)) # (L, F)

  output <- input %>%
    layer_lstm(units = grid$filters1[i], return_sequences = TRUE) %>%
    layer_dropout(rate = grid$drop1[i]) %>%
    layer_lstm(units = grid$filters2[i], return_sequences = FALSE) %>%
    layer_dropout(rate = grid$drop2[i]) %>%
    layer_dense(units = 6, activation = "softmax")

  model <- keras_model(inputs = input, outputs = output)

  model %>% compile(
    optimizer = "adam",
    loss = "categorical_crossentropy",
    metrics = "accuracy"
  )

  # ----- Train -----
  history <- model %>% fit(
    X.train, y.train,
    epochs = 60,
    batch_size = 64,
    verbose = 0,
    validation_split = 0.2,
    shuffle = FALSE
    # = cl_weights
  )

  # Collect validation accuracy from the last epoch
  val_acc <- tail(history$metrics$val_accuracy, 1)
  if (length(val_acc) == 0) {
    # Fallback name 
    val_acc <- tail(history$metrics$val_acc, 1)
  }

  # Evaluate on test set 
  test_metrics <- model %>% evaluate(X.test, y.test, verbose = 0)

  # test_metrics is usually a named numeric 
  test_loss <- as.numeric(test_metrics[[1]])
  test_acc  <- if (!is.null(names(test_metrics)) && "accuracy" %in% names(test_metrics)) {
    as.numeric(test_metrics[["accuracy"]])
  } else {
    as.numeric(test_metrics[[2]])
  }

  # Store results
  results[i, ] <- list(
    grid$filters1[i], grid$filters2[i],
    grid$kernel_sz[i], grid$drop1[i], grid$drop2[i],
    as.numeric(val_acc), test_loss, test_acc
  )
  histories[[i]] <- history
}


results <- results[order(-results$val_accuracy), ]

print(results)

# The best 
best <- results[1, ]

rm(W_) #For saving memory
gc()

return(results)

}



```

```{r}



#Window = 3 

rnn_window_3 <- keras_search(window = 3)
save(rnn_window_3, file =  "res_window_3")

#Window  = 7
rnn_window_7 <- keras_search(window = 7)
save(rnn_window_7, file =  "res_window_7")

#Window = 30
rnn_window_30 <- keras_search(window = 30)
save(rnn_window_30, file =  "res_window_30")
 

#Window = 120
rnn_window_120 <- keras_search(window = 120)
save(rnn_window_120, file =  "res_window_120")

load("rnn_window_3")
load( "rnn_window_7")
load("rnn_window_30")
load("rnn_window_120")

rnn_window_3 <- res_window_3 %>% 
  mutate(model_no = row_number(),
         window  = 3)

rnn_window_7 <- res_window_7 %>% 
  mutate(model_no = row_number(),
         window  = 7)

rnn_window_30 <- res_window_30 %>% 
  mutate(model_no = row_number(),
         window  = 30)

rnn_window_120 <- res_window_120%>% 
  mutate(model_no = row_number(),
         window  = 120)


```

## Modelling

## Modelling

```{r libs-and-seed}
# Load libraries for data wrangling, visualization, preprocessing, and modeling
library(dplyr)
library(tidyverse)
library(ggplot2)
library(janitor)
library(caTools)
library(caret)
library(e1071)
library(readr)
library(tidyverse)
library(janitor)
library(lubridate)
library(skimr)
library(naniar)
library(GGally)
library(corrplot)
library(factoextra)
library(cluster)
library(rsample)
library(recipes)
library(scales)
library(cowplot)
library(pROC)
library(reshape2)

# Load the required libraries so their functions are available for use.
library(xgboost)   # Core API for training and using XGBoost models.
library(caret)     # High-level interface for model training, tuning, and evaluation.
library(pROC)      # Provides functions for ROC curve plotting and AUC computation.
library(pdp)       # Used to generate Partial Dependence Plots for model interpretation.

# Set the random seed for reproducibility.
# Ensures that random processes like resampling and model training are consistent across runs.
set.seed(123)
```

```{r data-loading}
# # Load raw data from CSV
# df_raw <- read_csv("scotland_avalanche_forecasts_2009_2025.csv")
# 
# # Clean column names -> snake_case
# df <- df_raw %>% clean_names()
# 
# # Fix Cols
# df <- df %>% mutate(date = parse_date_time(date, orders = c("ymd HMS", "ymd HM", "ymd"), tz = "Europe/London"),
#                     fah = as.factor(fah),
#                     oah = as.factor(oah),
#                     area = as.factor(area),
#                     obs = as.factor(obs))
# 
# 
# # Inspect structure of cleaned dataset
# head(df)
```

```{r target-var-distr}
# ggplot(df, aes(x = oah)) + geom_bar() + labs(title = "Observed Avalanche Hazard (OAH)") + theme_minimal()
# 
# library(ggplot2)
# library(dplyr)
# 
# # First, prepare the data with counts and percentages
# df_counts <- df %>%
#   count(oah) %>%
#   mutate(percent = n / sum(n) * 100)
# 
# # Plot
# ggplot(df_counts, aes(x = reorder(oah, -n), y = n, fill = oah)) +
#   geom_bar(stat = "identity", show.legend = FALSE) +
#   geom_text(aes(label = paste0(n, " (", round(percent, 1), "%)")), 
#             vjust = -0.5, size = 4) +
#   labs(
#     title = "Observed Avalanche Hazard (OAH)",
#     x = "Hazard Level",
#     y = "Count"
#   ) +
#   theme_minimal(base_size = 14) +
#   theme(
#     plot.title = element_text(face = "bold", hjust = 0.5),
#     axis.title.y = element_text(margin = margin(r = 10))
#   ) +
#   scale_y_continuous(expand = expansion(mult = c(0, 0.1)))


# ggplot(train, aes(x = oah)) + geom_bar() + labs(title = "Observed Avalanche Hazard (OAH)") + theme_minimal()

# ggplot(test, aes(x = oah)) + geom_bar() + labs(title = "Observed Avalanche Hazard (OAH)") + theme_minimal()
```

```{r data-preparation}
## ---------------------------
# Data preparation 
## ---------------------------
# Define target column
#target <- "OAH" #changed from oah to OAH

# ---------------------------
# Train-test split
# ---------------------------
#set.seed(123)
#MATT EDIT: commenting this out because we are going to use my data above made before modeling going forward
# split <- sample.split(df[[target]], SplitRatio = 0.7)
# train <- subset(df, split == TRUE)
# test  <- subset(df, split == FALSE)
```

```{r drop-missing}
# ---------------------------
# Drop rows with missing values in both target and predictors
# ---------------------------
# Our predictors, col names
# setdiff(a, b): Returns elements in a that are not in b.

#predictor_cols <- setdiff(names(data), target) #Matt edit, replaced df with data, to be consistent as above

#Explanation:
# train[, c(target, predictor_cols)]: Selects columns from train that are either the target or one of the predictors.
# complete.cases(...): Returns a logical vector indicating which rows have no missing values (NA) across the selected columns.
# train[ ..., ]: Subsets the train data to only those rows that have complete data in both the target and predictors.
# Result:
#   Removes rows with missing values in any of the important columns (target or predictors) from the training set.
# train <- train[complete.cases(train[, c(target, predictor_cols)]), ]
# test  <- test[complete.cases(test[, c(target, predictor_cols)]), ]
```

```{r target-var-factor-encoding}
# ---------------------------
# Target variable conversion
#   Change the Target Variable (y) in our data into factor with all levels,
#   making sure all data has levels from whole data even if it doesn't show in train or test
#   because of the split.
# ---------------------------

#Matt comment: The data was converted to factors right at the begining of the data set so I dont think this is a necessary step. The test and training have the same factors levels.

# # Extract all possible levels (classes) of the target variable from the full dataset
# # This ensures consistency in factor levels across training and test sets, even if some classes are missing in one set.
# target_levels <- levels(factor(data[[target]])) #matt edit changed to "OAH" to make this more clear
# 
# # Convert the target variable in the training set to a factor with levels set to target_levels
# # This makes sure the factor levels are in the same order as in the full dataset (important for classification labels).
# train[[target]] <- factor(train[[target]], levels = target_levels)
# 
# # Do the same for the test set
# test[[target]]  <- factor(test[[target]],  levels = target_levels)
```

```{r target-var-encoding}
# # ------------------------------------------------------
# # Numeric encoding for XGBoost (0-based indexing)
# # XGB Model needs all our factor variables encoded into numerical values,
# # so we change all y levels into numbers, STARTING FROM 0 ...so -1L
# # ------------------------------------------------------
# 
# # Convert the factor target in training set to integer values, then subtract 1
# # XGBoost expects class labels as integers starting from 0, so this makes sure labels are 0-based.
# y_train <- as.integer(train[[target]]) - 1L
# 
# # Same conversion for test set
# y_test  <- as.integer(test[[target]]) - 1L
# 
# # Store the original class labels (factor levels)
# # Useful later to map predictions back to human-readable class names (e.g., for confusion matrix or reporting).
# levels_y <- target_levels
# 
# # Count the number of unique classes
# # This is needed when setting the "num_class" parameter in the XGBoost model (for multi-class classification).
# num_class <- length(levels_y)

```

```{r model-feature-matrix}
# # ---------------------------
# # Feature matrix creation
# # ---------------------------
# # Select only predictor columns from the training set
# # 'predictor_cols' contains the names of all input features (excluding the target)
# x_train <- train %>% 
#   select(all_of(predictor_cols))
# 
# # Do the same for the test set
# x_test  <- test %>% 
#   select(all_of(predictor_cols))
# 
# 
# # ------------------------------------------------------------
# # Convert to design matrices (dummy encoding for categorical variables)
# # ------------------------------------------------------------
# 
# # Convert the training data frame to a design matrix using model.matrix
# # '~ . -1' creates a formula that includes all variables (.) but removes the intercept (-1)
# # This will automatically dummy-encode categorical variables into binary columns (one-hot encoding)
# 
# # Combine train and test predictors before encoding
# x_all <- rbind(x_train, x_test)
# 
# # Create design matrix once (ensures consistent dummy columns)
# dm_all <- model.matrix(~ . -1, data = x_all)
# 
# 
# 
# # Split back into train and test matrices
# dm_train <- dm_all[1:nrow(x_train), ]
# 
# dm_test  <- dm_all[(nrow(x_train)+1):nrow(x_all), ]
# # ------------------------------------------------------------
# # Debugging check to confirm alignment
# # ------------------------------------------------------------
# # Output number of rows in design matrix vs. number of labels
# # This is a sanity check to ensure that the feature matrix (dm_train) has the same number of rows
# #   as the target label vector (y_train), which is essential for training
# cat("Rows in dm_train:", nrow(dm_train), "Length of y_train:", length(y_train), "\n")
# 
# # Same check for the test set
# cat("Rows in dm_test:", nrow(dm_test), "Length of y_test:", length(y_test), "\n")

```

```{r dmatrix}
# ---------------------------
# DMatrix creation
#   :( REQUIRED
# ---------------------------
# 
# dtrain <- xgb.DMatrix(data = dm_train, 
#                       label = y_train)
# 
# 
# dtest  <- xgb.DMatrix(data = dm_test,
#                       label = y_test)
```

```{r crossval-earlystop}
# ---------------------------
# Cross-validated early stopping for multi-class XGBoost
# ---------------------------

# # Define the set of hyperparameters for XGBoost cross-validation
# cv_params <- list(
# objective = "multi:softprob", # Predict probabilities for each class (required for multi-class)
# eval_metric = "mlogloss", # Use multi-class logarithmic loss as evaluation metric
# eta = 0.1, # Learning rate: smaller values improve generalization
# max_depth = 6, # Maximum depth of each tree (controls model complexity)
# min_child_weight = 1, # Minimum sum of instance weights needed in a child node
# subsample = 0.8, # Fraction of training data used for each tree (prevents overfitting)
# colsample_bytree = 0.8, # Fraction of features used for each tree (prevents overfitting)
# gamma = 0, # Minimum loss reduction to make a split
# num_class = num_class # Number of classes in the target variable
# )
# 
# # Perform k-fold cross-validation with early stopping
# cv <- xgb.cv(
# params = cv_params, # Hyperparameters defined above
# data = dtrain, # Training data in DMatrix format
# nrounds = 2000, # Maximum number of boosting iterations
# nfold = 5, # Number of folds for cross-validation
# stratified = TRUE, # Ensure class proportions are preserved in folds
# early_stopping_rounds = 25, # Stop if no improvement after 25 rounds
# verbose = 1, # Print progress during training
# maximize = FALSE # Minimize the logloss metric
# )
# 
# # Extract the optimal number of boosting rounds
# best_nrounds <- cv$best_iteration
# cat('Best nrounds from xgb.cv:', best_nrounds, "\n")
```

```{r xgbmodel}

#Matt edit, commented out for now
# ---------------------------
# Final Multi-Class XGBoost Model
# ---------------------------

# # Set hyperparameters (based on CV up there)
# final_params <- list(
#   objective = "multi:softprob",   # Multiclass classification: predict probability for each class
#   eval_metric = "mlogloss",       # Use multiclass log-loss as evaluation metric
#   eta = 0.1,                      # Learning rate: step size shrinkage to prevent overfitting
#   max_depth = 6,                  # Maximum depth of trees (controls model complexity)
#   min_child_weight = 1,           # Minimum sum of instance weights (hessian) needed in a child (controls overfitting)
#   subsample = 0.8,                # Subsample ratio of training instances (row sampling, adds randomness)
#   colsample_bytree = 0.8,         # Fraction of features sampled per tree (adds randomness)
#   gamma = 0,                      # Minimum loss reduction required for a split (regularization parameter)
#   num_class = num_class)           # Total number of target classes (required for multi-class setup)
# 
# 
# # Use optimal nrounds from CV
# final_nrounds <- 68  # Best number of boosting rounds determined by earlier cross-validation
# 
# # Train final model with early stopping on evaluation set
# final_model <- xgb.train(
#   params = final_params,          # Pass hyperparameters
#   data = dtrain,                  # Training data (DMatrix)
#   nrounds = final_nrounds,        # Number of boosting rounds (iterations)
#   watchlist = list(train = dtrain, eval = dtest),  # Monitor train and test sets during training
#   early_stopping_rounds = 25,     # Stop training if no improvement for 25 consecutive rounds
#   verbose = 1,                    # Print training log (progress per round)
#   maximize = FALSE)               # We are minimizing logloss (not maximizing accuracy)
```

```{r model-test-set-eval}

#MATT EDIT: COmmented out for now
# # ---------------------------
# # Test set evaluation
# # ---------------------------
# 
# # Predict probabilities on test set
# pred_test_prob <- predict(final_model, dtest)  
# # -> Returns a flat vector of probabilities for all classes stacked row-wise
# 
# # Convert probability vector into a matrix (rows = observations, cols = classes)
# pred_test_matrix <- matrix(pred_test_prob, 
#                            nrow = nrow(dm_test),   # Number of test samples
#                            ncol = num_class,       # Number of classes
#                            byrow = TRUE)           # Fill by rows (each row = one observation)
# 
# # Convert probabilities to predicted classes
# pred_test_class <- max.col(pred_test_matrix) - 1L  
# # -> max.col finds index of highest probability (predicted class)
# # -> Subtract 1 since XGBoost class indices start at 0
# 
# # Convert numeric labels back to factors for confusion matrix
# y_test_factor <- factor(y_test, levels = 0:(num_class-1), labels = levels_y)  
# pred_test_factor <- factor(pred_test_class, levels = 0:(num_class-1), labels = levels_y)  
# # -> Factors needed for caret's confusionMatrix
```

```{r eval-results}

#MATT EDIT: Commented out for now
# # Compute confusion matrix and overall accuracy
# cm_test <- confusionMatrix(pred_test_factor, y_test_factor)  
# 
# # Prints
# cat("\nFinal Model Test Accuracy:", round(cm_test$overall['Accuracy'], 4), "\n")  
# cat("\nConfusion Matrix (Test Set):\n")  
# print(cm_test$table)  
# # -> Shows prediction performance per class (actual vs predicted counts)
# 
# # Compute multi-class AUC per class (one-vs-all)
# test_auc_list <- list()
# for (i in 1:num_class) {
#   binary_labels <- as.numeric(y_test == (i - 1))          
#   # -> Convert labels into binary (one-vs-rest for class i)
#   test_auc_list[[levels_y[i]]] <- roc(binary_labels, pred_test_matrix[, i])$auc  
#   # -> Compute ROC-AUC for that class
# }
# 
# 
# cat("\nFinal Model Test AUC per class:\n")
# print(round(unlist(test_auc_list), 4))  
# # -> Print AUC values for each class
```

```{r model-feature-importance}

#MATT EDIT: COmmented out for now
# # ---------------------------
# # Feature importance and interpretation
# # ---------------------------
# # Get feature importance
# importance_matrix <- xgb.importance(model = final_model)  
# print(head(importance_matrix, 20))  # Print top 20 most important features ranked by gain
# 
# # Plot feature importance
# xgb.plot.importance(importance_matrix, top_n = 20)  
# # -> Visualize top 20 important features
```

===================================================== Neural Net ====================================================

```{r h2o-initialise}
## ---------------------------
## H2O Deep Learning (Neural Network) workflow
## Built from existing objects: `train`, `test`, `predictor_cols`, `target`, `levels_y`, `num_class`
## This script uses the data preparation already done above and trains an H2O neural net,
## evaluates on the test set and shows how to tune hyperparameters with H2O Grid Search.
## ---------------------------

#Comment from Matt: I think we need to change this approach so that the NN code can run without the above, because I am not certain that we are keeping much from the XGboost work?

target <- "OAH"
predictor_cols <- setdiff(names(data), target) #Matt edit, replaced df with data, to be consistent as above


# Load h2o and initialize --------------------------------------------------
# Note: if h2o isn't installed, run: install.packages("h2o")
library(h2o)
# Start (or connect to) an H2O cluster. This creates an in-memory instance used for training.
# nthreads = -1 will use all available cores.
h2o.init(nthreads = -1)

```

```{r turn-to-h2o-format}
# Convert existing R data.frames to H2O frames --------------------------------
# We use the `train` and `test` data.frames already prepared earlier in the pipeline.
# H2O works with its own frame objects; as.h2o() converts R data.frames into H2O frames.
train_h2o <- as.h2o(train) #Matt Comment: now this uses my test and train made before modeling
test_h2o  <- as.h2o(test)
```

```{r feature-modification-h2o}
# Define features (x) and response (y) -------------------------------------
# Using the same predictor list used for XGBoost so we reuse feature engineering already performed.
x_vars <- predictor_cols  # character vector of predictor column names
y_var  <- target          # the response column name ("oah")


# Ensure response is treated as a categorical (factor) in H2O for classification
train_h2o[[y_var]] <- as.factor(train_h2o[[y_var]])
test_h2o[[y_var]]  <- as.factor(test_h2o[[y_var]])
```

```{r neural-net-weights}

#Matt Edit: This is causing NA's when I run it. I dont necessarily think we have to weight rare classes more, but weight things equally, so for now I am setting balance classes = T in H2o.

# # ---------------------------
# # H2O neural network with class weighting + randomized grid search
# # ---------------------------
# # This block expects the following objects already exist:
# #  - train (R data.frame), test (R data.frame)
# #  - predictor_cols (character vector), target (string), levels_y (factor levels), num_class (int)
# #  - h2o cluster started (h2o.init() called earlier)
# # If train_h2o/test_h2o are not present, the script will create them from train/test.
# 
# 
# # 1) Create a class-weight column (inverse frequency) to help under-represented classes -
# #    weight_i = N / (K * count(class_i))  -> standard balanced weight
# freq_table <- as.data.frame(table(train[[target]]))
# names(freq_table) <- c("class", "count")
# N_total <- nrow(train)
# K <- num_class
# 
# # map class -> weight
# class_counts <- setNames(freq_table$count, freq_table$class)
# class_weights_map <- N_total / (K * class_counts)   # higher weight for rarer classes
# 
# # create a vector of weights for each row in train, matching the order of 'train'
# weights_vector <- as.numeric(class_weights_map[as.character(train[[target]])])
# 
# # attach weights column into train H2O frame (column name: "class_weights")
# train_h2o[,"class_weights"] <- as.h2o(weights_vector)
# 
# # For reproducibility, add the weights to the R 'train' too (keeps them in sync)
# train$class_weights <- weights_vector
```

```{r neural-net-weighted-data-train-test-split}
# Split training H2O frame into train/validation for early stopping ---------
# We'll keep a validation frame to enable early stopping during H2O training.
# Use an 80/20 split of the existing training data.
#    We keep a validation frame so early stopping works.
set.seed(123)

splits <- h2o.splitFrame(train_h2o, ratios = 0.8, seed = 123)

train_h2o_split <- splits[[1]]
valid_h2o_split <- splits[[2]]
```

```{r neural-net-hyper-params}
# Define hyperparameter search space (RandomDiscrete)
#    Note: large search space with many models will run long; adjust max_models for runtime.
hyper_params <- list(
  hidden = list(
    c(128,128),
    c(128,64),
    c(64,32,16),
    c(64,32)),
  activation =  c("RectifierWithDropout"),
  l2 = c(0, 1e-4),
  rate = c(0.01, 0.001),                # learning rates (smaller = slower but better generalization)
  input_dropout_ratio = c(0.1, 0.3, 0.5))     # apply some input dropout regularization
```

```{r neural-net-search-criteria}
# Because hidden_dropout_ratios must match hidden layer length, we will set a default dropout
# and not include it in the hyperparameter space (keeps models from failing due to mismatched lengths)

search_criteria <- list(
  strategy = "RandomDiscrete",
  max_models = 30,    # change to 40-60 if you want a longer run (4-5 hours) — careful with RAM
  seed = 123,
  max_runtime_secs = 60*60*4)  # optional: cap runtime to ~4 hours (comment out if you prefer model count)

```

```{r h2o-grid-search}
# Launch grid search using class weights (weights_column) ---------------------
#    Note: epochs increased to allow deeper training; early stopping will limit wasted epochs.
grid_id <- "h2o_nn_balanced_grid"

grid <- h2o.grid(algorithm = "deeplearning",
                 grid_id = grid_id,
                 x = predictor_cols,
                 y = target,
                 training_frame = train_h2o_split,
                 validation_frame = valid_h2o_split,
                 # Use the weights column computed earlier:
                 # weights_column = "class_weights",
                 # # Make H2O use classes weighting; setting balance_classes=FALSE because we supply weights explicitly
                 balance_classes = T,
                 # Training control
                 epochs = 200,                       # allow many epochs (early stopping will truncate)
                 stopping_metric = "logloss",
                 stopping_tolerance = 1e-4,
                 stopping_rounds = 10,
                 # optimization / regularization defaults; some are overridden by hyperparams
                 distribution = "multinomial",
                 #activation = "RectifierWithDropout",
                 adaptive_rate = TRUE,
                 # small-ish default l1/l2 (grid will explore others)
                 l1 = 0, l2 = 0,
                 # Use relatively small mini-batch default (H2O autodetects)
                 reproducible = T,   # reproducible=TRUE may slow things on multi-core machines
                 seed = 123,
                 # hyperparameter space and search criteria:
                 hyper_params = hyper_params,
                 search_criteria = search_criteria)

# Review the grid search summary (sorted by validation logloss) ---------------
grid_perf <- h2o.getGrid(grid_id = grid_id, sort_by = "logloss", decreasing = FALSE)
print(grid_perf)


```

```{r get-best-model}
# Pull the best model (lowest validation logloss) ----------------------------
best_model_id <- grid_perf@model_ids[[1]]
best_model <- h2o.getModel(best_model_id)

cat("Best model id:", best_model_id, "\n")
print(best_model)

h2o.performance(best_model)
#save(grid_perf,  file = "grid1")   first   round

#plot of performance

perf.df <- as.data.frame(grid_perf@summary_table) 


ggplot(data = perf.df, aes(x = hidden, y = logloss, colour = as.factor(input_dropout_ratio))) + 
  geom_point(size = 2)+
  labs(y = "logloss")

ggplot(data = perf.df, aes(x = hidden, y = logloss, colour = as.factor(l1))) + 
  geom_point(size = 2)+
  labs(y = "logloss")

ggplot(data = perf.df, aes(x = hidden, y = logloss, colour = as.factor(l2))) + 
  geom_point(size = 2)+
  labs(y = "logloss")


h2o.performance(best_model)


ggplot(perf.df, aes(x = factor(l1), y = factor(l2), fill = logloss)) +
  geom_tile() +
  facet_wrap(~ hidden) +
  scale_fill_viridis_c() +
  theme_minimal() +
  labs(title = "Effect of L1/L2 by Architecture")


ggplot(perf.df, aes(x = rate, y = logloss, color = factor(input_dropout_ratio))) +
  geom_point() +
  facet_wrap(~ hidden) +
  theme_minimal() +
  labs(title = "Learning Rate vs Logloss by Architecture & Dropout")


```

```{r best-model-eval}

levels_y <- levels(train$OAH)
# Evaluate the best model on the held-out test set ---------------------------
best_pred_h2o <- h2o.predict(best_model, test_h2o)   # returns 'predict' + per-class probabilities
best_pred_df <- as.data.frame(best_pred_h2o)

# predicted class labels (as factor)
pred_labels_best <- factor(best_pred_df$predict)
true_labels_best <- factor(as.vector(test_h2o[[target]]))

# confusion matrix & caret metrics
cm_best <- confusionMatrix(pred_labels_best, true_labels_best)  #adding the  levels
print(cm_best)
cat("Test Accuracy (best H2O NN):", round(cm_best$overall['Accuracy'],4), "\n")

cm_best$overall
mean(pred_labels_best==true_labels_best)
```

```{r per-class-auc}
# NEEDS DEBUGGIN!


# Compute per-class AUC (one-vs-all) using predicted probabilities ------------
# probability columns in best_pred_df must match levels_y order; H2O uses level names as columns
# prob_matrix <- as.matrix(best_pred_df[, levels_y])
# 
# auc_list <- sapply(seq_along(levels_y), function(i) {
#   cls <- levels_y[i]
#   bin_true <- as.numeric(true_labels_best == cls)
#   scores <- prob_matrix[, cls]
#   if (length(unique(bin_true)) < 2) return(NA)   # if a class not present in test
#   auc_val <- as.numeric(roc(bin_true, scores)$auc)
#   return(round(auc_val,4))
# })
# 
# names(auc_list) <- levels_y
# 
# print(auc_list)
```

```{r top-three-predicted-classes}
# NEEDS DEBUGGIN!

# # Top-K (e.g. Top-3) hit ratio - useful in practice -------------------------
# # calculate if true label is in top-3 predicted probabilities
# topk <- function(prob_row, k = 3) {
#   ord <- order(prob_row, decreasing = TRUE)
#   return(names(prob_row)[ord[1:k]])
# }
# 
# probs_df <- as.data.frame(prob_matrix)
# 
# in_topk <- mapply(function(i, true_lbl) {
#   topk_preds <- topk(as.numeric(probs_df[i, ]), k = 3)
#   true_lbl %in% topk_preds
# }, i = seq_len(nrow(probs_df)), true_lbl = as.character(true_labels_best))
# 
# 
# top3_hit_ratio <- mean(in_topk, na.rm = TRUE)
# 
# cat("Top-3 hit ratio (best model):", round(top3_hit_ratio,4), "\n")
```

```{r save-best-model}
# Save the best model to disk  -----------------------------------
model_path <- h2o.saveModel(best_model, path = getwd(), force = TRUE)
cat("Saved best model to:", model_path, "\n")

#------------------------------------ Notes  -----------------------------------------------------------
# - If you encounter OOM / memory errors: reduce `max_models` or reduce hidden layer sizes
#   or set max_runtime_secs in search_criteria to limit the run-time.
# - Because we used per-row weights, the model actively penalizes mistakes on rare classes.
# - If you want even stronger balancing, you can set balance_classes = TRUE as well,
#   or apply targeted over-sampling (SMOTE) on the R side before converting to H2O.
# - For an ensemble: keep a few of the top grid models and use h2o.stackedEnsemble() to combine them.
```

```{r best-model-metrics}
# ---------- Robust metrics, top-k, AUCs, calibration ----------

#Get predicted DF and true labels (from best_pred_df / test_h2o)
# If we don't have best_pred_df, compute it:
# best_pred_h2o <- h2o.predict(best_model, test_h2o)
# best_pred_df <- as.data.frame(best_pred_h2o)

pred_df <- best_pred_df   # data.frame containing "predict" and per-class probability columns named by levels_y
true_labels <- factor(as.vector(test_h2o[[target]]), levels = levels_y)


library(caret)
cm_best <- confusionMatrix(table(pred_labels_best, true_labels_best))


# Extract only probability columns (exclude "predict")
prob_cols <- pred_df[, setdiff(colnames(pred_df), "predict")]

# Confusion matrix (caret)
pred_factor <- factor(pred_df$predict, levels = levels_y)
cm <- confusionMatrix(pred_factor, true_labels)
print(cm)   # this is safe
```

```{r neural-net-per-class-metrices}
# Per-class precision, recall, F1, macro-F1 (safe numeric conversions)
byClass <- as.data.frame(cm$byClass)

# caret's byClass could have columns like 'Sensitivity','Specificity','Pos Pred Value' etc.
# Ensure numeric columns are numeric:
num_cols <- sapply(byClass, is.numeric)
if (!all(num_cols)) {
  # coerce columns that look like numbers but are factors/characters
  for (cname in names(byClass)) {
    byClass[[cname]] <- suppressWarnings(as.numeric(as.character(byClass[[cname]])))
  }
}

precision <- byClass$`Pos Pred Value`       # precision
recall    <- byClass$Sensitivity            # recall
f1 <- 2 * (precision * recall) / (precision + recall)
f1[is.na(f1)] <- 0
metrics_tbl <- data.frame(
  class = rownames(byClass),
  precision = round(precision,4),
  recall = round(recall,4),
  F1 = round(f1,4))

print(metrics_tbl)
cat("Macro F1:", round(mean(f1, na.rm = TRUE),4), "\n")
```

```{r top-hits}
# Top-K hit ratios (Top-1, Top-2, Top-3)
topk_hit <- function(probs_df, true_labels, k=3) {
  n <- nrow(probs_df)
  hits <- logical(n)
  for (i in seq_len(n)) {
    row_probs <- as.numeric(probs_df[i, ])
    names(row_probs) <- colnames(probs_df)
    topk_names <- names(sort(row_probs, decreasing = TRUE))[1:k]
    hits[i] <- as.character(true_labels[i]) %in% topk_names
  }
  mean(hits, na.rm = TRUE)
}

cat("Top-1 hit:", round(topk_hit(prob_cols, true_labels, 1),4), "\n")
cat("Top-2 hit:", round(topk_hit(prob_cols, true_labels, 2),4), "\n")
cat("Top-3 hit:", round(topk_hit(prob_cols, true_labels, 3),4), "\n")
```

```{r calibration-plot-perclass}
# Calibration plot per class (reliability diagram) - example for one class
calibration_plot_one <- function(class_label, prob_df, true_labels, n_bins = 10) {
  scores <- prob_df[[class_label]]
  true_bin <- as.numeric(true_labels == class_label)
  df_cal <- data.frame(scores = scores, true = true_bin)
  df_cal$bin <- cut(df_cal$scores, breaks = seq(0,1,length.out = n_bins+1), include.lowest = TRUE)
  agg <- aggregate(cbind(mean_prob = scores, obs_rate = true) ~ bin, data = df_cal, FUN = mean)
  agg$bin_mid <- (seq(0,1,length.out = n_bins+1)[-1] + seq(0,1,length.out = n_bins+1)[- (n_bins+1)])/2
  ggplot(agg, aes(x = mean_prob, y = obs_rate)) +
    geom_point() + geom_line() + geom_abline(intercept = 0, slope = 1, linetype = "dashed", color = "gray") +
    labs(title = paste("Calibration for", class_label), x = "Mean predicted probability", y = "Observed frequency")
}

# Example: plot calibration for 'Moderate' (change to any class)
if ("Moderate" %in% colnames(prob_cols)) {
  print(calibration_plot_one("Moderate", prob_cols, true_labels, n_bins = 10))
}
```

```{r neural-net-save-metrics}
# Save metrics to file 
write.csv(metrics_tbl, "h2o_nn_per_class_metrics.csv", row.names = FALSE)
```

```{r}
h2o.varimp(best_model)

h2o.varimp_plot(best_model)
```

## Discussion
